{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8007e7c1",
      "metadata": {},
      "source": [
        "# 00 Â· Prepare Data for LogBERT Pipeline\n",
        "\n",
        "This notebook downloads public log datasets, applies regex normalization, mines templates with Drain3, and materializes tokenized Parquet splits ready for training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af603d7",
      "metadata": {},
      "source": [
        "## Notebook Goals\n",
        "- Install pinned dependencies for the workstation environment.\n",
        "- Fetch HDFS and OpenStack log corpora with checksum verification and automatic mirror fallback.\n",
        "- Apply regex-based normalization rules from `configs/data.yaml` and preview before/after examples.\n",
        "- Mine log templates in streaming mode with Drain3 and persist template transitions as Parquet.\n",
        "- Build Hugging Face datasets with BERT-compatible tokenization, time-based splits (80/10/10), and truncation stats.\n",
        "- Save artifacts (tokenizer, processed Parquet, metadata) for downstream notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d0a20f",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fa2e3104",
      "metadata": {
        "tags": [
          "helper",
          "collapsible"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "requirements.txt not found; skipping installation\n"
          ]
        }
      ],
      "source": [
        "import os, sys, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "if os.environ.get('SKIP_REQUIREMENTS', '0') == '1':\n",
        "    print('SKIP_REQUIREMENTS=1 -> skipping pip install from requirements.txt')\n",
        "else:\n",
        "    req_path = Path('requirements.txt')\n",
        "    if req_path.exists():\n",
        "        print('[setup] Installing dependencies from requirements.txt ...')\n",
        "        completed = subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', str(req_path)])\n",
        "        if completed.returncode != 0:\n",
        "            raise RuntimeError('pip installation failed; inspect output above.')\n",
        "    else:\n",
        "        print('requirements.txt not found; skipping installation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "508748f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tpi/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/torch/cuda/__init__.py:54: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import math\n",
        "import tarfile\n",
        "import hashlib\n",
        "import shutil\n",
        "import textwrap\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from drain3 import TemplateMiner\n",
        "from drain3.template_miner_config import TemplateMinerConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import yaml\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "79da8a4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Install polars for faster data loading\n",
        "# !pip install polars\n",
        "\n",
        "import polars as pl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f8a5821",
      "metadata": {},
      "source": [
        "### Load Configuration and Prepare Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7d8df71b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00_prepare_data.ipynb\t02_finetune_openstack.ipynb  data\n",
            "01_pretrain_hdfs.ipynb\tartifacts\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5c525a35",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[setup] Configuration loaded and folders prepared.\n"
          ]
        }
      ],
      "source": [
        "def load_yaml(path: Path) -> Dict:\n",
        "    with path.open('r') as fh:\n",
        "        return yaml.safe_load(fh)\n",
        "\n",
        "data_config = load_yaml(Path('../configs/data.yaml'))\n",
        "RAW_HDFS_DIR = Path(data_config['raw_paths']['hdfs'])\n",
        "RAW_OPENSTACK_DIR = Path(data_config['raw_paths']['openstack'])\n",
        "ARTIFACTS_DIR = Path(data_config['artifacts_dir'])\n",
        "TOKENIZER_DIR = ARTIFACTS_DIR / 'tokenizer'\n",
        "PARQUET_DIR = Path(data_config['preprocessing']['parquet_dir'])\n",
        "METADATA_PATH = Path(data_config['preprocessing']['dataset_metadata'])\n",
        "for folder in [RAW_HDFS_DIR, RAW_OPENSTACK_DIR, ARTIFACTS_DIR, TOKENIZER_DIR, PARQUET_DIR, METADATA_PATH.parent]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "print('[setup] Configuration loaded and folders prepared.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c516d6",
      "metadata": {},
      "source": [
        "## 2. Download Public Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c9f84d4a",
      "metadata": {
        "tags": [
          "helper",
          "download"
        ]
      },
      "outputs": [],
      "source": [
        "def stream_download(url: str, destination: Path) -> bool:\n",
        "    chunk = 1 << 20\n",
        "    try:\n",
        "        with requests.get(url, stream=True, timeout=60) as resp:\n",
        "            resp.raise_for_status()\n",
        "            total = int(resp.headers.get('content-length', 0))\n",
        "            with destination.open('wb') as fh, tqdm(total=total, unit='B', unit_scale=True, desc=f'download {destination.name}') as bar:\n",
        "                for part in resp.iter_content(chunk_size=chunk):\n",
        "                    if part:\n",
        "                        fh.write(part)\n",
        "                        bar.update(len(part))\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f'[warn] download failed from {url}: {exc}')\n",
        "        return False\n",
        "\n",
        "def sha256sum(path: Path) -> str:\n",
        "    hash_obj = hashlib.sha256()\n",
        "    with path.open('rb') as fh:\n",
        "        for chunk in iter(lambda: fh.read(1 << 20), b''):\n",
        "            hash_obj.update(chunk)\n",
        "    return hash_obj.hexdigest()\n",
        "\n",
        "def ensure_download(urls: List[str], target: Path, expected_sha: Optional[str] = None) -> Path:\n",
        "    for url in urls:\n",
        "        if stream_download(url, target):\n",
        "            break\n",
        "    else:\n",
        "        raise RuntimeError(f'all download mirrors failed for {target.name}')\n",
        "    checksum = sha256sum(target)\n",
        "    if expected_sha and checksum != expected_sha:\n",
        "        raise ValueError(f'sha mismatch for {target.name}: expected {expected_sha}, got {checksum}')\n",
        "    return target\n",
        "\n",
        "def maybe_update_sha(config_section: Dict, key: str, computed_sha: str) -> None:\n",
        "    current = config_section.get(key)\n",
        "    if isinstance(current, str) and current:\n",
        "        return\n",
        "    config_section[key] = computed_sha\n",
        "    with Path('../configs/data.yaml').open('w') as fh:\n",
        "        yaml.safe_dump(data_config, fh, sort_keys=False)\n",
        "    print(f'[sha] recorded SHA256 for {key}: {computed_sha}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3fb2cd23",
      "metadata": {
        "tags": [
          "download"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[download] HDFS archive already exists, skipping download\n",
            "[extract] HDFS log already present\n",
            "[ready] HDFS log at data/hdfs/raw/HDFS.log\n"
          ]
        }
      ],
      "source": [
        "hdfs_cfg = data_config['datasets']['hdfs']\n",
        "archive_path = RAW_HDFS_DIR / hdfs_cfg['archive_name']\n",
        "archive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Check if archive already exists before downloading\n",
        "if archive_path.exists():\n",
        "    print('[download] HDFS archive already exists, skipping download')\n",
        "    computed_sha = sha256sum(archive_path)\n",
        "    maybe_update_sha(hdfs_cfg, 'sha256', computed_sha)\n",
        "else:\n",
        "    print('[download] HDFS corpus')\n",
        "    ensure_download(hdfs_cfg['urls'], archive_path, hdfs_cfg.get('sha256') or None)\n",
        "    computed_sha = sha256sum(archive_path)\n",
        "    maybe_update_sha(hdfs_cfg, 'sha256', computed_sha)\n",
        "\n",
        "if not (RAW_HDFS_DIR / hdfs_cfg['log_file']).exists():\n",
        "    print('[extract] HDFS archive')\n",
        "    with tarfile.open(archive_path, 'r:gz') as tf:\n",
        "        tf.extractall(RAW_HDFS_DIR)\n",
        "else:\n",
        "    print('[extract] HDFS log already present')\n",
        "\n",
        "hdfs_log_path = RAW_HDFS_DIR / hdfs_cfg['log_file']\n",
        "print(f'[ready] HDFS log at {hdfs_log_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d54902ca",
      "metadata": {
        "tags": [
          "download"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[download] OpenStack normal logs\n",
            "[cache] Detected existing openstack_normal.log, skipping download.\n",
            "[download] OpenStack abnormal logs\n",
            "[cache] Detected existing openstack_abnormal.log, skipping download.\n",
            "[ready] OpenStack logs -> {'normal': PosixPath('data/openstack/raw/openstack_normal.log'), 'abnormal': PosixPath('data/openstack/raw/openstack_abnormal.log')}\n"
          ]
        }
      ],
      "source": [
        "openstack_cfg = data_config['datasets']['openstack']\n",
        "RAW_OPENSTACK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "paths: Dict[str, Path] = {}\n",
        "print('[download] OpenStack normal logs')\n",
        "normal_target = RAW_OPENSTACK_DIR / 'openstack_normal.log'\n",
        "normal_urls = openstack_cfg['normal_urls']\n",
        "normal_sha_cfg = (openstack_cfg.get('sha256') or {}).get('normal')\n",
        "normal_acquired = False\n",
        "if normal_target.exists():\n",
        "    print('[cache] Detected existing openstack_normal.log, skipping download.')\n",
        "    normal_acquired = True\n",
        "    normal_urls = []\n",
        "    maybe_update_sha(openstack_cfg['sha256'], 'normal', sha256sum(normal_target))\n",
        "if not normal_acquired:\n",
        "    for url in normal_urls:\n",
        "        try:\n",
        "            if url.endswith('.tar.gz'):\n",
        "                tmp_tar = RAW_OPENSTACK_DIR / 'openstack_bundle.tar.gz'\n",
        "                ensure_download([url], tmp_tar, None)\n",
        "                with tarfile.open(tmp_tar, 'r:gz') as tf:\n",
        "                    member = next((m for m in tf.getmembers() if m.name.endswith('openstack_normal.log')), None)\n",
        "                    if member is None:\n",
        "                        raise ValueError('openstack_normal.log not found inside archive')\n",
        "                    tf.extract(member, RAW_OPENSTACK_DIR)\n",
        "                    extracted_path = RAW_OPENSTACK_DIR / member.name\n",
        "                    final_path = RAW_OPENSTACK_DIR / 'openstack_normal.log'\n",
        "                    final_path.write_bytes(extracted_path.read_bytes())\n",
        "                    if extracted_path.exists():\n",
        "                        extracted_path.unlink()\n",
        "                    extracted_dir = extracted_path.parent\n",
        "                    if extracted_dir != RAW_OPENSTACK_DIR and extracted_dir.exists():\n",
        "                        shutil.rmtree(extracted_dir, ignore_errors=True)\n",
        "                tmp_tar.unlink(missing_ok=True)\n",
        "                normal_acquired = True\n",
        "            else:\n",
        "                ensure_download([url], normal_target, normal_sha_cfg)\n",
        "                normal_acquired = True\n",
        "            if normal_acquired:\n",
        "                maybe_update_sha(openstack_cfg['sha256'], 'normal', sha256sum(normal_target))\n",
        "                break\n",
        "        except Exception as exc:\n",
        "            print(f'[warn] fallback download failed from {url}: {exc}')\n",
        "if not normal_acquired:\n",
        "    raise RuntimeError('All download mirrors failed for OpenStack normal logs')\n",
        "paths['normal'] = normal_target\n",
        "\n",
        "print('[download] OpenStack abnormal logs')\n",
        "abnormal_target = RAW_OPENSTACK_DIR / 'openstack_abnormal.log'\n",
        "if abnormal_target.exists():\n",
        "    print('[cache] Detected existing openstack_abnormal.log, skipping download.')\n",
        "else:\n",
        "    ensure_download(openstack_cfg['abnormal_urls'], abnormal_target, (openstack_cfg.get('sha256') or {}).get('abnormal'))\n",
        "    maybe_update_sha(openstack_cfg['sha256'], 'abnormal', sha256sum(abnormal_target))\n",
        "paths['abnormal'] = abnormal_target\n",
        "print(f'[ready] OpenStack logs -> {paths}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c041c59b",
      "metadata": {},
      "source": [
        "## 3. Load and Inspect Raw Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "aaa97922",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [],
      "source": [
        "def read_hdfs_log(path: Path, sample_fraction: float = 0.5) -> pd.DataFrame:\n",
        "    \"\"\"Read HDFS log file efficiently with sampling\"\"\"\n",
        "    print(f\"[hdfs] Reading {sample_fraction*100:.0f}% of HDFS log file...\")\n",
        "    \n",
        "    # First, count total lines for sampling\n",
        "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        total_lines = sum(1 for _ in f if _.strip())\n",
        "    \n",
        "    target_lines = int(total_lines * sample_fraction)\n",
        "    step = max(1, total_lines // target_lines)\n",
        "    \n",
        "    print(f\"[hdfs] Total lines: {total_lines:,}, sampling every {step} lines -> {target_lines:,} records\")\n",
        "    \n",
        "    # Read with sampling\n",
        "    sampled_lines = []\n",
        "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if line.strip() and i % step == 0:\n",
        "                sampled_lines.append(line.strip())\n",
        "                if len(sampled_lines) >= target_lines:\n",
        "                    break\n",
        "    \n",
        "    df = pd.DataFrame({'raw': sampled_lines})\n",
        "    \n",
        "    # HDFS format: YYMMDD HHMMSS (e.g., \"081109 203518\")\n",
        "    def parse_hdfs_timestamp(timestamp_str):\n",
        "        try:\n",
        "            if len(timestamp_str) >= 13:\n",
        "                date_part = timestamp_str[:6]  # YYMMDD\n",
        "                time_part = timestamp_str[7:13]  # HHMMSS\n",
        "                # Convert to YYYY-MM-DD HH:MM:SS format\n",
        "                year = '20' + date_part[:2]\n",
        "                month = date_part[2:4]\n",
        "                day = date_part[4:6]\n",
        "                hour = time_part[:2]\n",
        "                minute = time_part[2:4]\n",
        "                second = time_part[4:6]\n",
        "                formatted = f\"{year}-{month}-{day} {hour}:{minute}:{second}\"\n",
        "                return pd.to_datetime(formatted)\n",
        "        except:\n",
        "            pass\n",
        "        return pd.NaT\n",
        "    \n",
        "    df['timestamp'] = df['raw'].apply(parse_hdfs_timestamp)\n",
        "    df['content'] = df['raw']\n",
        "    result_df = df.dropna(subset=['timestamp']).reset_index(drop=True)\n",
        "    print(f\"[hdfs] Processed {len(result_df):,} valid records\")\n",
        "    return result_df\n",
        "\n",
        "def read_openstack_logs(paths: Dict[str, Path]) -> pd.DataFrame:\n",
        "    frames = []\n",
        "    for label, path in paths.items():\n",
        "        print(f\"[openstack] Reading {label} logs...\")\n",
        "        # Read the log file as plain text lines\n",
        "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            lines = [line.strip() for line in f if line.strip()]\n",
        "        \n",
        "        df = pd.DataFrame({'raw': lines})\n",
        "        df['timestamp'] = pd.to_datetime(df['raw'].str.extract(r'(\\d{4}-\\d{2}-\\d{2} [^ ]+)')[0], errors='coerce')\n",
        "        df['label'] = 1 if label == 'abnormal' else 0\n",
        "        df['content'] = df['raw']\n",
        "        valid_df = df.dropna(subset=['timestamp'])\n",
        "        print(f\"[openstack] {label}: {len(valid_df):,} valid records\")\n",
        "        frames.append(valid_df)\n",
        "    return pd.concat(frames, ignore_index=True).sort_values('timestamp').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "da131bb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Fast Polars-based implementation\n",
        "def read_hdfs_log_fast(path: Path, sample_fraction: float = 0.5) -> pd.DataFrame:\n",
        "    \"\"\"Ultra-fast HDFS log reading using Polars with streaming\"\"\"\n",
        "    print(f\"[hdfs-fast] Reading {sample_fraction*100:.0f}% of HDFS log with Polars...\")\n",
        "    \n",
        "    try:\n",
        "        # Read the entire file as text lines first (Polars way)\n",
        "        df_pl = pl.read_csv(\n",
        "            path,\n",
        "            has_header=False,\n",
        "            new_columns=['raw'],\n",
        "            separator='\\x00',  # Use null separator so each line is one field\n",
        "            ignore_errors=True,\n",
        "            truncate_ragged_lines=True,\n",
        "            encoding='utf8-lossy'\n",
        "        )\n",
        "        \n",
        "        # Filter out empty lines\n",
        "        df_pl = df_pl.filter(pl.col('raw').str.len_chars() > 0)\n",
        "        \n",
        "        # Sample the DataFrame\n",
        "        if sample_fraction < 1.0:\n",
        "            df_pl = df_pl.sample(fraction=sample_fraction, seed=42)\n",
        "        \n",
        "        # Convert to pandas for compatibility\n",
        "        df = df_pl.to_pandas()\n",
        "        \n",
        "        # Parse timestamps (vectorized)\n",
        "        def parse_hdfs_timestamp_vectorized(series):\n",
        "            # Extract timestamp part more efficiently\n",
        "            timestamp_part = series.str[:13]\n",
        "            \n",
        "            # Vectorized parsing using pandas datetime\n",
        "            parsed = pd.to_datetime(\n",
        "                '20' + timestamp_part.str[:2] + '-' + \n",
        "                timestamp_part.str[2:4] + '-' + \n",
        "                timestamp_part.str[4:6] + ' ' +\n",
        "                timestamp_part.str[7:9] + ':' + \n",
        "                timestamp_part.str[9:11] + ':' + \n",
        "                timestamp_part.str[11:13],\n",
        "                errors='coerce'\n",
        "            )\n",
        "            return parsed\n",
        "        \n",
        "        df['timestamp'] = parse_hdfs_timestamp_vectorized(df['raw'])\n",
        "        df['content'] = df['raw']\n",
        "        \n",
        "        result_df = df.dropna(subset=['timestamp']).reset_index(drop=True)\n",
        "        print(f\"[hdfs-fast] Processed {len(result_df):,} valid records (Polars)\")\n",
        "        return result_df\n",
        "        \n",
        "    except ImportError:\n",
        "        print(\"[hdfs-fast] Polars not available, falling back to pandas method\")\n",
        "        return read_hdfs_log(path, sample_fraction)\n",
        "    except Exception as e:\n",
        "        print(f\"[hdfs-fast] Polars method failed ({e}), falling back to pandas\")\n",
        "        return read_hdfs_log(path, sample_fraction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "52fca9a5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[loading] Choosing fastest loading method...\n",
            "[hdfs-fast] Reading 50% of HDFS log with Polars...\n",
            "[hdfs-fast] Processed 5,587,814 valid records (Polars)\n",
            "[openstack] Reading normal logs...\n",
            "[hdfs-fast] Processed 5,587,814 valid records (Polars)\n",
            "[openstack] Reading normal logs...\n",
            "[openstack] normal: 189,386 valid records\n",
            "[openstack] Reading abnormal logs...\n",
            "[openstack] abnormal: 18,434 valid records\n",
            "[dataset] HDFS records: 5,587,814 | OpenStack records: 207,820\n",
            "[openstack] normal: 189,386 valid records\n",
            "[openstack] Reading abnormal logs...\n",
            "[openstack] abnormal: 18,434 valid records\n",
            "[dataset] HDFS records: 5,587,814 | OpenStack records: 207,820\n"
          ]
        }
      ],
      "source": [
        "# Alternative: Memory-mapped reading for huge files\n",
        "def read_hdfs_log_mmap(path: Path, sample_fraction: float = 0.5) -> pd.DataFrame:\n",
        "    \"\"\"Memory-mapped reading for very large files\"\"\"\n",
        "    import mmap\n",
        "    print(f\"[hdfs-mmap] Reading {sample_fraction*100:.0f}% with memory mapping...\")\n",
        "    \n",
        "    sampled_lines = []\n",
        "    with open(path, 'rb') as f:  # Open in binary mode for mmap\n",
        "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
        "            # Count lines first by reading the entire mmap and counting newlines\n",
        "            total_lines = mm[:].count(b'\\n')\n",
        "            target_lines = int(total_lines * sample_fraction)\n",
        "            step = max(1, total_lines // target_lines)\n",
        "            \n",
        "            print(f\"[hdfs-mmap] Total lines: {total_lines:,}, sampling every {step} -> {target_lines:,}\")\n",
        "            \n",
        "            # Reset to beginning and sample\n",
        "            mm.seek(0)\n",
        "            line_num = 0\n",
        "            for line in iter(mm.readline, b\"\"):\n",
        "                if line_num % step == 0:\n",
        "                    line_str = line.decode('utf-8', errors='ignore').strip()\n",
        "                    if line_str:\n",
        "                        sampled_lines.append(line_str)\n",
        "                        if len(sampled_lines) >= target_lines:\n",
        "                            break\n",
        "                line_num += 1\n",
        "    \n",
        "    # Process sampled data\n",
        "    df = pd.DataFrame({'raw': sampled_lines})\n",
        "    \n",
        "    # Vectorized timestamp parsing\n",
        "    timestamp_parts = df['raw'].str[:13]\n",
        "    df['timestamp'] = pd.to_datetime(\n",
        "        '20' + timestamp_parts.str[:2] + '-' + \n",
        "        timestamp_parts.str[2:4] + '-' + \n",
        "        timestamp_parts.str[4:6] + ' ' +\n",
        "        timestamp_parts.str[7:9] + ':' + \n",
        "        timestamp_parts.str[9:11] + ':' + \n",
        "        timestamp_parts.str[11:13],\n",
        "        errors='coerce'\n",
        "    )\n",
        "    df['content'] = df['raw']\n",
        "    \n",
        "    result_df = df.dropna(subset=['timestamp']).reset_index(drop=True)\n",
        "    print(f\"[hdfs-mmap] Processed {len(result_df):,} valid records\")\n",
        "    return result_df\n",
        "\n",
        "# Use the fastest available method\n",
        "print(\"[loading] Choosing fastest loading method...\")\n",
        "try:\n",
        "    # Try Polars first (fastest)\n",
        "    hdfs_df = read_hdfs_log_fast(hdfs_log_path, sample_fraction=0.5)\n",
        "except:\n",
        "    try:\n",
        "        # Fall back to memory mapping\n",
        "        hdfs_df = read_hdfs_log_mmap(hdfs_log_path, sample_fraction=0.5)\n",
        "    except:\n",
        "        # Final fallback to pandas\n",
        "        hdfs_df = read_hdfs_log(hdfs_log_path, sample_fraction=0.5)\n",
        "\n",
        "openstack_df = read_openstack_logs(paths)\n",
        "print(f'[dataset] HDFS records: {len(hdfs_df):,} | OpenStack records: {len(openstack_df):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "274fb81b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2008-11-10 12:12:08</td>\n",
              "      <td>081110 121208 31 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2008-11-11 02:32:38</td>\n",
              "      <td>081111 023238 19 INFO dfs.FSDataset: Deleting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2008-11-10 07:00:10</td>\n",
              "      <td>081110 070010 7424 INFO dfs.DataNode$DataXceiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2008-11-10 13:04:40</td>\n",
              "      <td>081110 130440 11588 WARN dfs.DataNode$DataXcei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2008-11-10 22:06:43</td>\n",
              "      <td>081110 220643 32 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            timestamp                                            content\n",
              "0 2008-11-10 12:12:08  081110 121208 31 INFO dfs.FSNamesystem: BLOCK*...\n",
              "1 2008-11-11 02:32:38  081111 023238 19 INFO dfs.FSDataset: Deleting ...\n",
              "2 2008-11-10 07:00:10  081110 070010 7424 INFO dfs.DataNode$DataXceiv...\n",
              "3 2008-11-10 13:04:40  081110 130440 11588 WARN dfs.DataNode$DataXcei...\n",
              "4 2008-11-10 22:06:43  081110 220643 32 INFO dfs.FSNamesystem: BLOCK*..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>label</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-05-14 19:39:01.445</td>\n",
              "      <td>1</td>\n",
              "      <td>nova-api.log.2017-05-14_21:27:04 2017-05-14 19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-05-14 19:39:01.650</td>\n",
              "      <td>1</td>\n",
              "      <td>nova-api.log.2017-05-14_21:27:04 2017-05-14 19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-05-14 19:39:02.007</td>\n",
              "      <td>1</td>\n",
              "      <td>nova-compute.log.2017-05-14_21:27:09 2017-05-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-05-14 19:39:02.924</td>\n",
              "      <td>1</td>\n",
              "      <td>nova-api.log.2017-05-14_21:27:04 2017-05-14 19...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-05-14 19:39:03.166</td>\n",
              "      <td>1</td>\n",
              "      <td>nova-compute.log.2017-05-14_21:27:09 2017-05-1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                timestamp  label  \\\n",
              "0 2017-05-14 19:39:01.445      1   \n",
              "1 2017-05-14 19:39:01.650      1   \n",
              "2 2017-05-14 19:39:02.007      1   \n",
              "3 2017-05-14 19:39:02.924      1   \n",
              "4 2017-05-14 19:39:03.166      1   \n",
              "\n",
              "                                             content  \n",
              "0  nova-api.log.2017-05-14_21:27:04 2017-05-14 19...  \n",
              "1  nova-api.log.2017-05-14_21:27:04 2017-05-14 19...  \n",
              "2  nova-compute.log.2017-05-14_21:27:09 2017-05-1...  \n",
              "3  nova-api.log.2017-05-14_21:27:04 2017-05-14 19...  \n",
              "4  nova-compute.log.2017-05-14_21:27:09 2017-05-1...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(hdfs_df[['timestamp','content']].head())\n",
        "display(openstack_df[['timestamp','label','content']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01525674",
      "metadata": {},
      "source": [
        "## 4. Regex Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "90f35b0a",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[normalization] Processing samples first for verification...\n",
            "HDFS normalization preview:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>081110 121208 31 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
              "      <td>&lt;NUM_1e3&gt; &lt;NUM_1e3&gt; &lt;PORT&gt; INFO dfs.FSNamesyst...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>081111 023238 19 INFO dfs.FSDataset: Deleting ...</td>\n",
              "      <td>&lt;NUM_1e3&gt; &lt;NUM_1e3&gt; &lt;PORT&gt; INFO dfs.FSDataset:...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>081110 070010 7424 INFO dfs.DataNode$DataXceiv...</td>\n",
              "      <td>&lt;NUM_1e3&gt; &lt;NUM_1e3&gt; &lt;PORT&gt; INFO dfs.DataNode$D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>081110 130440 11588 WARN dfs.DataNode$DataXcei...</td>\n",
              "      <td>&lt;NUM_1e3&gt; &lt;NUM_1e3&gt; &lt;PORT&gt; WARN dfs.DataNode$D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>081110 220643 32 INFO dfs.FSNamesystem: BLOCK*...</td>\n",
              "      <td>&lt;NUM_1e3&gt; &lt;NUM_1e3&gt; &lt;PORT&gt; INFO dfs.FSNamesyst...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            original  \\\n",
              "0  081110 121208 31 INFO dfs.FSNamesystem: BLOCK*...   \n",
              "1  081111 023238 19 INFO dfs.FSDataset: Deleting ...   \n",
              "2  081110 070010 7424 INFO dfs.DataNode$DataXceiv...   \n",
              "3  081110 130440 11588 WARN dfs.DataNode$DataXcei...   \n",
              "4  081110 220643 32 INFO dfs.FSNamesystem: BLOCK*...   \n",
              "\n",
              "                                          normalized  \n",
              "0  <NUM_1e3> <NUM_1e3> <PORT> INFO dfs.FSNamesyst...  \n",
              "1  <NUM_1e3> <NUM_1e3> <PORT> INFO dfs.FSDataset:...  \n",
              "2  <NUM_1e3> <NUM_1e3> <PORT> INFO dfs.DataNode$D...  \n",
              "3  <NUM_1e3> <NUM_1e3> <PORT> WARN dfs.DataNode$D...  \n",
              "4  <NUM_1e3> <NUM_1e3> <PORT> INFO dfs.FSNamesyst...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample sizes - HDFS: 1,000, OpenStack: 1,000\n",
            "Ready to process full datasets. Continue to next cell when satisfied with preview.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class LogNormalizer:\n",
        "    def __init__(self, rules: Iterable[Dict[str, str]]):\n",
        "        self.rules = [(rule['name'], re.compile(rule['pattern']), rule['replace']) for rule in rules]\n",
        "\n",
        "    def apply(self, text: str) -> str:\n",
        "        result = text\n",
        "        for _, pattern, repl in self.rules:\n",
        "            result = pattern.sub(repl, result)\n",
        "        return result\n",
        "\n",
        "    def normalize_series(self, series: pd.Series) -> pd.Series:\n",
        "        return series.astype(str).apply(self.apply)\n",
        "\n",
        "normalizer = LogNormalizer(data_config['normalizer']['rules'])\n",
        "\n",
        "# For performance, let's first work with a sample to test\n",
        "print(\"[normalization] Processing samples first for verification...\")\n",
        "hdfs_sample = hdfs_df.head(1000)  # Use first 1000 records for testing\n",
        "openstack_sample = openstack_df.head(1000)\n",
        "\n",
        "hdfs_sample_norm = normalizer.normalize_series(hdfs_sample['content'])\n",
        "openstack_sample_norm = normalizer.normalize_series(openstack_sample['content'])\n",
        "\n",
        "# Display preview with samples\n",
        "preview = pd.DataFrame({\n",
        "    'original': hdfs_sample['content'].head(5), \n",
        "    'normalized': hdfs_sample_norm.head(5)\n",
        "})\n",
        "print(\"HDFS normalization preview:\")\n",
        "display(preview)\n",
        "\n",
        "print(f\"\\nSample sizes - HDFS: {len(hdfs_sample):,}, OpenStack: {len(openstack_sample):,}\")\n",
        "print(\"Ready to process full datasets. Continue to next cell when satisfied with preview.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6550cd7",
      "metadata": {},
      "source": [
        "## 5. Template Mining with Drain3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5168a698",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[drain3] streaming normalized HDFS logs\n",
            "[drain3] Applying normalization to full dataset...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "feda2f56b86f459fb8cfeeedf4859cb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Drain3 HDFS:   0%|          | 0/5587814 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ngram</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>6-&gt;6-&gt;6</td>\n",
              "      <td>20608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>6-&gt;8-&gt;6</td>\n",
              "      <td>20591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>8-&gt;6-&gt;6</td>\n",
              "      <td>20490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1058</th>\n",
              "      <td>19-&gt;19-&gt;19</td>\n",
              "      <td>20404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>7-&gt;6-&gt;6</td>\n",
              "      <td>20401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>6-&gt;6-&gt;7</td>\n",
              "      <td>20377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>6-&gt;6-&gt;8</td>\n",
              "      <td>20364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1031</th>\n",
              "      <td>6-&gt;19-&gt;19</td>\n",
              "      <td>20360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981</th>\n",
              "      <td>6-&gt;6-&gt;19</td>\n",
              "      <td>20328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>7-&gt;7-&gt;6</td>\n",
              "      <td>20308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           ngram  count\n",
              "221      6->6->6  20608\n",
              "117      6->8->6  20591\n",
              "490      8->6->6  20490\n",
              "1058  19->19->19  20404\n",
              "137      7->6->6  20401\n",
              "130      6->6->7  20377\n",
              "375      6->6->8  20364\n",
              "1031   6->19->19  20360\n",
              "981     6->6->19  20328\n",
              "292      7->7->6  20308"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "drain_cfg = data_config['drain3']\n",
        "\n",
        "# Configure Drain3 template miner directly (correct API)\n",
        "template_config = TemplateMinerConfig()\n",
        "template_config.drain_depth = drain_cfg['depth']\n",
        "template_config.drain_sim_th = drain_cfg['st']  # similarity threshold\n",
        "template_config.drain_max_children = drain_cfg['max_children']\n",
        "template_config.drain_extra_delimiters = drain_cfg['extra_delimiters']\n",
        "\n",
        "# Create TemplateMiner with config parameter (not template_config)\n",
        "template_miner = TemplateMiner(config=template_config)\n",
        "\n",
        "records = []\n",
        "transition_counts = {}\n",
        "ngram = drain_cfg.get('template_transition_ngram', 3)\n",
        "print('[drain3] streaming normalized HDFS logs')\n",
        "cluster_trace: List[str] = []\n",
        "\n",
        "# First, we need to normalize the data before template mining\n",
        "print(\"[drain3] Applying normalization to full dataset...\")\n",
        "hdfs_df['normalized'] = normalizer.normalize_series(hdfs_df['content'])\n",
        "\n",
        "for row in tqdm(hdfs_df.to_dict('records'), desc='Drain3 HDFS'):\n",
        "    result = template_miner.add_log_message(row['normalized'])\n",
        "    cluster_id = result['cluster_id']\n",
        "    records.append({\n",
        "        'timestamp': row['timestamp'],\n",
        "        'template_id': cluster_id,\n",
        "        'template': result['template_mined'],\n",
        "        'content': row['normalized']\n",
        "    })\n",
        "    # Convert cluster_id to string for transition tracking\n",
        "    cluster_trace.append(str(cluster_id))\n",
        "\n",
        "for idx in range(len(cluster_trace) - ngram + 1):\n",
        "    key = tuple(cluster_trace[idx: idx + ngram])\n",
        "    transition_counts[key] = transition_counts.get(key, 0) + 1\n",
        "\n",
        "transition_output = Path(drain_cfg['transition_output'])\n",
        "transition_output.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "template_df = pd.DataFrame(records)\n",
        "index_path = transition_output.with_name('template_index.parquet')\n",
        "template_df.to_parquet(index_path, index=False)\n",
        "\n",
        "if transition_counts:\n",
        "    transition_df = pd.DataFrame([\n",
        "        {'ngram': '->'.join(key), 'count': count} for key, count in transition_counts.items()\n",
        "    ])\n",
        "    transition_df.to_parquet(transition_output, index=False)\n",
        "    display(transition_df.sort_values('count', ascending=False).head(10))\n",
        "else:\n",
        "    print('[drain3] no transitions recorded; dataset may be small')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b1d406",
      "metadata": {},
      "source": [
        "## 6. Tokenizer with Special Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d42a2f76",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tpi/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tokenizer] added 9 special tokens and saved to artifacts/tokenizer\n",
            "sample tokens: ['<NUM_1e3>', '<NUM_1e3>', '<PORT>', 'info', 'd', '##fs', '.', 'f', '##s', '##name', '##sy', '##ste', '##m', ':', 'block', '*', 'names', '##yst', '##em', '.', 'adds', '##tore', '##db', '##lock', ':', 'block', '##ma', '##p', 'updated', ':']\n"
          ]
        }
      ],
      "source": [
        "tokenizer_cfg = data_config['tokenizer']\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_cfg['base_model'], use_fast=True)\n",
        "special_tokens = data_config['tokens']['special']\n",
        "added = tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "TOKENIZER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "tokenizer.save_pretrained(TOKENIZER_DIR)\n",
        "print(f'[tokenizer] added {added} special tokens and saved to {TOKENIZER_DIR}')\n",
        "print('sample tokens:', tokenizer.tokenize(hdfs_df['normalized'].iloc[0])[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958e3eb9",
      "metadata": {},
      "source": [
        "## 7. Build Tokenized Parquet Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "08065c9d",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[setup] Loading existing artifacts...\n",
            "[tokenizer] Loaded from artifacts/tokenizer\n",
            "[template] Loaded 5,587,814 template-indexed records\n",
            "[hdfs] Loading HDFS data efficiently...\n",
            "[hdfs] Applying normalization...\n",
            "[template] Loaded 5,587,814 template-indexed records\n",
            "[hdfs] Loading HDFS data efficiently...\n",
            "[hdfs] Applying normalization...\n",
            "  Processed 100,000/5,587,814 records...\n",
            "  Processed 100,000/5,587,814 records...\n",
            "  Processed 200,000/5,587,814 records...\n",
            "  Processed 200,000/5,587,814 records...\n",
            "  Processed 300,000/5,587,814 records...\n",
            "  Processed 300,000/5,587,814 records...\n",
            "  Processed 400,000/5,587,814 records...\n",
            "  Processed 400,000/5,587,814 records...\n",
            "  Processed 500,000/5,587,814 records...\n",
            "  Processed 500,000/5,587,814 records...\n",
            "  Processed 600,000/5,587,814 records...\n",
            "  Processed 600,000/5,587,814 records...\n",
            "  Processed 700,000/5,587,814 records...\n",
            "  Processed 700,000/5,587,814 records...\n",
            "  Processed 800,000/5,587,814 records...\n",
            "  Processed 800,000/5,587,814 records...\n",
            "  Processed 900,000/5,587,814 records...\n",
            "  Processed 900,000/5,587,814 records...\n",
            "  Processed 1,000,000/5,587,814 records...\n",
            "  Processed 1,000,000/5,587,814 records...\n",
            "  Processed 1,100,000/5,587,814 records...\n",
            "  Processed 1,100,000/5,587,814 records...\n",
            "  Processed 1,200,000/5,587,814 records...\n",
            "  Processed 1,200,000/5,587,814 records...\n",
            "  Processed 1,300,000/5,587,814 records...\n",
            "  Processed 1,300,000/5,587,814 records...\n",
            "  Processed 1,400,000/5,587,814 records...\n",
            "  Processed 1,400,000/5,587,814 records...\n",
            "  Processed 1,500,000/5,587,814 records...\n",
            "  Processed 1,500,000/5,587,814 records...\n",
            "  Processed 1,600,000/5,587,814 records...\n",
            "  Processed 1,600,000/5,587,814 records...\n",
            "  Processed 1,700,000/5,587,814 records...\n",
            "  Processed 1,700,000/5,587,814 records...\n",
            "  Processed 1,800,000/5,587,814 records...\n",
            "  Processed 1,800,000/5,587,814 records...\n",
            "  Processed 1,900,000/5,587,814 records...\n",
            "  Processed 1,900,000/5,587,814 records...\n",
            "  Processed 2,000,000/5,587,814 records...\n",
            "  Processed 2,000,000/5,587,814 records...\n",
            "  Processed 2,100,000/5,587,814 records...\n",
            "  Processed 2,100,000/5,587,814 records...\n",
            "  Processed 2,200,000/5,587,814 records...\n",
            "  Processed 2,200,000/5,587,814 records...\n",
            "  Processed 2,300,000/5,587,814 records...\n",
            "  Processed 2,300,000/5,587,814 records...\n",
            "  Processed 2,400,000/5,587,814 records...\n",
            "  Processed 2,400,000/5,587,814 records...\n",
            "  Processed 2,500,000/5,587,814 records...\n",
            "  Processed 2,500,000/5,587,814 records...\n",
            "  Processed 2,600,000/5,587,814 records...\n",
            "  Processed 2,600,000/5,587,814 records...\n",
            "  Processed 2,700,000/5,587,814 records...\n",
            "  Processed 2,700,000/5,587,814 records...\n",
            "  Processed 2,800,000/5,587,814 records...\n",
            "  Processed 2,800,000/5,587,814 records...\n",
            "  Processed 2,900,000/5,587,814 records...\n",
            "  Processed 2,900,000/5,587,814 records...\n",
            "  Processed 3,000,000/5,587,814 records...\n",
            "  Processed 3,000,000/5,587,814 records...\n",
            "  Processed 3,100,000/5,587,814 records...\n",
            "  Processed 3,100,000/5,587,814 records...\n",
            "  Processed 3,200,000/5,587,814 records...\n",
            "  Processed 3,200,000/5,587,814 records...\n",
            "  Processed 3,300,000/5,587,814 records...\n",
            "  Processed 3,300,000/5,587,814 records...\n",
            "  Processed 3,400,000/5,587,814 records...\n",
            "  Processed 3,400,000/5,587,814 records...\n",
            "  Processed 3,500,000/5,587,814 records...\n",
            "  Processed 3,500,000/5,587,814 records...\n",
            "  Processed 3,600,000/5,587,814 records...\n",
            "  Processed 3,600,000/5,587,814 records...\n",
            "  Processed 3,700,000/5,587,814 records...\n",
            "  Processed 3,700,000/5,587,814 records...\n",
            "  Processed 3,800,000/5,587,814 records...\n",
            "  Processed 3,800,000/5,587,814 records...\n",
            "  Processed 3,900,000/5,587,814 records...\n",
            "  Processed 3,900,000/5,587,814 records...\n",
            "  Processed 4,000,000/5,587,814 records...\n",
            "  Processed 4,000,000/5,587,814 records...\n",
            "  Processed 4,100,000/5,587,814 records...\n",
            "  Processed 4,100,000/5,587,814 records...\n",
            "  Processed 4,200,000/5,587,814 records...\n",
            "  Processed 4,200,000/5,587,814 records...\n",
            "  Processed 4,300,000/5,587,814 records...\n",
            "  Processed 4,300,000/5,587,814 records...\n",
            "  Processed 4,400,000/5,587,814 records...\n",
            "  Processed 4,400,000/5,587,814 records...\n",
            "  Processed 4,500,000/5,587,814 records...\n",
            "  Processed 4,500,000/5,587,814 records...\n",
            "  Processed 4,600,000/5,587,814 records...\n",
            "  Processed 4,600,000/5,587,814 records...\n",
            "  Processed 4,700,000/5,587,814 records...\n",
            "  Processed 4,700,000/5,587,814 records...\n",
            "  Processed 4,800,000/5,587,814 records...\n",
            "  Processed 4,800,000/5,587,814 records...\n",
            "  Processed 4,900,000/5,587,814 records...\n",
            "  Processed 4,900,000/5,587,814 records...\n",
            "  Processed 5,000,000/5,587,814 records...\n",
            "  Processed 5,000,000/5,587,814 records...\n",
            "  Processed 5,100,000/5,587,814 records...\n",
            "  Processed 5,100,000/5,587,814 records...\n",
            "  Processed 5,200,000/5,587,814 records...\n",
            "  Processed 5,200,000/5,587,814 records...\n",
            "  Processed 5,300,000/5,587,814 records...\n",
            "  Processed 5,300,000/5,587,814 records...\n",
            "  Processed 5,400,000/5,587,814 records...\n",
            "  Processed 5,400,000/5,587,814 records...\n",
            "  Processed 5,500,000/5,587,814 records...\n",
            "  Processed 5,500,000/5,587,814 records...\n",
            "[ready] All functions loaded. Ready to process datasets...\n",
            "[ready] All functions loaded. Ready to process datasets...\n"
          ]
        }
      ],
      "source": [
        "# Load existing processed data efficiently to avoid memory issues\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "import gc\n",
        "\n",
        "print(\"[setup] Loading existing artifacts...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, use_fast=True)\n",
        "print(f\"[tokenizer] Loaded from {TOKENIZER_DIR}\")\n",
        "\n",
        "# Load template index (processed HDFS data with template_ids)\n",
        "template_df = pd.read_parquet(ARTIFACTS_DIR / 'drain3' / 'template_index.parquet')\n",
        "print(f\"[template] Loaded {len(template_df):,} template-indexed records\")\n",
        "\n",
        "# Load HDFS data efficiently in chunks to avoid memory issues\n",
        "def load_hdfs_efficiently():\n",
        "    \"\"\"Load only the essential HDFS data we need for tokenization\"\"\"\n",
        "    print(\"[hdfs] Loading HDFS data efficiently...\")\n",
        "    \n",
        "    # We already have processed template data, so we just need the basic structure\n",
        "    hdfs_essential = template_df[['timestamp', 'template_id', 'content']].copy()\n",
        "    \n",
        "    # Add normalized content (reapply normalization if needed)\n",
        "    print(\"[hdfs] Applying normalization...\")\n",
        "    \n",
        "    # Load normalizer rules\n",
        "    normalizer_rules = data_config['normalizer']['rules']\n",
        "    import re\n",
        "    \n",
        "    class LogNormalizer:\n",
        "        def __init__(self, rules):\n",
        "            self.rules = [(rule['name'], re.compile(rule['pattern']), rule['replace']) for rule in rules]\n",
        "\n",
        "        def apply(self, text: str) -> str:\n",
        "            result = text\n",
        "            for _, pattern, repl in self.rules:\n",
        "                result = pattern.sub(repl, result)\n",
        "            return result\n",
        "\n",
        "    normalizer = LogNormalizer(normalizer_rules)\n",
        "    \n",
        "    # Apply normalization in chunks to avoid memory issues\n",
        "    chunk_size = 10000\n",
        "    normalized_chunks = []\n",
        "    \n",
        "    for i in range(0, len(hdfs_essential), chunk_size):\n",
        "        chunk = hdfs_essential.iloc[i:i+chunk_size]\n",
        "        chunk_norm = chunk['content'].apply(normalizer.apply)\n",
        "        normalized_chunks.append(chunk_norm)\n",
        "        \n",
        "        if (i // chunk_size + 1) % 10 == 0:\n",
        "            print(f\"  Processed {i+len(chunk):,}/{len(hdfs_essential):,} records...\")\n",
        "    \n",
        "    hdfs_essential['normalized'] = pd.concat(normalized_chunks, ignore_index=True)\n",
        "    return hdfs_essential\n",
        "\n",
        "hdfs_df = load_hdfs_efficiently()\n",
        "\n",
        "def tokenize_dataframe_efficient(df: pd.DataFrame, max_length: int, batch_size: int = 1000) -> Tuple[pd.DataFrame, float]:\n",
        "    \"\"\"Memory-efficient tokenization with batching\"\"\"\n",
        "    print(f\"[tokenizer] Processing {len(df):,} records in batches of {batch_size:,}\")\n",
        "    \n",
        "    all_input_ids = []\n",
        "    all_attention_masks = []\n",
        "    truncated_count = 0\n",
        "    \n",
        "    # Process in batches to avoid memory issues\n",
        "    for i in range(0, len(df), batch_size):\n",
        "        batch_df = df.iloc[i:i+batch_size]\n",
        "        batch_texts = list(batch_df['normalized'])\n",
        "        \n",
        "        # Tokenize batch\n",
        "        encodings = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=False,\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "        \n",
        "        # Count truncated sequences in this batch\n",
        "        batch_truncated = sum(len(ids) == max_length for ids in encodings['input_ids'])\n",
        "        truncated_count += batch_truncated\n",
        "        \n",
        "        # Collect results\n",
        "        all_input_ids.extend(encodings['input_ids'])\n",
        "        all_attention_masks.extend(encodings['attention_mask'])\n",
        "        \n",
        "        # Progress update\n",
        "        if (i // batch_size + 1) % 10 == 0:\n",
        "            print(f\"  Tokenized {i+len(batch_df):,}/{len(df):,} records...\")\n",
        "        \n",
        "        # Clean up batch to free memory\n",
        "        del encodings, batch_texts\n",
        "        gc.collect()\n",
        "    \n",
        "    # Create result DataFrame\n",
        "    tokens = pd.DataFrame({\n",
        "        'input_ids': all_input_ids,\n",
        "        'attention_mask': all_attention_masks,\n",
        "        'labels': all_input_ids.copy(),  # For MLM, labels = input_ids\n",
        "    })\n",
        "    \n",
        "    # Add template_id if available\n",
        "    if 'template_id' in df.columns:\n",
        "        tokens['template_id'] = df['template_id'].values\n",
        "    else:\n",
        "        tokens['template_id'] = ''  # Empty string for datasets without templates\n",
        "    \n",
        "    # Add other columns\n",
        "    if 'label' in df.columns:\n",
        "        tokens['anomaly_label'] = df['label'].astype(int).values\n",
        "    else:\n",
        "        tokens['anomaly_label'] = 0\n",
        "    \n",
        "    tokens['timestamp'] = df['timestamp'].values\n",
        "    tokens['raw'] = df['content'].values\n",
        "    tokens['normalized'] = df['normalized'].values\n",
        "    \n",
        "    trunc_rate = truncated_count / max(len(df), 1)\n",
        "    print(f\"[tokenizer] Completed. Truncation rate: {trunc_rate:.4f}\")\n",
        "    \n",
        "    return tokens, trunc_rate\n",
        "\n",
        "def time_splits(df: pd.DataFrame, splits_cfg: Dict[str, float]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Create time-based splits\"\"\"\n",
        "    df_sorted = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    n = len(df_sorted)\n",
        "    train_end = int(n * splits_cfg['train'])\n",
        "    val_end = train_end + int(n * splits_cfg['val'])\n",
        "    return {\n",
        "        'train': df_sorted.iloc[:train_end],\n",
        "        'val': df_sorted.iloc[train_end:val_end],\n",
        "        'test': df_sorted.iloc[val_end:]\n",
        "    }\n",
        "\n",
        "print(\"[ready] All functions loaded. Ready to process datasets...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d29a1aaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[splits] Creating HDFS time-based splits...\n",
            "[splits] Created splits:\n",
            "  train: 4,470,251 records (80.0%)\n",
            "  val: 558,781 records (10.0%)\n",
            "  test: 558,782 records (10.0%)\n",
            "\n",
            "[processing] Starting train split (4,470,251 records)...\n",
            "[tokenizer] Processing 4,470,251 records in batches of 2,000\n",
            "  Tokenized 20,000/4,470,251 records...\n",
            "  Tokenized 40,000/4,470,251 records...\n",
            "  Tokenized 60,000/4,470,251 records...\n",
            "  Tokenized 80,000/4,470,251 records...\n",
            "  Tokenized 100,000/4,470,251 records...\n",
            "  Tokenized 120,000/4,470,251 records...\n",
            "  Tokenized 140,000/4,470,251 records...\n",
            "  Tokenized 160,000/4,470,251 records...\n",
            "  Tokenized 180,000/4,470,251 records...\n",
            "  Tokenized 200,000/4,470,251 records...\n",
            "  Tokenized 220,000/4,470,251 records...\n",
            "  Tokenized 240,000/4,470,251 records...\n",
            "  Tokenized 260,000/4,470,251 records...\n",
            "  Tokenized 280,000/4,470,251 records...\n",
            "  Tokenized 300,000/4,470,251 records...\n",
            "  Tokenized 320,000/4,470,251 records...\n",
            "  Tokenized 340,000/4,470,251 records...\n",
            "  Tokenized 360,000/4,470,251 records...\n",
            "  Tokenized 380,000/4,470,251 records...\n",
            "  Tokenized 400,000/4,470,251 records...\n",
            "  Tokenized 420,000/4,470,251 records...\n",
            "  Tokenized 440,000/4,470,251 records...\n",
            "  Tokenized 460,000/4,470,251 records...\n",
            "  Tokenized 480,000/4,470,251 records...\n",
            "  Tokenized 500,000/4,470,251 records...\n",
            "  Tokenized 520,000/4,470,251 records...\n",
            "  Tokenized 540,000/4,470,251 records...\n",
            "  Tokenized 560,000/4,470,251 records...\n",
            "  Tokenized 580,000/4,470,251 records...\n",
            "  Tokenized 600,000/4,470,251 records...\n",
            "  Tokenized 620,000/4,470,251 records...\n",
            "  Tokenized 640,000/4,470,251 records...\n",
            "  Tokenized 660,000/4,470,251 records...\n",
            "  Tokenized 680,000/4,470,251 records...\n",
            "  Tokenized 700,000/4,470,251 records...\n",
            "  Tokenized 720,000/4,470,251 records...\n",
            "  Tokenized 740,000/4,470,251 records...\n",
            "  Tokenized 760,000/4,470,251 records...\n",
            "  Tokenized 780,000/4,470,251 records...\n",
            "  Tokenized 800,000/4,470,251 records...\n",
            "  Tokenized 820,000/4,470,251 records...\n",
            "  Tokenized 840,000/4,470,251 records...\n",
            "  Tokenized 860,000/4,470,251 records...\n",
            "  Tokenized 880,000/4,470,251 records...\n",
            "  Tokenized 900,000/4,470,251 records...\n",
            "  Tokenized 920,000/4,470,251 records...\n",
            "  Tokenized 940,000/4,470,251 records...\n",
            "  Tokenized 960,000/4,470,251 records...\n",
            "  Tokenized 980,000/4,470,251 records...\n",
            "  Tokenized 1,000,000/4,470,251 records...\n",
            "  Tokenized 1,020,000/4,470,251 records...\n",
            "  Tokenized 1,040,000/4,470,251 records...\n",
            "  Tokenized 1,060,000/4,470,251 records...\n",
            "  Tokenized 1,080,000/4,470,251 records...\n",
            "  Tokenized 1,100,000/4,470,251 records...\n",
            "  Tokenized 1,120,000/4,470,251 records...\n",
            "  Tokenized 1,140,000/4,470,251 records...\n",
            "  Tokenized 1,160,000/4,470,251 records...\n",
            "  Tokenized 1,180,000/4,470,251 records...\n",
            "  Tokenized 1,200,000/4,470,251 records...\n",
            "  Tokenized 1,220,000/4,470,251 records...\n",
            "  Tokenized 1,240,000/4,470,251 records...\n",
            "  Tokenized 1,260,000/4,470,251 records...\n",
            "  Tokenized 1,280,000/4,470,251 records...\n",
            "  Tokenized 1,300,000/4,470,251 records...\n",
            "  Tokenized 1,320,000/4,470,251 records...\n",
            "  Tokenized 1,340,000/4,470,251 records...\n",
            "  Tokenized 1,360,000/4,470,251 records...\n",
            "  Tokenized 1,380,000/4,470,251 records...\n",
            "  Tokenized 1,400,000/4,470,251 records...\n",
            "  Tokenized 1,420,000/4,470,251 records...\n",
            "  Tokenized 1,440,000/4,470,251 records...\n",
            "  Tokenized 1,460,000/4,470,251 records...\n",
            "  Tokenized 1,480,000/4,470,251 records...\n",
            "  Tokenized 1,500,000/4,470,251 records...\n",
            "  Tokenized 1,520,000/4,470,251 records...\n",
            "  Tokenized 1,540,000/4,470,251 records...\n",
            "  Tokenized 1,560,000/4,470,251 records...\n",
            "  Tokenized 1,580,000/4,470,251 records...\n",
            "  Tokenized 1,600,000/4,470,251 records...\n",
            "  Tokenized 1,620,000/4,470,251 records...\n",
            "  Tokenized 1,640,000/4,470,251 records...\n",
            "  Tokenized 1,660,000/4,470,251 records...\n",
            "  Tokenized 1,680,000/4,470,251 records...\n",
            "  Tokenized 1,700,000/4,470,251 records...\n",
            "  Tokenized 1,720,000/4,470,251 records...\n",
            "  Tokenized 1,740,000/4,470,251 records...\n",
            "  Tokenized 1,760,000/4,470,251 records...\n",
            "  Tokenized 1,780,000/4,470,251 records...\n",
            "  Tokenized 1,800,000/4,470,251 records...\n",
            "  Tokenized 1,820,000/4,470,251 records...\n",
            "  Tokenized 1,840,000/4,470,251 records...\n",
            "  Tokenized 1,860,000/4,470,251 records...\n",
            "  Tokenized 1,880,000/4,470,251 records...\n",
            "  Tokenized 1,900,000/4,470,251 records...\n",
            "  Tokenized 1,920,000/4,470,251 records...\n",
            "  Tokenized 1,940,000/4,470,251 records...\n",
            "  Tokenized 1,960,000/4,470,251 records...\n",
            "  Tokenized 1,980,000/4,470,251 records...\n",
            "  Tokenized 2,000,000/4,470,251 records...\n",
            "  Tokenized 2,020,000/4,470,251 records...\n",
            "  Tokenized 2,040,000/4,470,251 records...\n",
            "  Tokenized 2,060,000/4,470,251 records...\n",
            "  Tokenized 2,080,000/4,470,251 records...\n",
            "  Tokenized 2,100,000/4,470,251 records...\n",
            "  Tokenized 2,120,000/4,470,251 records...\n",
            "  Tokenized 2,140,000/4,470,251 records...\n",
            "  Tokenized 2,160,000/4,470,251 records...\n",
            "  Tokenized 2,180,000/4,470,251 records...\n",
            "  Tokenized 2,200,000/4,470,251 records...\n",
            "  Tokenized 2,220,000/4,470,251 records...\n",
            "  Tokenized 2,240,000/4,470,251 records...\n",
            "  Tokenized 2,260,000/4,470,251 records...\n",
            "  Tokenized 2,280,000/4,470,251 records...\n",
            "  Tokenized 2,300,000/4,470,251 records...\n",
            "  Tokenized 2,320,000/4,470,251 records...\n",
            "  Tokenized 2,340,000/4,470,251 records...\n",
            "  Tokenized 2,360,000/4,470,251 records...\n",
            "  Tokenized 2,380,000/4,470,251 records...\n",
            "  Tokenized 2,400,000/4,470,251 records...\n",
            "  Tokenized 2,420,000/4,470,251 records...\n",
            "  Tokenized 2,440,000/4,470,251 records...\n",
            "  Tokenized 2,460,000/4,470,251 records...\n",
            "  Tokenized 2,480,000/4,470,251 records...\n",
            "  Tokenized 2,500,000/4,470,251 records...\n",
            "  Tokenized 2,520,000/4,470,251 records...\n",
            "  Tokenized 2,540,000/4,470,251 records...\n",
            "  Tokenized 2,560,000/4,470,251 records...\n",
            "  Tokenized 2,580,000/4,470,251 records...\n",
            "  Tokenized 2,600,000/4,470,251 records...\n",
            "  Tokenized 2,620,000/4,470,251 records...\n",
            "  Tokenized 2,640,000/4,470,251 records...\n",
            "  Tokenized 2,660,000/4,470,251 records...\n",
            "  Tokenized 2,680,000/4,470,251 records...\n",
            "  Tokenized 2,700,000/4,470,251 records...\n",
            "  Tokenized 2,720,000/4,470,251 records...\n",
            "  Tokenized 2,740,000/4,470,251 records...\n",
            "  Tokenized 2,760,000/4,470,251 records...\n",
            "  Tokenized 2,780,000/4,470,251 records...\n",
            "  Tokenized 2,800,000/4,470,251 records...\n",
            "  Tokenized 2,820,000/4,470,251 records...\n",
            "  Tokenized 2,840,000/4,470,251 records...\n",
            "  Tokenized 2,860,000/4,470,251 records...\n",
            "  Tokenized 2,880,000/4,470,251 records...\n",
            "  Tokenized 2,900,000/4,470,251 records...\n",
            "  Tokenized 2,920,000/4,470,251 records...\n",
            "  Tokenized 2,940,000/4,470,251 records...\n",
            "  Tokenized 2,960,000/4,470,251 records...\n",
            "  Tokenized 2,980,000/4,470,251 records...\n",
            "  Tokenized 3,000,000/4,470,251 records...\n",
            "  Tokenized 3,020,000/4,470,251 records...\n",
            "  Tokenized 3,040,000/4,470,251 records...\n",
            "  Tokenized 3,060,000/4,470,251 records...\n",
            "  Tokenized 3,080,000/4,470,251 records...\n",
            "  Tokenized 3,100,000/4,470,251 records...\n",
            "  Tokenized 3,120,000/4,470,251 records...\n",
            "  Tokenized 3,140,000/4,470,251 records...\n",
            "  Tokenized 3,160,000/4,470,251 records...\n",
            "  Tokenized 3,180,000/4,470,251 records...\n",
            "  Tokenized 3,200,000/4,470,251 records...\n",
            "  Tokenized 3,220,000/4,470,251 records...\n",
            "  Tokenized 3,240,000/4,470,251 records...\n",
            "  Tokenized 3,260,000/4,470,251 records...\n",
            "  Tokenized 3,280,000/4,470,251 records...\n",
            "  Tokenized 3,300,000/4,470,251 records...\n",
            "  Tokenized 3,320,000/4,470,251 records...\n",
            "  Tokenized 3,340,000/4,470,251 records...\n",
            "  Tokenized 3,360,000/4,470,251 records...\n",
            "  Tokenized 3,380,000/4,470,251 records...\n",
            "  Tokenized 3,400,000/4,470,251 records...\n",
            "  Tokenized 3,420,000/4,470,251 records...\n",
            "  Tokenized 3,440,000/4,470,251 records...\n",
            "  Tokenized 3,460,000/4,470,251 records...\n",
            "  Tokenized 3,480,000/4,470,251 records...\n",
            "  Tokenized 3,500,000/4,470,251 records...\n",
            "  Tokenized 3,520,000/4,470,251 records...\n",
            "  Tokenized 3,540,000/4,470,251 records...\n",
            "  Tokenized 3,560,000/4,470,251 records...\n",
            "  Tokenized 3,580,000/4,470,251 records...\n",
            "  Tokenized 3,600,000/4,470,251 records...\n",
            "  Tokenized 3,620,000/4,470,251 records...\n",
            "  Tokenized 3,640,000/4,470,251 records...\n",
            "  Tokenized 3,660,000/4,470,251 records...\n",
            "  Tokenized 3,680,000/4,470,251 records...\n",
            "  Tokenized 3,700,000/4,470,251 records...\n",
            "  Tokenized 3,720,000/4,470,251 records...\n",
            "  Tokenized 3,740,000/4,470,251 records...\n",
            "  Tokenized 3,760,000/4,470,251 records...\n",
            "  Tokenized 3,780,000/4,470,251 records...\n",
            "  Tokenized 3,800,000/4,470,251 records...\n",
            "  Tokenized 3,820,000/4,470,251 records...\n",
            "  Tokenized 3,840,000/4,470,251 records...\n",
            "  Tokenized 3,860,000/4,470,251 records...\n",
            "  Tokenized 3,880,000/4,470,251 records...\n",
            "  Tokenized 3,900,000/4,470,251 records...\n",
            "  Tokenized 3,920,000/4,470,251 records...\n",
            "  Tokenized 3,940,000/4,470,251 records...\n",
            "  Tokenized 3,960,000/4,470,251 records...\n",
            "  Tokenized 3,980,000/4,470,251 records...\n",
            "  Tokenized 4,000,000/4,470,251 records...\n",
            "  Tokenized 4,020,000/4,470,251 records...\n",
            "  Tokenized 4,040,000/4,470,251 records...\n",
            "  Tokenized 4,060,000/4,470,251 records...\n",
            "  Tokenized 4,080,000/4,470,251 records...\n",
            "  Tokenized 4,100,000/4,470,251 records...\n",
            "  Tokenized 4,120,000/4,470,251 records...\n",
            "  Tokenized 4,140,000/4,470,251 records...\n",
            "  Tokenized 4,160,000/4,470,251 records...\n",
            "  Tokenized 4,180,000/4,470,251 records...\n",
            "  Tokenized 4,200,000/4,470,251 records...\n",
            "  Tokenized 4,220,000/4,470,251 records...\n",
            "  Tokenized 4,240,000/4,470,251 records...\n",
            "  Tokenized 4,260,000/4,470,251 records...\n",
            "  Tokenized 4,280,000/4,470,251 records...\n",
            "  Tokenized 4,300,000/4,470,251 records...\n",
            "  Tokenized 4,320,000/4,470,251 records...\n",
            "  Tokenized 4,340,000/4,470,251 records...\n",
            "  Tokenized 4,360,000/4,470,251 records...\n",
            "  Tokenized 4,380,000/4,470,251 records...\n",
            "  Tokenized 4,400,000/4,470,251 records...\n",
            "  Tokenized 4,420,000/4,470,251 records...\n",
            "  Tokenized 4,440,000/4,470,251 records...\n",
            "  Tokenized 4,460,000/4,470,251 records...\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[save] Parquet saved: artifacts/datasets/hdfs_train.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ef7e08bf7bd40b1b596c6df115d4561",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/10 shards):   0%|          | 0/4470251 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[save] HuggingFace dataset saved: artifacts/datasets/hdfs_train_hf\n",
            "[stats] train: {'count': 4470251, 'avg_length': 44.4681468669209, 'truncation_rate': 0.0}\n",
            "\n",
            "[processing] Starting val split (558,781 records)...\n",
            "[tokenizer] Processing 558,781 records in batches of 2,000\n",
            "  Tokenized 20,000/558,781 records...\n",
            "  Tokenized 40,000/558,781 records...\n",
            "  Tokenized 60,000/558,781 records...\n",
            "  Tokenized 80,000/558,781 records...\n",
            "  Tokenized 100,000/558,781 records...\n",
            "  Tokenized 120,000/558,781 records...\n",
            "  Tokenized 140,000/558,781 records...\n",
            "  Tokenized 160,000/558,781 records...\n",
            "  Tokenized 180,000/558,781 records...\n",
            "  Tokenized 200,000/558,781 records...\n",
            "  Tokenized 220,000/558,781 records...\n",
            "  Tokenized 240,000/558,781 records...\n",
            "  Tokenized 260,000/558,781 records...\n",
            "  Tokenized 280,000/558,781 records...\n",
            "  Tokenized 300,000/558,781 records...\n",
            "  Tokenized 320,000/558,781 records...\n",
            "  Tokenized 340,000/558,781 records...\n",
            "  Tokenized 360,000/558,781 records...\n",
            "  Tokenized 380,000/558,781 records...\n",
            "  Tokenized 400,000/558,781 records...\n",
            "  Tokenized 420,000/558,781 records...\n",
            "  Tokenized 440,000/558,781 records...\n",
            "  Tokenized 460,000/558,781 records...\n",
            "  Tokenized 480,000/558,781 records...\n",
            "  Tokenized 500,000/558,781 records...\n",
            "  Tokenized 520,000/558,781 records...\n",
            "  Tokenized 540,000/558,781 records...\n",
            "  Tokenized 558,781/558,781 records...\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[save] Parquet saved: artifacts/datasets/hdfs_val.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8979b8036bf64e739c752ff036765192",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/2 shards):   0%|          | 0/558781 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[save] HuggingFace dataset saved: artifacts/datasets/hdfs_val_hf\n",
            "[stats] val: {'count': 558781, 'avg_length': 46.11112940490103, 'truncation_rate': 0.0}\n",
            "\n",
            "[processing] Starting test split (558,782 records)...\n",
            "[tokenizer] Processing 558,782 records in batches of 2,000\n",
            "  Tokenized 20,000/558,782 records...\n",
            "  Tokenized 40,000/558,782 records...\n",
            "  Tokenized 60,000/558,782 records...\n",
            "  Tokenized 80,000/558,782 records...\n",
            "  Tokenized 100,000/558,782 records...\n",
            "  Tokenized 120,000/558,782 records...\n",
            "  Tokenized 140,000/558,782 records...\n",
            "  Tokenized 160,000/558,782 records...\n",
            "  Tokenized 180,000/558,782 records...\n",
            "  Tokenized 200,000/558,782 records...\n",
            "  Tokenized 220,000/558,782 records...\n",
            "  Tokenized 240,000/558,782 records...\n",
            "  Tokenized 260,000/558,782 records...\n",
            "  Tokenized 280,000/558,782 records...\n",
            "  Tokenized 300,000/558,782 records...\n",
            "  Tokenized 320,000/558,782 records...\n",
            "  Tokenized 340,000/558,782 records...\n",
            "  Tokenized 360,000/558,782 records...\n",
            "  Tokenized 380,000/558,782 records...\n",
            "  Tokenized 400,000/558,782 records...\n",
            "  Tokenized 420,000/558,782 records...\n",
            "  Tokenized 440,000/558,782 records...\n",
            "  Tokenized 460,000/558,782 records...\n",
            "  Tokenized 480,000/558,782 records...\n",
            "  Tokenized 500,000/558,782 records...\n",
            "  Tokenized 520,000/558,782 records...\n",
            "  Tokenized 540,000/558,782 records...\n",
            "  Tokenized 558,782/558,782 records...\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[save] Parquet saved: artifacts/datasets/hdfs_test.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "361e59a5eee84fce8074f1f65f5fa107",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/2 shards):   0%|          | 0/558782 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[save] HuggingFace dataset saved: artifacts/datasets/hdfs_test_hf\n",
            "[stats] test: {'count': 558782, 'avg_length': 44.91538918576475, 'truncation_rate': 0.0}\n",
            "\n",
            "[complete] All HDFS splits processed successfully!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>avg_length</th>\n",
              "      <th>truncation_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>train</th>\n",
              "      <td>4470251.0</td>\n",
              "      <td>44.468147</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>558781.0</td>\n",
              "      <td>46.111129</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>558782.0</td>\n",
              "      <td>44.915389</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           count  avg_length  truncation_rate\n",
              "train  4470251.0   44.468147              0.0\n",
              "val     558781.0   46.111129              0.0\n",
              "test    558782.0   44.915389              0.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create HDFS dataset splits with memory-efficient processing\n",
        "print(\"[splits] Creating HDFS time-based splits...\")\n",
        "\n",
        "# Create time-based splits\n",
        "splits_cfg = data_config['splits']\n",
        "hdfs_splits = time_splits(hdfs_df, splits_cfg)\n",
        "\n",
        "print(f\"[splits] Created splits:\")\n",
        "for name, split_df in hdfs_splits.items():\n",
        "    print(f\"  {name}: {len(split_df):,} records ({len(split_df)/len(hdfs_df)*100:.1f}%)\")\n",
        "\n",
        "# Process each split separately to avoid memory issues\n",
        "hdfs_stats = {}\n",
        "max_length = data_config['tokenizer']['max_length']\n",
        "batch_size = 2000  # Smaller batch size for memory efficiency\n",
        "\n",
        "for split_name, split_df in hdfs_splits.items():\n",
        "    print(f\"\\n[processing] Starting {split_name} split ({len(split_df):,} records)...\")\n",
        "    \n",
        "    # Tokenize with batching\n",
        "    tokens_df, trunc = tokenize_dataframe_efficient(split_df, max_length, batch_size)\n",
        "    \n",
        "    # Save Parquet file\n",
        "    file_path = PARQUET_DIR / f'hdfs_{split_name}.parquet'\n",
        "    tokens_df.to_parquet(file_path, index=False)\n",
        "    print(f\"[save] Parquet saved: {file_path}\")\n",
        "    \n",
        "    # Create HuggingFace dataset (remove large text columns to save memory)\n",
        "    hf_df = tokens_df.drop(columns=['raw', 'normalized']).copy()\n",
        "    ds = Dataset.from_pandas(hf_df, preserve_index=False)\n",
        "    hf_path = str(PARQUET_DIR / f'hdfs_{split_name}_hf')\n",
        "    ds.save_to_disk(hf_path)\n",
        "    print(f\"[save] HuggingFace dataset saved: {hf_path}\")\n",
        "    \n",
        "    # Collect statistics\n",
        "    hdfs_stats[split_name] = {\n",
        "        'count': len(tokens_df),\n",
        "        'avg_length': float(tokens_df['input_ids'].map(len).mean()),\n",
        "        'truncation_rate': round(trunc, 4)\n",
        "    }\n",
        "    \n",
        "    print(f\"[stats] {split_name}: {hdfs_stats[split_name]}\")\n",
        "    \n",
        "    # Clean up to free memory\n",
        "    del tokens_df, hf_df, ds, split_df\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\n[complete] All HDFS splits processed successfully!\")\n",
        "display(pd.DataFrame(hdfs_stats).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4374deaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[openstack] Loading OpenStack logs...\n",
            "[openstack] Reading normal logs from data/openstack/raw/openstack_normal.log...\n",
            "[openstack] normal: 189,386 valid records\n",
            "[openstack] Reading abnormal logs from data/openstack/raw/openstack_abnormal.log...\n",
            "[openstack] abnormal: 18,434 valid records\n",
            "[openstack] Total: 207,820 records\n",
            "[openstack] Applying normalization...\n",
            "[openstack] normal: 189,386 valid records\n",
            "[openstack] Reading abnormal logs from data/openstack/raw/openstack_abnormal.log...\n",
            "[openstack] abnormal: 18,434 valid records\n",
            "[openstack] Total: 207,820 records\n",
            "[openstack] Applying normalization...\n",
            "[openstack] Creating time-based splits...\n",
            "[openstack] Split sizes:\n",
            "  train: 166,256 records (80.0%)\n",
            "  val: 20,782 records (10.0%)\n",
            "  test: 20,782 records (10.0%)\n",
            "\n",
            "[processing] Starting OpenStack train split (166,256 records)...\n",
            "[tokenizer] Processing 166,256 records in batches of 2,000\n",
            "[openstack] Creating time-based splits...\n",
            "[openstack] Split sizes:\n",
            "  train: 166,256 records (80.0%)\n",
            "  val: 20,782 records (10.0%)\n",
            "  test: 20,782 records (10.0%)\n",
            "\n",
            "[processing] Starting OpenStack train split (166,256 records)...\n",
            "[tokenizer] Processing 166,256 records in batches of 2,000\n",
            "  Tokenized 20,000/166,256 records...\n",
            "  Tokenized 20,000/166,256 records...\n",
            "  Tokenized 40,000/166,256 records...\n",
            "  Tokenized 40,000/166,256 records...\n",
            "  Tokenized 60,000/166,256 records...\n",
            "  Tokenized 60,000/166,256 records...\n",
            "  Tokenized 80,000/166,256 records...\n",
            "  Tokenized 80,000/166,256 records...\n",
            "  Tokenized 100,000/166,256 records...\n",
            "  Tokenized 100,000/166,256 records...\n",
            "  Tokenized 120,000/166,256 records...\n",
            "  Tokenized 120,000/166,256 records...\n",
            "  Tokenized 140,000/166,256 records...\n",
            "  Tokenized 140,000/166,256 records...\n",
            "  Tokenized 160,000/166,256 records...\n",
            "  Tokenized 160,000/166,256 records...\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[save] Parquet saved: artifacts/datasets/openstack_train.parquet\n",
            "[save] Parquet saved: artifacts/datasets/openstack_train.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1c25b456eef43fd8710ea0a7e58f50f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/166256 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[save] HuggingFace dataset saved: artifacts/datasets/openstack_train_hf\n",
            "[stats] train: {'count': 166256, 'avg_length': 81.68753007410258, 'truncation_rate': 0.0, 'anomaly_rate': 0.1108772014243095}\n",
            "\n",
            "[processing] Starting OpenStack val split (20,782 records)...\n",
            "[tokenizer] Processing 20,782 records in batches of 2,000\n",
            "\n",
            "[processing] Starting OpenStack val split (20,782 records)...\n",
            "[tokenizer] Processing 20,782 records in batches of 2,000\n",
            "  Tokenized 20,000/20,782 records...\n",
            "  Tokenized 20,000/20,782 records...\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[save] Parquet saved: artifacts/datasets/openstack_val.parquet\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[save] Parquet saved: artifacts/datasets/openstack_val.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e27d62efe63416d847b218a0b280e25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/20782 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[save] HuggingFace dataset saved: artifacts/datasets/openstack_val_hf\n",
            "[stats] val: {'count': 20782, 'avg_length': 81.9116543162352, 'truncation_rate': 0.0, 'anomaly_rate': 0.0}\n",
            "\n",
            "[processing] Starting OpenStack test split (20,782 records)...\n",
            "[tokenizer] Processing 20,782 records in batches of 2,000\n",
            "\n",
            "[processing] Starting OpenStack test split (20,782 records)...\n",
            "[tokenizer] Processing 20,782 records in batches of 2,000\n",
            "  Tokenized 20,000/20,782 records...\n",
            "  Tokenized 20,000/20,782 records...\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[save] Parquet saved: artifacts/datasets/openstack_test.parquet\n",
            "[tokenizer] Completed. Truncation rate: 0.0000\n",
            "[save] Parquet saved: artifacts/datasets/openstack_test.parquet\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c49cc1373f654cf5ad10e47cdfb9fcc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/20782 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[save] HuggingFace dataset saved: artifacts/datasets/openstack_test_hf\n",
            "[stats] test: {'count': 20782, 'avg_length': 83.08338947165817, 'truncation_rate': 0.0, 'anomaly_rate': 0.0}\n",
            "\n",
            "[complete] All OpenStack splits processed successfully!\n",
            "\n",
            "[complete] All OpenStack splits processed successfully!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>avg_length</th>\n",
              "      <th>truncation_rate</th>\n",
              "      <th>anomaly_rate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>train</th>\n",
              "      <td>166256.0</td>\n",
              "      <td>81.687530</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.110877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>20782.0</td>\n",
              "      <td>81.911654</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>20782.0</td>\n",
              "      <td>83.083389</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          count  avg_length  truncation_rate  anomaly_rate\n",
              "train  166256.0   81.687530              0.0      0.110877\n",
              "val     20782.0   81.911654              0.0      0.000000\n",
              "test    20782.0   83.083389              0.0      0.000000"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load and process OpenStack data efficiently\n",
        "print(\"[openstack] Loading OpenStack logs...\")\n",
        "\n",
        "# Load OpenStack data\n",
        "def read_openstack_logs() -> pd.DataFrame:\n",
        "    \"\"\"Load OpenStack logs efficiently\"\"\"\n",
        "    # Get paths from config\n",
        "    normal_path = RAW_OPENSTACK_DIR / 'openstack_normal.log'\n",
        "    abnormal_path = RAW_OPENSTACK_DIR / 'openstack_abnormal.log'\n",
        "    \n",
        "    frames = []\n",
        "    paths = {'normal': normal_path, 'abnormal': abnormal_path}\n",
        "    \n",
        "    for label, path in paths.items():\n",
        "        print(f\"[openstack] Reading {label} logs from {path}...\")\n",
        "        \n",
        "        # Read the log file as plain text lines\n",
        "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            lines = [line.strip() for line in f if line.strip()]\n",
        "        \n",
        "        df = pd.DataFrame({'raw': lines})\n",
        "        df['timestamp'] = pd.to_datetime(df['raw'].str.extract(r'(\\d{4}-\\d{2}-\\d{2} [^ ]+)')[0], errors='coerce')\n",
        "        df['label'] = 1 if label == 'abnormal' else 0\n",
        "        df['content'] = df['raw']\n",
        "        valid_df = df.dropna(subset=['timestamp'])\n",
        "        print(f\"[openstack] {label}: {len(valid_df):,} valid records\")\n",
        "        frames.append(valid_df)\n",
        "    \n",
        "    result = pd.concat(frames, ignore_index=True).sort_values('timestamp').reset_index(drop=True)\n",
        "    print(f\"[openstack] Total: {len(result):,} records\")\n",
        "    return result\n",
        "\n",
        "# Load OpenStack data\n",
        "openstack_df = read_openstack_logs()\n",
        "\n",
        "# Apply normalization to OpenStack data\n",
        "print(\"[openstack] Applying normalization...\")\n",
        "normalizer_rules = data_config['normalizer']['rules']\n",
        "import re\n",
        "\n",
        "class LogNormalizer:\n",
        "    def __init__(self, rules):\n",
        "        self.rules = [(rule['name'], re.compile(rule['pattern']), rule['replace']) for rule in rules]\n",
        "\n",
        "    def apply(self, text: str) -> str:\n",
        "        result = text\n",
        "        for _, pattern, repl in self.rules:\n",
        "            result = pattern.sub(repl, result)\n",
        "        return result\n",
        "\n",
        "normalizer = LogNormalizer(normalizer_rules)\n",
        "openstack_df['normalized'] = openstack_df['content'].apply(normalizer.apply)\n",
        "\n",
        "# Create time-based splits\n",
        "print(\"[openstack] Creating time-based splits...\")\n",
        "openstack_splits = time_splits(openstack_df, splits_cfg)\n",
        "\n",
        "print(f\"[openstack] Split sizes:\")\n",
        "for name, split_df in openstack_splits.items():\n",
        "    print(f\"  {name}: {len(split_df):,} records ({len(split_df)/len(openstack_df)*100:.1f}%)\")\n",
        "\n",
        "# Process each split with memory-efficient tokenization\n",
        "openstack_stats = {}\n",
        "max_length = data_config['tokenizer']['max_length']\n",
        "batch_size = 2000  # Same batch size as HDFS\n",
        "\n",
        "for split_name, split_df in openstack_splits.items():\n",
        "    print(f\"\\n[processing] Starting OpenStack {split_name} split ({len(split_df):,} records)...\")\n",
        "    \n",
        "    # Tokenize with batching\n",
        "    tokens_df, trunc = tokenize_dataframe_efficient(split_df, max_length, batch_size)\n",
        "    \n",
        "    # Save Parquet file\n",
        "    file_path = PARQUET_DIR / f'openstack_{split_name}.parquet'\n",
        "    tokens_df.to_parquet(file_path, index=False)\n",
        "    print(f\"[save] Parquet saved: {file_path}\")\n",
        "    \n",
        "    # Create HuggingFace dataset\n",
        "    hf_df = tokens_df.drop(columns=['raw', 'normalized']).copy()\n",
        "    ds = Dataset.from_pandas(hf_df, preserve_index=False)\n",
        "    hf_path = str(PARQUET_DIR / f'openstack_{split_name}_hf')\n",
        "    ds.save_to_disk(hf_path)\n",
        "    print(f\"[save] HuggingFace dataset saved: {hf_path}\")\n",
        "    \n",
        "    # Collect statistics\n",
        "    openstack_stats[split_name] = {\n",
        "        'count': len(tokens_df),\n",
        "        'avg_length': float(tokens_df['input_ids'].map(len).mean()),\n",
        "        'truncation_rate': round(trunc, 4),\n",
        "        'anomaly_rate': float(tokens_df['anomaly_label'].mean()) if 'anomaly_label' in tokens_df.columns else 0.0\n",
        "    }\n",
        "    \n",
        "    print(f\"[stats] {split_name}: {openstack_stats[split_name]}\")\n",
        "    \n",
        "    # Clean up to free memory\n",
        "    del tokens_df, hf_df, ds, split_df\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\n[complete] All OpenStack splits processed successfully!\")\n",
        "display(pd.DataFrame(openstack_stats).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0acf28e5",
      "metadata": {},
      "source": [
        "## 8. Persist Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c1a84bb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[metadata] saved to artifacts/metadata/datasets.json\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "metadata = {\n",
        "    'generated_at': datetime.utcnow().isoformat() + 'Z',\n",
        "    'hdfs': hdfs_stats,\n",
        "    'openstack': openstack_stats,\n",
        "    'tokenizer_dir': str(TOKENIZER_DIR),\n",
        "    'template_index': str(Path(data_config['drain3']['transition_output']).with_name('template_index.parquet')),\n",
        "    'template_transition': str(Path(data_config['drain3']['transition_output']))\n",
        "}\n",
        "METADATA_PATH.write_text(json.dumps(metadata, indent=2))\n",
        "print(f'[metadata] saved to {METADATA_PATH}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54d5852",
      "metadata": {},
      "source": [
        "## Artifacts Produced\n",
        "- Tokenizer with special tokens -> `artifacts/tokenizer/`\n",
        "- HDFS Parquet splits and HF datasets -> `artifacts/datasets/hdfs_*.parquet`, `*_hf/`\n",
        "- OpenStack Parquet splits and HF datasets -> `artifacts/datasets/openstack_*.parquet`, `*_hf/`\n",
        "- Drain3 template index and transitions -> `artifacts/drain3/`\n",
        "- Dataset metadata summary -> `artifacts/metadata/datasets.json`\n",
        "\n",
        "Continue with notebook `01_pretrain_hdfs.ipynb` for multi-GPU MLM pretraining."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "log_anomaly",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
