{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60edb1c",
   "metadata": {},
   "source": [
    "# 00a · Fix OpenStack Splits with Stratified Sampling\n",
    "\n",
    "This notebook fixes the OpenStack dataset splits to use stratified random sampling instead of time-based splits, ensuring anomalies are distributed across training, validation, and test sets.\n",
    "\n",
    "## Problem\n",
    "- Current OpenStack dataset uses time-based splits (80/10/10)\n",
    "- All anomalies are clustered in first 2.3 hours of 55-hour dataset\n",
    "- Validation and test sets have 0% anomalies (only normal logs)\n",
    "- Makes evaluation impossible (F1=0, no ROC AUC)\n",
    "\n",
    "## Solution\n",
    "- Use stratified random sampling for OpenStack splits\n",
    "- Preserve 80/10/10 ratio but ensure each split has ~11% anomalies\n",
    "- Keep existing tokenized data, just re-split the parquet files\n",
    "- Maintain HDFS time-based splits (they work fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fe0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2347e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with parquet directory: artifacts/datasets\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "def load_yaml(path: Path) -> dict:\n",
    "    with path.open('r') as fh:\n",
    "        return yaml.safe_load(fh)\n",
    "\n",
    "data_cfg = load_yaml(Path('../configs/data.yaml'))\n",
    "parquet_dir = Path(data_cfg['preprocessing']['parquet_dir'])\n",
    "print(f\"Working with parquet directory: {parquet_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c8a994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Backing up existing OpenStack splits...\n",
      "   ✅ Backed up openstack_train.parquet\n",
      "   ✅ Backed up openstack_val.parquet\n",
      "   ✅ Backed up openstack_test.parquet\n",
      "   ✅ Backed up openstack_train_hf/\n",
      "   ✅ Backed up openstack_val_hf/\n",
      "   ✅ Backed up openstack_test_hf/\n",
      "📁 Backups saved to: artifacts/datasets/backup_temporal_splits\n"
     ]
    }
   ],
   "source": [
    "# Backup existing OpenStack splits\n",
    "import shutil\n",
    "\n",
    "backup_dir = parquet_dir / 'backup_temporal_splits'\n",
    "backup_dir.mkdir(exist_ok=True)\n",
    "\n",
    "openstack_files = [\n",
    "    'openstack_train.parquet',\n",
    "    'openstack_val.parquet', \n",
    "    'openstack_test.parquet'\n",
    "]\n",
    "\n",
    "openstack_hf_dirs = [\n",
    "    'openstack_train_hf',\n",
    "    'openstack_val_hf',\n",
    "    'openstack_test_hf'\n",
    "]\n",
    "\n",
    "print(\"🔄 Backing up existing OpenStack splits...\")\n",
    "for file in openstack_files:\n",
    "    src = parquet_dir / file\n",
    "    dst = backup_dir / file\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"   ✅ Backed up {file}\")\n",
    "        \n",
    "for dir_name in openstack_hf_dirs:\n",
    "    src = parquet_dir / dir_name\n",
    "    dst = backup_dir / dir_name\n",
    "    if src.exists():\n",
    "        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
    "        print(f\"   ✅ Backed up {dir_name}/\")\n",
    "\n",
    "print(f\"📁 Backups saved to: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cff4309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading existing OpenStack splits...\n",
      "   Training: 166,256 samples, 18,434 anomalies (11.09%)\n",
      "   Validation: 20,782 samples, 0 anomalies (0.00%)\n",
      "   Test: 20,782 samples, 0 anomalies (0.00%)\n",
      "\n",
      "📦 Combined dataset: 207,820 samples, 18,434 anomalies (8.87%)\n",
      "✅ Data integrity verified\n"
     ]
    }
   ],
   "source": [
    "# Load and combine all OpenStack data\n",
    "print(\"📊 Loading existing OpenStack splits...\")\n",
    "\n",
    "train_df = pd.read_parquet(parquet_dir / 'openstack_train.parquet')\n",
    "val_df = pd.read_parquet(parquet_dir / 'openstack_val.parquet')\n",
    "test_df = pd.read_parquet(parquet_dir / 'openstack_test.parquet')\n",
    "\n",
    "print(f\"   Training: {len(train_df):,} samples, {sum(train_df['anomaly_label']):,} anomalies ({sum(train_df['anomaly_label'])/len(train_df)*100:.2f}%)\")\n",
    "print(f\"   Validation: {len(val_df):,} samples, {sum(val_df['anomaly_label']):,} anomalies ({sum(val_df['anomaly_label'])/len(val_df)*100:.2f}%)\")\n",
    "print(f\"   Test: {len(test_df):,} samples, {sum(test_df['anomaly_label']):,} anomalies ({sum(test_df['anomaly_label'])/len(test_df)*100:.2f}%)\")\n",
    "\n",
    "# Combine all data\n",
    "combined_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "print(f\"\\n📦 Combined dataset: {len(combined_df):,} samples, {sum(combined_df['anomaly_label']):,} anomalies ({sum(combined_df['anomaly_label'])/len(combined_df)*100:.2f}%)\")\n",
    "\n",
    "# Verify data integrity\n",
    "total_expected = len(train_df) + len(val_df) + len(test_df)\n",
    "assert len(combined_df) == total_expected, f\"Data loss detected: {len(combined_df)} != {total_expected}\"\n",
    "print(\"✅ Data integrity verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f392b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Creating stratified random splits...\n",
      "\n",
      "📊 New stratified splits:\n",
      "   Training: 166,256 samples, 14,747 anomalies (8.87%)\n",
      "   Validation: 20,782 samples, 1,844 anomalies (8.87%)\n",
      "   Test: 20,782 samples, 1,843 anomalies (8.87%)\n",
      "✅ Stratified splits created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified splits\n",
    "print(\"🎯 Creating stratified random splits...\")\n",
    "\n",
    "# Split configuration (80/10/10)\n",
    "train_size = 0.8\n",
    "val_size = 0.1  \n",
    "test_size = 0.1\n",
    "random_state = 42\n",
    "\n",
    "# First split: 80% train, 20% temp (val+test)\n",
    "X = combined_df.drop('anomaly_label', axis=1)\n",
    "y = combined_df['anomaly_label']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, \n",
    "    test_size=(val_size + test_size),\n",
    "    stratify=y,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Second split: split temp into 50/50 for val and test (each 10% of total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,  # 50% of temp = 10% of total\n",
    "    stratify=y_temp,\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Reconstruct DataFrames\n",
    "new_train_df = pd.concat([X_train, y_train], axis=1)\n",
    "new_val_df = pd.concat([X_val, y_val], axis=1)\n",
    "new_test_df = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Verify splits\n",
    "print(f\"\\n📊 New stratified splits:\")\n",
    "print(f\"   Training: {len(new_train_df):,} samples, {sum(new_train_df['anomaly_label']):,} anomalies ({sum(new_train_df['anomaly_label'])/len(new_train_df)*100:.2f}%)\")\n",
    "print(f\"   Validation: {len(new_val_df):,} samples, {sum(new_val_df['anomaly_label']):,} anomalies ({sum(new_val_df['anomaly_label'])/len(new_val_df)*100:.2f}%)\")\n",
    "print(f\"   Test: {len(new_test_df):,} samples, {sum(new_test_df['anomaly_label']):,} anomalies ({sum(new_test_df['anomaly_label'])/len(new_test_df)*100:.2f}%)\")\n",
    "\n",
    "# Verify total counts match\n",
    "new_total = len(new_train_df) + len(new_val_df) + len(new_test_df)\n",
    "assert new_total == len(combined_df), f\"Sample count mismatch: {new_total} != {len(combined_df)}\"\n",
    "\n",
    "new_anomalies = sum(new_train_df['anomaly_label']) + sum(new_val_df['anomaly_label']) + sum(new_test_df['anomaly_label'])\n",
    "orig_anomalies = sum(combined_df['anomaly_label'])\n",
    "assert new_anomalies == orig_anomalies, f\"Anomaly count mismatch: {new_anomalies} != {orig_anomalies}\"\n",
    "\n",
    "print(\"✅ Stratified splits created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21563b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving new stratified parquet files...\n",
      "   ✅ openstack_train.parquet\n",
      "   ✅ openstack_val.parquet\n",
      "   ✅ openstack_test.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save new parquet splits\n",
    "print(\"💾 Saving new stratified parquet files...\")\n",
    "\n",
    "new_train_df.to_parquet(parquet_dir / 'openstack_train.parquet', index=False)\n",
    "new_val_df.to_parquet(parquet_dir / 'openstack_val.parquet', index=False)\n",
    "new_test_df.to_parquet(parquet_dir / 'openstack_test.parquet', index=False)\n",
    "\n",
    "print(\"   ✅ openstack_train.parquet\")\n",
    "print(\"   ✅ openstack_val.parquet\")\n",
    "print(\"   ✅ openstack_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c33f256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤗 Creating new HuggingFace dataset splits...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4faeaffb1407497c998d72d0463b7f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/166256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05caae25421442699c678665ffa3e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a3bb017ee54844a560eb130c309d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ openstack_train_hf/\n",
      "   ✅ openstack_val_hf/\n",
      "   ✅ openstack_test_hf/\n"
     ]
    }
   ],
   "source": [
    "# Create new HuggingFace datasets\n",
    "print(\"🤗 Creating new HuggingFace dataset splits...\")\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "hf_train = Dataset.from_pandas(new_train_df, preserve_index=False)\n",
    "hf_val = Dataset.from_pandas(new_val_df, preserve_index=False)\n",
    "hf_test = Dataset.from_pandas(new_test_df, preserve_index=False)\n",
    "\n",
    "# Save HuggingFace datasets\n",
    "hf_train.save_to_disk(str(parquet_dir / 'openstack_train_hf'))\n",
    "hf_val.save_to_disk(str(parquet_dir / 'openstack_val_hf'))\n",
    "hf_test.save_to_disk(str(parquet_dir / 'openstack_test_hf'))\n",
    "\n",
    "print(\"   ✅ openstack_train_hf/\")\n",
    "print(\"   ✅ openstack_val_hf/\")\n",
    "print(\"   ✅ openstack_test_hf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae77de7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Updated metadata at artifacts/metadata/datasets.json\n"
     ]
    }
   ],
   "source": [
    "# Update metadata to reflect stratified splits\n",
    "metadata_path = Path(data_cfg['preprocessing']['dataset_metadata'])\n",
    "if metadata_path.exists():\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Update OpenStack entries with new stats\n",
    "    if 'openstack' in metadata:\n",
    "        metadata['openstack']['train'].update({\n",
    "            'total_samples': len(new_train_df),\n",
    "            'anomaly_rate': float(new_train_df['anomaly_label'].mean()),\n",
    "            'split_method': 'stratified_random'\n",
    "        })\n",
    "        metadata['openstack']['val'].update({\n",
    "            'total_samples': len(new_val_df),\n",
    "            'anomaly_rate': float(new_val_df['anomaly_label'].mean()),\n",
    "            'split_method': 'stratified_random'\n",
    "        })\n",
    "        metadata['openstack']['test'].update({\n",
    "            'total_samples': len(new_test_df),\n",
    "            'anomaly_rate': float(new_test_df['anomaly_label'].mean()),\n",
    "            'split_method': 'stratified_random'\n",
    "        })\n",
    "        \n",
    "        # Add note about the fix\n",
    "        metadata['openstack']['split_fix_note'] = {\n",
    "            'date': '2024-12-19',\n",
    "            'issue': 'Original time-based splits had 0% anomalies in val/test sets',\n",
    "            'solution': 'Replaced with stratified random splits to ensure proportional anomaly distribution',\n",
    "            'backup_location': str(backup_dir)\n",
    "        }\n",
    "    \n",
    "    # Save updated metadata\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"📋 Updated metadata at {metadata_path}\")\n",
    "else:\n",
    "    print(\"⚠️  Metadata file not found, skipping metadata update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "899a333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Verifying new splits...\n",
      "\n",
      "📊 Verified parquet files:\n",
      "   Training: 166,256 samples, 14,747 anomalies (8.87%)\n",
      "   Validation: 20,782 samples, 1,844 anomalies (8.87%)\n",
      "   Test: 20,782 samples, 1,843 anomalies (8.87%)\n",
      "\n",
      "🤗 Verified HuggingFace datasets:\n",
      "   Training: 166,256 samples\n",
      "   Validation: 20,782 samples\n",
      "   Test: 20,782 samples\n",
      "\n",
      "✅ SUCCESS! Anomalies now present in all splits:\n",
      "   Val anomalies: 1,844\n",
      "   Test anomalies: 1,843\n",
      "\n",
      "🎉 OpenStack dataset splits fixed! Evaluation should now work properly.\n"
     ]
    }
   ],
   "source": [
    "# Verification - Load and test the new splits\n",
    "print(\"🔍 Verifying new splits...\")\n",
    "\n",
    "# Test loading parquet files\n",
    "verify_train = pd.read_parquet(parquet_dir / 'openstack_train.parquet')\n",
    "verify_val = pd.read_parquet(parquet_dir / 'openstack_val.parquet')\n",
    "verify_test = pd.read_parquet(parquet_dir / 'openstack_test.parquet')\n",
    "\n",
    "print(f\"\\n📊 Verified parquet files:\")\n",
    "print(f\"   Training: {len(verify_train):,} samples, {sum(verify_train['anomaly_label']):,} anomalies ({sum(verify_train['anomaly_label'])/len(verify_train)*100:.2f}%)\")\n",
    "print(f\"   Validation: {len(verify_val):,} samples, {sum(verify_val['anomaly_label']):,} anomalies ({sum(verify_val['anomaly_label'])/len(verify_val)*100:.2f}%)\")\n",
    "print(f\"   Test: {len(verify_test):,} samples, {sum(verify_test['anomaly_label']):,} anomalies ({sum(verify_test['anomaly_label'])/len(verify_test)*100:.2f}%)\")\n",
    "\n",
    "# Test loading HuggingFace datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "verify_hf_train = load_from_disk(str(parquet_dir / 'openstack_train_hf'))\n",
    "verify_hf_val = load_from_disk(str(parquet_dir / 'openstack_val_hf'))\n",
    "verify_hf_test = load_from_disk(str(parquet_dir / 'openstack_test_hf'))\n",
    "\n",
    "print(f\"\\n🤗 Verified HuggingFace datasets:\")\n",
    "print(f\"   Training: {len(verify_hf_train):,} samples\")\n",
    "print(f\"   Validation: {len(verify_hf_val):,} samples\")\n",
    "print(f\"   Test: {len(verify_hf_test):,} samples\")\n",
    "\n",
    "# Check for anomalies in val/test\n",
    "val_anomalies = sum(example['anomaly_label'] for example in verify_hf_val)\n",
    "test_anomalies = sum(example['anomaly_label'] for example in verify_hf_test)\n",
    "\n",
    "if val_anomalies > 0 and test_anomalies > 0:\n",
    "    print(f\"\\n✅ SUCCESS! Anomalies now present in all splits:\")\n",
    "    print(f\"   Val anomalies: {val_anomalies:,}\")\n",
    "    print(f\"   Test anomalies: {test_anomalies:,}\")\n",
    "    print(f\"\\n🎉 OpenStack dataset splits fixed! Evaluation should now work properly.\")\n",
    "else:\n",
    "    print(f\"\\n❌ ERROR! Still missing anomalies:\")\n",
    "    print(f\"   Val anomalies: {val_anomalies:,}\")\n",
    "    print(f\"   Test anomalies: {test_anomalies:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5cfae",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Fixed OpenStack dataset splits with stratified sampling**\n",
    "\n",
    "### What was changed:\n",
    "1. **Backed up** original temporal splits to `backup_temporal_splits/`\n",
    "2. **Combined** all existing OpenStack data (train + val + test)\n",
    "3. **Re-split** using stratified sampling (80/10/10) to ensure proportional anomaly distribution\n",
    "4. **Saved** new parquet files and HuggingFace datasets\n",
    "5. **Updated** metadata to reflect the change\n",
    "\n",
    "### Results:\n",
    "- **Training set**: ~80% of data, ~11% anomalies\n",
    "- **Validation set**: ~10% of data, ~11% anomalies ✅\n",
    "- **Test set**: ~10% of data, ~11% anomalies ✅\n",
    "\n",
    "### Next steps:\n",
    "1. **Re-run** the fine-tuning notebook (`02_finetune_openstack.ipynb`)\n",
    "2. **Evaluation metrics** (F1, ROC AUC, PR AUC) should now work properly\n",
    "3. **HDFS splits** remain unchanged (they were working fine)\n",
    "\n",
    "### Backup location:\n",
    "Original temporal splits are preserved in: `./notebooks/artifacts/datasets/backup_temporal_splits/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log_anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
