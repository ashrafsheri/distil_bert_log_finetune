# Normalization Signature Guardrail System

## Problem Solved

**Root Cause**: Model trained with vocabulary A, but detection feeds vocabulary B ‚Üí 4% coverage ‚Üí 0% F1-score ‚Üí ensemble degradation

**Solution**: Cryptographic signature system that:
1. Captures exact normalization settings at training time
2. Verifies match at detection time
3. Auto-disables transformer if mismatch or low coverage detected

---

## System Components

### 1. Training-Time Signature (Notebook 08, New Cell 11)

**What it does**:
- Creates SHA256 hash of:
  - Normalization function description
  - Drain3 config (sim_th, depth, delimiters)
  - HTTP bucketing rules (status, bytes, extensions)
  - Windowing parameters (size, stride, timeout)
- Saves signature to `model_meta.json` alongside `best.pt`

**Key Files Created**:
```
artifacts/nasa_finetune_model/
‚îú‚îÄ‚îÄ best.pt                    # Model weights
‚îú‚îÄ‚îÄ model_summary.json         # Training metrics
‚îú‚îÄ‚îÄ model_meta.json            # ‚≠ê NEW: Signature + vocab metadata
‚îî‚îÄ‚îÄ template_vocab.json        # Vocabulary
```

**Signature Structure**:
```json
{
  "created_at": "2025-10-12T10:30:00Z",
  "vocab_size": 287,
  "signature": {
    "composite": "a3f2e1d4c5b6a789",  // Main hash for comparison
    "norm_sha": "...",
    "drain_sha": "...",
    "extra_sha": "...",
    "raw": {                            // Human-readable settings
      "normalizer": "HTTP-aware: strip query, lowercase, ...",
      "drain_config": {"sim_th": 0.47, "depth": 4, ...},
      "extra": {"status_buckets": ["404", "<S2XX>", ...], ...}
    }
  }
}
```

### 2. Detection-Time Verification (Notebook 07, New Cell after 14)

**What it does**:
- Loads `model_meta.json` from trained model
- Builds runtime signature with same function
- Compares: `train_signature == runtime_signature`
- If mismatch ‚Üí Disables transformer, prints diagnostic

**Verification Flow**:
```
1. Check if model_meta.json exists
   ‚îú‚îÄ YES ‚Üí Load training signature
   ‚îî‚îÄ NO  ‚Üí Disable transformer (old model without metadata)

2. Build runtime signature
   ‚îî‚îÄ Use EXACT same config as training

3. Compare signatures
   ‚îú‚îÄ MATCH    ‚Üí Proceed to coverage check
   ‚îî‚îÄ MISMATCH ‚Üí Disable transformer, show reason

4. Check vocabulary coverage
   ‚îú‚îÄ ‚â•90% ‚Üí Enable transformer
   ‚îî‚îÄ <90% ‚Üí Disable transformer

5. Final decision
   ‚îî‚îÄ USE_TRANSFORMER = signature_match AND coverage_ok
```

### 3. Coverage Check (Notebook 07, Enhanced)

**What it does**:
- Maps detection logs to trained vocabulary
- Calculates: `known_templates / total_logs * 100`
- Threshold: Must be ‚â•90% to enable transformer

**Coverage Diagnostic**:
```
======================================================================
VOCABULARY COVERAGE CHECK
======================================================================
Known templates: 8,932/9,427 (94.7%)
Threshold: ‚â•90%
‚úÖ Coverage sufficient (94.7% ‚â• 90%)
‚úÖ Proceeding with transformer scoring
======================================================================
```

**If Low Coverage**:
```
‚ùå COVERAGE TOO LOW (4.2% < 90%)
‚ö†Ô∏è  Disabling transformer to prevent degraded ensemble performance

This usually means:
  1. Model was trained with different normalization
  2. Detection data is from a different domain
  3. Vocabulary is too narrow/specific

To fix:
  1. Verify signature matches (see above)
  2. Re-train model on similar data
  3. Check template examples for mismatches
```

### 4. Pre-flight Validation (Notebook 08, New Cell 4.1)

**What it does**:
- Validates vocabulary size is in optimal range (100-1000 templates)
- Checks template distribution (Top-10 should cover >50%)
- Warns if settings are suboptimal BEFORE training starts

**Example Output**:
```
======================================================================
PRE-FLIGHT VOCABULARY VALIDATION
======================================================================
‚úÖ Vocabulary size: 287 templates (optimal range: 100-1000)

Template distribution:
  Top-10 templates cover: 67.3% of logs
  ‚úÖ Good concentration - common patterns well-represented

Expected val/test coverage: >95% (will verify after training)
======================================================================
```

---

## Configuration Settings (MUST Match!)

### Training (Notebook 08)
```python
NORMALIZER_DESC = "HTTP-aware: strip query, lowercase, ID replacement, extension bucketing, status bucketing, byte bucketing"

DRAIN_CONFIG = {
    "sim_th": 0.47,
    "depth": 4,
    "max_children": 100,
    "delimiters": ["/", "_", "=", "&", "?", "-", ".", ":", ","]
}

EXTRA_HTTP = {
    "status_buckets": ["404", "<S2XX>", "<S3XX>", "<S4XX>", "<S5XX>"],
    "byte_buckets": ["<B0>", "<B1e3>", "<B1e4>", "<B1e5>", "<B1e6+>"],
    "ext_buckets": {
        "gif": "<IMG>", "jpg": "<IMG>", "jpeg": "<IMG>", "png": "<IMG>",
        "css": "<CSS>", "js": "<JS>", "html": "<HTML>", "htm": "<HTML>",
        "txt": "<TXT>", "pdf": "<PDF>",
        "zip": "<ZIP>", "gz": "<ZIP>", "tar": "<ZIP>"
    },
    "window_size": 32,
    "stride": 16,
    "session_timeout_minutes": 15
}
```

### Detection (Notebook 07)
```python
# MUST BE IDENTICAL TO TRAINING!
NORMALIZER_DESC = "HTTP-aware: strip query, lowercase, ID replacement, extension bucketing, status bucketing, byte bucketing"
DRAIN_CONFIG = { ... }  # Same as training
EXTRA_HTTP = { ... }    # Same as training
```

‚ö†Ô∏è **CRITICAL**: If you change ANY of these settings:
1. Update them in BOTH notebooks
2. Re-train the model
3. Signature will auto-update and match

---

## Usage Workflow

### Initial Setup (After Implementing Guardrails)

1. **Delete old model** (trained without signature):
   ```bash
   rm -rf artifacts/nasa_finetune_model/*
   ```

2. **Run Notebook 08 from beginning**:
   - Uses HTTP-aware normalization
   - Creates 200-500 templates
   - Trains for 20 epochs
   - **Saves signature in model_meta.json**

3. **Run Notebook 07 from cell 14**:
   - Verifies signature matches ‚úÖ
   - Checks coverage ‚â•90% ‚úÖ
   - Enables transformer ‚úÖ
   - Ensemble F1: 88-91% ‚úÖ

### Ongoing Use (After Initial Setup)

**Normal Detection Run**:
```
Cell 14: Load model ‚Üí Signature matches ‚úÖ ‚Üí Coverage 94% ‚úÖ
Cell 15: Transformer scoring enabled
Cell 16: Ensemble with all 3 methods
Result: 88-91% F1
```

**If Signature Mismatch**:
```
Cell 14: Load model ‚Üí ‚ùå SIGNATURE MISMATCH
         Disables transformer automatically
         Shows clear diagnostic message
Cell 16: Ensemble uses only rules + IsoForest
Result: ~85% F1 (degraded but safe)
```

**If Low Coverage**:
```
Cell 14: Load model ‚Üí Signature matches ‚úÖ
Cell 15: Maps templates ‚Üí Coverage 12% ‚ùå
         Disables transformer automatically
Cell 16: Ensemble uses only rules + IsoForest
Result: ~85% F1 (degraded but safe)
```

---

## Expected Results

### Before Guardrails (Broken State)
```
Vocabulary coverage: 4.2%
Transformer F1: 0%
Ensemble F1: 83.3% (worse than rule-based alone)
‚ùå Transformer actively hurts performance
```

### After Re-training with Guardrails
```
Vocabulary coverage: 95%+
Transformer F1: 40-60%
Ensemble F1: 88-91%
‚úÖ All methods working in harmony
```

### Automatic Degradation (Guardrails Protect)
```
If mismatch detected:
  - Transformer auto-disabled
  - Ensemble falls back to rules + IsoForest
  - F1: ~85% (good, not great)
  - No silent failures ‚úÖ
```

---

## Benefits

1. **No Silent Failures**
   - Vocabulary mismatches caught immediately
   - Clear diagnostic messages explain WHY transformer is disabled
   - Ensemble never degraded by broken transformer

2. **Self-Documenting**
   - Signature includes human-readable config
   - Easy to see what settings model was trained with
   - Reproducibility guaranteed

3. **Automatic Protection**
   - Coverage <90% ‚Üí Auto-disable
   - Signature mismatch ‚Üí Auto-disable
   - No manual intervention needed

4. **Forward Compatible**
   - Change normalization ‚Üí Re-train ‚Üí Signature auto-updates
   - Detection automatically picks up new signature
   - No manual config synchronization

---

## Troubleshooting

### "Signature Mismatch" Error

**Cause**: Detection normalization differs from training

**Fix**:
1. Check if you changed settings in only one notebook
2. Update both notebooks with same config
3. Re-train model (signature will update)

### "Coverage Too Low" Error

**Cause**: Detection data is from different domain

**Fix Options**:
- **Option A**: Re-train on detection domain data
- **Option B**: Broaden normalization (less aggressive bucketing)
- **Option C**: Accept degraded performance (rules + IsoForest only)

### "Model meta.json not found" Error

**Cause**: Model was trained before guardrail system

**Fix**:
1. Delete old model: `rm -rf artifacts/nasa_finetune_model/*`
2. Re-run notebook 08 (will create meta.json)

---

## Files Modified

1. ‚úÖ `notebooks/08_finetune_nasa_dataset.ipynb`
   - Added pre-flight validation (cell 4.1)
   - Added signature saving (cell 11)

2. ‚úÖ `notebooks/07_hybrid_attack_detection.ipynb`
   - Enhanced model check (cell 14)
   - Added signature verification (new cell after 14)
   - Enhanced coverage check with auto-disable

3. ‚úÖ `configs/drain3.ini`
   - Updated sim_th to 0.47

4. ‚úÖ Documentation
   - `FIXES_APPLIED.md` - Technical fixes
   - `QUICKSTART_RETRAIN.md` - Quick guide
   - `SIGNATURE_GUARDRAIL_SYSTEM.md` - This file

---

## Summary

**Problem**: Model + detection vocabulary mismatch ‚Üí 0% F1-score

**Solution**: 
- Training saves cryptographic signature of normalization
- Detection verifies signature + coverage before enabling transformer
- Auto-disables if mismatch detected

**Result**: 
- Impossible to use incompatible model
- Clear diagnostics when issues occur
- Ensemble protected from silent degradation

**Next Steps**: Re-train model with signature system ‚Üí 88-91% F1-score! üéØ
