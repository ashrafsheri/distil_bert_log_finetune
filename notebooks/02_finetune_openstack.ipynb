{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f8dba6",
   "metadata": {},
   "source": [
    "# 02 ¬∑ Fine-tune on OpenStack Logs\n",
    "\n",
    "This notebook adapts the HDFS-pretrained DistilBERT model to OpenStack anomaly detection with optional replay and LoRA support. Hugging Face Accelerate drives training on multi-GPU Linux or falls back to Apple MPS when available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f43735",
   "metadata": {},
   "source": [
    "## Notebook Goals\n",
    "\n",
    "- Load OpenStack fine-tuning hyperparameters and reuse the Accelerate configuration (skipped for MPS).\n",
    "- Optionally replay a slice of HDFS data and/or enable LoRA adapters via config toggles.\n",
    "- Train with early stopping, checkpoint cadence, and GPU/MPS memory hygiene utilities.\n",
    "- Evaluate on validation/test splits with F1, ROC-AUC, PR-AUC, and confusion matrices.\n",
    "- Export TorchScript and ONNX artifacts and capture a MODEL_CARD snippet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04398c31",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aeab24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpi/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/torch/cuda/__init__.py:54: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_scheduler\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2488938",
   "metadata": {},
   "source": [
    "### Load YAML configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9782fae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs loaded.\n"
     ]
    }
   ],
   "source": [
    "def load_yaml(path: Path) -> Dict:\n",
    "    with path.open('r') as fh:\n",
    "        return yaml.safe_load(fh)\n",
    "\n",
    "data_cfg = load_yaml(Path('../configs/data.yaml'))\n",
    "train_cfg = load_yaml(Path('../configs/train_openstack.yaml'))\n",
    "print('Configs loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830924e3",
   "metadata": {},
   "source": [
    "### Device detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c4eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS not available; using CUDA/CPU settings from training config.\n"
     ]
    }
   ],
   "source": [
    "IS_MPS = torch.backends.mps.is_available()\n",
    "if IS_MPS:\n",
    "    os.environ.setdefault('ACCELERATE_USE_MPS_DEVICE', '1')\n",
    "    print('Apple Silicon (MPS) detected. Accelerate will use the MPS backend.')\n",
    "else:\n",
    "    print('MPS not available; using CUDA/CPU settings from training config.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2edb5",
   "metadata": {},
   "source": [
    "## 2. Prepare Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cbca4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenStack columns: ['input_ids', 'attention_mask', 'labels', 'template_id', 'anomaly_label', 'timestamp']\n",
      "HDFS columns: ['input_ids', 'attention_mask', 'labels', 'template_id', 'anomaly_label', 'timestamp']\n",
      "Schema alignment completed. Columns: ['input_ids', 'attention_mask', 'labels', 'template_id', 'anomaly_label', 'timestamp']\n",
      "Replay enabled: mixed 16625 HDFS rows with 166256 OpenStack rows.\n"
     ]
    }
   ],
   "source": [
    "parquet_dir = Path(data_cfg['preprocessing']['parquet_dir'])\n",
    "metadata_path = Path(data_cfg['preprocessing']['dataset_metadata'])\n",
    "metadata = json.loads(metadata_path.read_text()) if metadata_path.exists() else {}\n",
    "\n",
    "openstack_train = load_from_disk(str(parquet_dir / 'openstack_train_hf'))\n",
    "openstack_val = load_from_disk(str(parquet_dir / 'openstack_val_hf'))\n",
    "openstack_test = load_from_disk(str(parquet_dir / 'openstack_test_hf'))\n",
    "\n",
    "replay_cfg = train_cfg['replay']\n",
    "if replay_cfg['enabled']:\n",
    "    hdfs_dataset = load_from_disk(str(parquet_dir / 'hdfs_train_hf'))\n",
    "    replay_size = max(1, int(len(openstack_train) * replay_cfg['ratio']))\n",
    "    replay_subset = hdfs_dataset.shuffle(seed=train_cfg['seed']).select(range(replay_size))\n",
    "    \n",
    "    # Convert to pandas and fix schema compatibility\n",
    "    openstack_df = openstack_train.to_pandas()\n",
    "    hdfs_df = replay_subset.to_pandas()\n",
    "    \n",
    "    # Ensure compatible schemas\n",
    "    print(f\"OpenStack columns: {list(openstack_df.columns)}\")\n",
    "    print(f\"HDFS columns: {list(hdfs_df.columns)}\")\n",
    "    \n",
    "    # Align columns - keep only common columns or add missing ones with default values\n",
    "    common_columns = set(openstack_df.columns) & set(hdfs_df.columns)\n",
    "    \n",
    "    # For missing columns in HDFS, add defaults\n",
    "    for col in openstack_df.columns:\n",
    "        if col not in hdfs_df.columns:\n",
    "            if col == 'anomaly_label':\n",
    "                hdfs_df[col] = 0  # Normal logs by default\n",
    "            elif 'template_id' in col:\n",
    "                hdfs_df[col] = -1  # Default template ID\n",
    "            else:\n",
    "                hdfs_df[col] = None  # Default to None for other missing columns\n",
    "    \n",
    "    # For missing columns in OpenStack, add defaults  \n",
    "    for col in hdfs_df.columns:\n",
    "        if col not in openstack_df.columns:\n",
    "            if 'template_id' in col:\n",
    "                openstack_df[col] = -1\n",
    "            else:\n",
    "                openstack_df[col] = None\n",
    "    \n",
    "    # Ensure template_id columns are properly typed as integers\n",
    "    for col in openstack_df.columns:\n",
    "        if 'template_id' in col:\n",
    "            # Handle string template_ids that can't convert to int\n",
    "            if col in hdfs_df.columns:\n",
    "                hdfs_df[col] = pd.to_numeric(hdfs_df[col], errors='coerce').fillna(-1).astype('int64')\n",
    "            openstack_df[col] = pd.to_numeric(openstack_df[col], errors='coerce').fillna(-1).astype('int64')\n",
    "    \n",
    "    # Reorder columns to match\n",
    "    column_order = list(openstack_df.columns)\n",
    "    hdfs_df = hdfs_df.reindex(columns=column_order)\n",
    "    \n",
    "    print(f\"Schema alignment completed. Columns: {column_order}\")\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined = pd.concat([openstack_df, hdfs_df], ignore_index=True)\n",
    "    combined = combined.sample(frac=1.0, random_state=train_cfg['seed']).reset_index(drop=True)\n",
    "    train_dataset = Dataset.from_pandas(combined, preserve_index=False)\n",
    "    print(f'Replay enabled: mixed {replay_size} HDFS rows with {len(openstack_train)} OpenStack rows.')\n",
    "else:\n",
    "    train_dataset = openstack_train\n",
    "    print('Replay disabled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b80481",
   "metadata": {},
   "source": [
    "## 3. Tokenizer and Base Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c3e8c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for HDFS pretrained model checkpoints...\n",
      "üìä Found 1 epoch checkpoints\n",
      "üìä Found 10 step checkpoints\n",
      "üöÄ Loading HDFS pretrained DistilBERT from: step_epoch3_step800000\n",
      "üìÇ Full path: artifacts/logbert-mlm-hdfs/step_epoch3_step800000\n",
      "‚úÖ Successfully loaded pretrained HDFS model!\n",
      "   Model vocab size: 30531\n",
      "   Tokenizer vocab size: 30531\n",
      "   Model parameters: 66,992,451\n",
      "LoRA disabled; full fine-tuning will run.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_dir = Path(train_cfg['artifacts']['tokenizer_dir'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, use_fast=True)\n",
    "\n",
    "base_dir = Path(train_cfg['base_checkpoint_dir'])\n",
    "if not base_dir.exists():\n",
    "    raise FileNotFoundError(f'Base checkpoint directory not found: {base_dir}')\n",
    "\n",
    "# Look for the completed HDFS pretraining checkpoint (final epoch)\n",
    "print(\"üîç Searching for HDFS pretrained model checkpoints...\")\n",
    "candidate_checkpoints = []\n",
    "\n",
    "# Check for epoch checkpoint (preferred - final trained model)\n",
    "epoch_checkpoints = sorted(base_dir.glob('epoch_epoch*_step*/config.json'))\n",
    "if epoch_checkpoints:\n",
    "    candidate_checkpoints.extend(epoch_checkpoints)\n",
    "    print(f\"üìä Found {len(epoch_checkpoints)} epoch checkpoints\")\n",
    "\n",
    "# Check for step checkpoints as fallback\n",
    "step_checkpoints = sorted(base_dir.glob('step_epoch*_step*/config.json'))\n",
    "if step_checkpoints:\n",
    "    candidate_checkpoints.extend(step_checkpoints)\n",
    "    print(f\"üìä Found {len(step_checkpoints)} step checkpoints\")\n",
    "\n",
    "if not candidate_checkpoints:\n",
    "    raise FileNotFoundError(f'No pretrained checkpoints found in {base_dir}')\n",
    "\n",
    "# Select the most recent checkpoint (highest step number)\n",
    "model_path = candidate_checkpoints[-1].parent\n",
    "checkpoint_name = model_path.name\n",
    "\n",
    "print(f'üöÄ Loading HDFS pretrained DistilBERT from: {checkpoint_name}')\n",
    "print(f'üìÇ Full path: {model_path}')\n",
    "\n",
    "# Verify checkpoint has required files\n",
    "required_files = ['config.json', 'model.safetensors']\n",
    "missing_files = [f for f in required_files if not (model_path / f).exists()]\n",
    "if missing_files:\n",
    "    # Try pytorch_model.bin as fallback\n",
    "    if (model_path / 'pytorch_model.bin').exists():\n",
    "        print(\"‚úÖ Found pytorch_model.bin (using instead of model.safetensors)\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f'Missing required files in checkpoint: {missing_files}')\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_path, config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f'‚úÖ Successfully loaded pretrained HDFS model!')\n",
    "print(f'   Model vocab size: {model.config.vocab_size}')\n",
    "print(f'   Tokenizer vocab size: {len(tokenizer)}')\n",
    "print(f'   Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "\n",
    "peft_cfg = train_cfg['peft']\n",
    "if peft_cfg['lora_enabled']:\n",
    "    lora_config = LoraConfig(\n",
    "        r=peft_cfg['r'],\n",
    "        lora_alpha=peft_cfg['alpha'],\n",
    "        target_modules=peft_cfg['target_modules'],\n",
    "        lora_dropout=peft_cfg['dropout'],\n",
    "        bias=peft_cfg['bias']\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print('LoRA adapters enabled.')\n",
    "else:\n",
    "    print('LoRA disabled; full fine-tuning will run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b043fb",
   "metadata": {},
   "source": [
    "## 4. DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3c9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=train_cfg['sequence']['mlm_probability'],\n",
    "    pad_to_multiple_of=8  # Efficient padding for GPU memory\n",
    ")\n",
    "\n",
    "def collate_train(examples):\n",
    "    # Remove anomaly_label for MLM training\n",
    "    for example in examples:\n",
    "        example.pop('anomaly_label', None)\n",
    "    \n",
    "    # Ensure proper tokenization with padding and truncation\n",
    "    processed_examples = []\n",
    "    for example in examples:\n",
    "        # Handle potential nested inputs by extracting the core tokenized data\n",
    "        if 'input_ids' in example:\n",
    "            processed_example = {\n",
    "                'input_ids': example['input_ids'][:train_cfg['sequence']['max_length']],  # Truncate if needed\n",
    "                'attention_mask': example.get('attention_mask', [1] * len(example['input_ids']))[:train_cfg['sequence']['max_length']]\n",
    "            }\n",
    "        else:\n",
    "            # Fallback for unexpected format\n",
    "            processed_example = {k: v for k, v in example.items() if k not in ['anomaly_label']}\n",
    "        processed_examples.append(processed_example)\n",
    "    \n",
    "    return collator(processed_examples)\n",
    "\n",
    "def collate_eval(examples):\n",
    "    labels = [example.get('anomaly_label', 0) for example in examples]\n",
    "    \n",
    "    # Process features similar to training\n",
    "    processed_features = []\n",
    "    for example in examples:\n",
    "        if 'input_ids' in example:\n",
    "            processed_feature = {\n",
    "                'input_ids': example['input_ids'][:train_cfg['sequence']['max_length']],\n",
    "                'attention_mask': example.get('attention_mask', [1] * len(example['input_ids']))[:train_cfg['sequence']['max_length']]\n",
    "            }\n",
    "        else:\n",
    "            processed_feature = {k: v for k, v in example.items() if k != 'anomaly_label'}\n",
    "        processed_features.append(processed_feature)\n",
    "    \n",
    "    batch = collator(processed_features)\n",
    "    batch['anomaly_label'] = torch.tensor(labels, dtype=torch.long)\n",
    "    return batch\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_cfg['training']['train_batch_size_per_device'], shuffle=True, collate_fn=collate_train)\n",
    "val_loader = DataLoader(openstack_val, batch_size=train_cfg['training']['eval_batch_size_per_device'], shuffle=False, collate_fn=collate_eval)\n",
    "test_loader = DataLoader(openstack_test, batch_size=train_cfg['training']['eval_batch_size_per_device'], shuffle=False, collate_fn=collate_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7f7d84",
   "metadata": {},
   "source": [
    "## 5. Accelerator and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08960923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mixed_precision = 'no' if IS_MPS else train_cfg['precision']['mixed_precision']\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=train_cfg['training']['grad_accumulation_steps'],\n",
    "    mixed_precision=mixed_precision\n",
    ")\n",
    "print(accelerator.state)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_cfg['optimizer']['lr'],\n",
    "    betas=tuple(train_cfg['optimizer']['betas']),\n",
    "    eps=train_cfg['optimizer']['eps'],\n",
    "    weight_decay=train_cfg['optimizer']['weight_decay']\n",
    ")\n",
    "\n",
    "model, optimizer, train_loader, val_loader, test_loader = accelerator.prepare(\n",
    "    model, optimizer, train_loader, val_loader, test_loader\n",
    ")\n",
    "\n",
    "total_steps = math.ceil(len(train_loader) / train_cfg['training']['grad_accumulation_steps']) * train_cfg['training']['epochs']\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=train_cfg['optimizer']['scheduler'],\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=train_cfg['optimizer']['warmup_steps'],\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c56955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing batch creation...\n",
      "‚úÖ Batch created successfully!\n",
      "   Input IDs shape: torch.Size([8, 88])\n",
      "   Attention mask shape: torch.Size([8, 88])\n",
      "   Labels shape: torch.Size([8, 88])\n",
      "   Sequence lengths are consistent: 88 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test batch creation to verify collation works\n",
    "print(\"üß™ Testing batch creation...\")\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    print(f\"‚úÖ Batch created successfully!\")\n",
    "    print(f\"   Input IDs shape: {test_batch['input_ids'].shape}\")\n",
    "    print(f\"   Attention mask shape: {test_batch['attention_mask'].shape}\")\n",
    "    print(f\"   Labels shape: {test_batch['labels'].shape}\")\n",
    "    print(f\"   Sequence lengths are consistent: {test_batch['input_ids'].shape[1]} tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Batch creation failed: {e}\")\n",
    "    print(\"This needs to be fixed before training can proceed.\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94d82e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Resetting training state...\n",
      "‚úÖ Training state reset. Ready to begin fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "# Reset training state variables\n",
    "print(\"üîÑ Resetting training state...\")\n",
    "\n",
    "history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "best_val = float('inf')\n",
    "best_checkpoint_path = None\n",
    "wait = 0\n",
    "\n",
    "print(\"‚úÖ Training state reset. Ready to begin fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00f72a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def log_gpu_memory(tag: str):\n",
    "    if torch.cuda.is_available():\n",
    "        alloc = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        accelerator.print(f'[{tag}] gpu allocated={alloc:.2f} GB reserved={reserved:.2f} GB')\n",
    "    elif IS_MPS:\n",
    "        try:\n",
    "            import torch.mps\n",
    "            stats = torch.mps.current_allocated_memory() / (1024 ** 3)\n",
    "            accelerator.print(f'[{tag}] mps allocated={stats:.2f} GB')\n",
    "        except Exception:\n",
    "            accelerator.print(f'[{tag}] mps memory stats unavailable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1514d286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training setup initialized!\n",
      "   üéØ Learning rate: 2.0e-06\n",
      "   ‚è∞ Warmup steps: 2,000\n",
      "   üöÄ Epochs: 3\n",
      "   üõë Patience: 3\n",
      "   üìè Max grad norm: 0.5\n",
      "   üîÑ Grad accumulation: 8\n",
      "   üìä Checkpoint every: 3429 batches (15% of dataset)\n"
     ]
    }
   ],
   "source": [
    "# Initialize training setup\n",
    "checkpoint_cfg = train_cfg['checkpointing']\n",
    "metrics_dir = Path(train_cfg['artifacts']['metrics_dir'])\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "eval_dir = Path(train_cfg['artifacts']['eval_dir'])\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "run_config_path = Path(train_cfg['artifacts']['run_config_path'])\n",
    "run_config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training state variables\n",
    "history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "best_val = float('inf')\n",
    "best_checkpoint_path = None\n",
    "wait = 0\n",
    "patience = train_cfg['training']['patience']\n",
    "min_delta = train_cfg['training']['min_delta']\n",
    "epochs_total = train_cfg['training']['epochs']\n",
    "max_grad_norm = train_cfg['training']['max_grad_norm']\n",
    "log_steps = train_cfg['logging']['log_steps'] if 'logging' in train_cfg else None\n",
    "\n",
    "# Calculate checkpoint frequency for every 15% of dataset\n",
    "total_batches_per_epoch = len(train_loader)\n",
    "checkpoint_every_batches = max(1, int(total_batches_per_epoch * 0.15))\n",
    "\n",
    "print(\"‚úÖ Training setup initialized!\")\n",
    "print(f\"   üéØ Learning rate: {train_cfg['optimizer']['lr']:.1e}\")\n",
    "print(f\"   ‚è∞ Warmup steps: {train_cfg['optimizer']['warmup_steps']:,}\")\n",
    "print(f\"   üöÄ Epochs: {epochs_total}\")\n",
    "print(f\"   üõë Patience: {patience}\")\n",
    "print(f\"   üìè Max grad norm: {max_grad_norm}\")\n",
    "print(f\"   üîÑ Grad accumulation: {train_cfg['training']['grad_accumulation_steps']}\")\n",
    "print(f\"   üìä Checkpoint every: {checkpoint_every_batches} batches (15% of dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6a98974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation dataloaders created with anomaly labels preserved\n"
     ]
    }
   ],
   "source": [
    "# Create evaluation collate function that preserves anomaly labels\n",
    "def collate_eval(examples):\n",
    "    \"\"\"Evaluation collate function that preserves anomaly labels for evaluation\"\"\"\n",
    "    labels = [example.get('anomaly_label', 0) for example in examples]\n",
    "    \n",
    "    # Process features for MLM\n",
    "    processed_features = []\n",
    "    for example in examples:\n",
    "        if 'input_ids' in example:\n",
    "            processed_feature = {\n",
    "                'input_ids': example['input_ids'][:train_cfg['sequence']['max_length']],\n",
    "                'attention_mask': example.get('attention_mask', [1] * len(example['input_ids']))[:train_cfg['sequence']['max_length']]\n",
    "            }\n",
    "        else:\n",
    "            processed_feature = {k: v for k, v in example.items() if k != 'anomaly_label'}\n",
    "        processed_features.append(processed_feature)\n",
    "    \n",
    "    # Use MLM collator for the features\n",
    "    batch = collator(processed_features)\n",
    "    # Add back the anomaly labels for evaluation\n",
    "    batch['anomaly_label'] = torch.tensor(labels, dtype=torch.long)\n",
    "    return batch\n",
    "\n",
    "# Recreate evaluation loaders with the fixed collate function\n",
    "val_loader_eval = DataLoader(openstack_val, batch_size=train_cfg['training']['eval_batch_size_per_device'], shuffle=False, collate_fn=collate_eval)\n",
    "test_loader_eval = DataLoader(openstack_test, batch_size=train_cfg['training']['eval_batch_size_per_device'], shuffle=False, collate_fn=collate_eval)\n",
    "\n",
    "# Prepare the evaluation loaders\n",
    "_, _, _, val_loader_eval, test_loader_eval = accelerator.prepare(\n",
    "    None, None, None, val_loader_eval, test_loader_eval\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Evaluation dataloaders created with anomaly labels preserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b9ba591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Starting ultra-stable training loop...\n",
      "üìä Ultra-conservative settings active\n",
      "üìä Training samples: 182,881\n",
      "üìä Validation samples: 20,782\n",
      "üìä Batches per epoch: 22,861\n",
      "‚úÖ Created MLM-specific validation loader (without anomaly labels)\n",
      "==== Epoch 1/3 (Ultra-Stable Mode) ====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462bf3170cbd4237833a489149c6e384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 - Stable:   0%|          | 0/22861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Step    25 | Loss: 7.9509 | Avg: 7.7425 | LR: 2.5e-08\n",
      "üìà Step    50 | Loss: 6.9753 | Avg: 7.6525 | LR: 5.0e-08\n",
      "üìà Step    75 | Loss: 8.5835 | Avg: 8.0105 | LR: 7.5e-08\n",
      "üìà Step   100 | Loss: 8.2300 | Avg: 7.6603 | LR: 1.0e-07\n",
      "üìà Step   125 | Loss: 8.7046 | Avg: 7.6113 | LR: 1.2e-07\n",
      "üìà Step   150 | Loss: 8.3361 | Avg: 7.6327 | LR: 1.5e-07\n",
      "üìà Step   175 | Loss: 9.1738 | Avg: 7.6767 | LR: 1.7e-07\n",
      "üìà Step   200 | Loss: 6.5370 | Avg: 7.8188 | LR: 2.0e-07\n",
      "üìà Step   225 | Loss: 7.6995 | Avg: 7.7456 | LR: 2.2e-07\n",
      "üìà Step   250 | Loss: 8.5709 | Avg: 7.9315 | LR: 2.5e-07\n",
      "üìà Step   275 | Loss: 8.2594 | Avg: 7.6343 | LR: 2.8e-07\n",
      "üìà Step   300 | Loss: 8.0072 | Avg: 7.6323 | LR: 3.0e-07\n",
      "üìà Step   325 | Loss: 8.3055 | Avg: 8.0554 | LR: 3.3e-07\n",
      "üìà Step   350 | Loss: 7.8392 | Avg: 7.4493 | LR: 3.5e-07\n",
      "üìà Step   375 | Loss: 6.5561 | Avg: 7.6315 | LR: 3.8e-07\n",
      "üìà Step   400 | Loss: 7.1656 | Avg: 7.8862 | LR: 4.0e-07\n",
      "üìà Step   425 | Loss: 6.5033 | Avg: 7.7086 | LR: 4.2e-07\n",
      "üìà Step   450 | Loss: 8.0992 | Avg: 7.3373 | LR: 4.5e-07\n",
      "üìà Step   475 | Loss: 7.3237 | Avg: 7.7011 | LR: 4.7e-07\n",
      "üìà Step   500 | Loss: 7.9106 | Avg: 7.2961 | LR: 5.0e-07\n",
      "üìà Step   525 | Loss: 7.1135 | Avg: 7.8665 | LR: 5.2e-07\n",
      "üìà Step   550 | Loss: 6.7646 | Avg: 7.6300 | LR: 5.5e-07\n",
      "üìà Step   575 | Loss: 5.5542 | Avg: 7.3746 | LR: 5.7e-07\n",
      "üìà Step   600 | Loss: 8.6322 | Avg: 7.5302 | LR: 6.0e-07\n",
      "üìà Step   625 | Loss: 7.9935 | Avg: 7.6016 | LR: 6.2e-07\n",
      "üìà Step   650 | Loss: 7.3441 | Avg: 7.6834 | LR: 6.5e-07\n",
      "üìà Step   675 | Loss: 7.4303 | Avg: 7.2730 | LR: 6.7e-07\n",
      "üìà Step   700 | Loss: 7.6959 | Avg: 7.3190 | LR: 7.0e-07\n",
      "üìà Step   725 | Loss: 8.3237 | Avg: 7.3484 | LR: 7.2e-07\n",
      "üìà Step   750 | Loss: 7.5135 | Avg: 7.2080 | LR: 7.5e-07\n",
      "üìà Step   775 | Loss: 8.1448 | Avg: 7.4444 | LR: 7.7e-07\n",
      "üìà Step   800 | Loss: 8.5852 | Avg: 7.4235 | LR: 8.0e-07\n",
      "üìà Step   825 | Loss: 7.0199 | Avg: 7.4291 | LR: 8.2e-07\n",
      "üìà Step   850 | Loss: 5.9909 | Avg: 6.9203 | LR: 8.5e-07\n",
      "üìà Step   875 | Loss: 6.8557 | Avg: 7.1510 | LR: 8.7e-07\n",
      "üìà Step   900 | Loss: 8.7907 | Avg: 7.2195 | LR: 9.0e-07\n",
      "üìà Step   925 | Loss: 7.1459 | Avg: 7.3590 | LR: 9.3e-07\n",
      "üìà Step   950 | Loss: 7.0393 | Avg: 7.1480 | LR: 9.5e-07\n",
      "üìà Step   975 | Loss: 6.6457 | Avg: 7.3493 | LR: 9.7e-07\n",
      "üìà Step  1000 | Loss: 6.1571 | Avg: 7.3219 | LR: 1.0e-06\n",
      "üìà Step  1025 | Loss: 7.7766 | Avg: 7.1272 | LR: 1.0e-06\n",
      "üìà Step  1050 | Loss: 7.1083 | Avg: 7.0713 | LR: 1.0e-06\n",
      "üìà Step  1075 | Loss: 6.9002 | Avg: 7.2584 | LR: 1.1e-06\n",
      "üìà Step  1100 | Loss: 6.0403 | Avg: 6.8180 | LR: 1.1e-06\n",
      "üìà Step  1125 | Loss: 7.3640 | Avg: 7.1124 | LR: 1.1e-06\n",
      "üìà Step  1150 | Loss: 7.4236 | Avg: 6.9940 | LR: 1.1e-06\n",
      "üìà Step  1175 | Loss: 5.5963 | Avg: 7.0898 | LR: 1.2e-06\n",
      "üìà Step  1200 | Loss: 7.5978 | Avg: 6.9652 | LR: 1.2e-06\n",
      "üìà Step  1225 | Loss: 6.4525 | Avg: 6.9283 | LR: 1.2e-06\n",
      "üìà Step  1250 | Loss: 8.2022 | Avg: 7.0228 | LR: 1.2e-06\n",
      "üìà Step  1275 | Loss: 7.7318 | Avg: 6.8463 | LR: 1.3e-06\n",
      "üìà Step  1300 | Loss: 5.9096 | Avg: 6.6706 | LR: 1.3e-06\n",
      "üìà Step  1325 | Loss: 6.3982 | Avg: 6.6342 | LR: 1.3e-06\n",
      "üìà Step  1350 | Loss: 6.3336 | Avg: 6.8019 | LR: 1.3e-06\n",
      "üìà Step  1375 | Loss: 6.9071 | Avg: 6.5955 | LR: 1.4e-06\n",
      "üìà Step  1400 | Loss: 6.8986 | Avg: 6.5174 | LR: 1.4e-06\n",
      "üìà Step  1425 | Loss: 6.7536 | Avg: 6.8142 | LR: 1.4e-06\n",
      "üìà Step  1450 | Loss: 7.1363 | Avg: 6.4728 | LR: 1.4e-06\n",
      "üìà Step  1475 | Loss: 7.4260 | Avg: 6.6531 | LR: 1.5e-06\n",
      "üìà Step  1500 | Loss: 6.5443 | Avg: 6.5848 | LR: 1.5e-06\n",
      "üìà Step  1525 | Loss: 5.9357 | Avg: 6.5572 | LR: 1.5e-06\n",
      "üìà Step  1550 | Loss: 6.0282 | Avg: 6.2657 | LR: 1.5e-06\n",
      "üìà Step  1575 | Loss: 4.9474 | Avg: 6.5763 | LR: 1.6e-06\n",
      "üìà Step  1600 | Loss: 5.8856 | Avg: 6.5134 | LR: 1.6e-06\n",
      "üìà Step  1625 | Loss: 7.1406 | Avg: 6.7022 | LR: 1.6e-06\n",
      "üìà Step  1650 | Loss: 7.1686 | Avg: 6.5688 | LR: 1.6e-06\n",
      "üìà Step  1675 | Loss: 5.8581 | Avg: 6.3959 | LR: 1.7e-06\n",
      "üìà Step  1700 | Loss: 5.2973 | Avg: 6.2930 | LR: 1.7e-06\n",
      "üìà Step  1725 | Loss: 7.4250 | Avg: 6.6119 | LR: 1.7e-06\n",
      "üìà Step  1750 | Loss: 6.5794 | Avg: 6.2674 | LR: 1.7e-06\n",
      "üìà Step  1775 | Loss: 7.0487 | Avg: 6.3202 | LR: 1.8e-06\n",
      "üìà Step  1800 | Loss: 6.3677 | Avg: 6.4107 | LR: 1.8e-06\n",
      "üìà Step  1825 | Loss: 5.9202 | Avg: 6.2285 | LR: 1.8e-06\n",
      "üìà Step  1850 | Loss: 6.3984 | Avg: 6.2924 | LR: 1.9e-06\n",
      "üìà Step  1875 | Loss: 5.4526 | Avg: 6.2114 | LR: 1.9e-06\n",
      "üìà Step  1900 | Loss: 5.3697 | Avg: 6.3701 | LR: 1.9e-06\n",
      "üìà Step  1925 | Loss: 5.7748 | Avg: 6.0306 | LR: 1.9e-06\n",
      "üìà Step  1950 | Loss: 6.3178 | Avg: 6.2620 | LR: 1.9e-06\n",
      "üìà Step  1975 | Loss: 6.2279 | Avg: 6.2010 | LR: 2.0e-06\n",
      "üìà Step  2000 | Loss: 5.9202 | Avg: 6.1991 | LR: 2.0e-06\n",
      "üìà Step  2025 | Loss: 5.4119 | Avg: 5.7271 | LR: 2.0e-06\n",
      "üìà Step  2050 | Loss: 6.5033 | Avg: 5.9811 | LR: 2.0e-06\n",
      "üìà Step  2075 | Loss: 6.3848 | Avg: 5.9949 | LR: 2.0e-06\n",
      "üìà Step  2100 | Loss: 5.9845 | Avg: 5.8942 | LR: 2.0e-06\n",
      "üìà Step  2125 | Loss: 6.8119 | Avg: 6.1230 | LR: 2.0e-06\n",
      "üìà Step  2150 | Loss: 6.3944 | Avg: 5.9825 | LR: 2.0e-06\n",
      "üìà Step  2175 | Loss: 6.2460 | Avg: 5.7695 | LR: 2.0e-06\n",
      "üìà Step  2200 | Loss: 5.4906 | Avg: 6.0759 | LR: 2.0e-06\n",
      "üìà Step  2225 | Loss: 5.8116 | Avg: 5.8862 | LR: 2.0e-06\n",
      "üìà Step  2250 | Loss: 5.9591 | Avg: 5.6847 | LR: 2.0e-06\n",
      "üìà Step  2275 | Loss: 5.4728 | Avg: 5.8562 | LR: 2.0e-06\n",
      "üìà Step  2300 | Loss: 5.9363 | Avg: 5.7780 | LR: 2.0e-06\n",
      "üìà Step  2325 | Loss: 5.1305 | Avg: 5.7829 | LR: 2.0e-06\n",
      "üìà Step  2350 | Loss: 6.4782 | Avg: 5.7379 | LR: 2.0e-06\n",
      "üìà Step  2375 | Loss: 5.7658 | Avg: 5.4922 | LR: 2.0e-06\n",
      "üìà Step  2400 | Loss: 5.9735 | Avg: 5.8536 | LR: 2.0e-06\n",
      "üìà Step  2425 | Loss: 5.4604 | Avg: 5.4959 | LR: 2.0e-06\n",
      "üìà Step  2450 | Loss: 5.1419 | Avg: 5.7769 | LR: 2.0e-06\n",
      "üìà Step  2475 | Loss: 5.5754 | Avg: 5.5341 | LR: 2.0e-06\n",
      "üìà Step  2500 | Loss: 5.9842 | Avg: 5.5074 | LR: 2.0e-06\n",
      "üìà Step  2525 | Loss: 5.7246 | Avg: 5.5980 | LR: 2.0e-06\n",
      "üìà Step  2550 | Loss: 5.6045 | Avg: 5.5206 | LR: 2.0e-06\n",
      "üìà Step  2575 | Loss: 5.5438 | Avg: 5.4440 | LR: 2.0e-06\n",
      "üìà Step  2600 | Loss: 5.1832 | Avg: 5.5267 | LR: 2.0e-06\n",
      "üìà Step  2625 | Loss: 5.4780 | Avg: 5.4618 | LR: 2.0e-06\n",
      "üìà Step  2650 | Loss: 5.4501 | Avg: 5.6259 | LR: 2.0e-06\n",
      "üìà Step  2675 | Loss: 5.4819 | Avg: 5.4829 | LR: 1.9e-06\n",
      "üìà Step  2700 | Loss: 4.9909 | Avg: 5.3303 | LR: 1.9e-06\n",
      "üìà Step  2725 | Loss: 6.6208 | Avg: 5.5464 | LR: 1.9e-06\n",
      "üìà Step  2750 | Loss: 5.5794 | Avg: 5.4909 | LR: 1.9e-06\n",
      "üìà Step  2775 | Loss: 5.9622 | Avg: 5.2463 | LR: 1.9e-06\n",
      "üìà Step  2800 | Loss: 5.4585 | Avg: 5.3025 | LR: 1.9e-06\n",
      "üìà Step  2825 | Loss: 4.0923 | Avg: 5.3345 | LR: 1.9e-06\n",
      "üìà Step  2850 | Loss: 5.4241 | Avg: 5.1536 | LR: 1.9e-06\n",
      "üìà Step  2875 | Loss: 5.1805 | Avg: 5.5972 | LR: 1.9e-06\n",
      "üìà Step  2900 | Loss: 5.2300 | Avg: 5.4734 | LR: 1.9e-06\n",
      "üìà Step  2925 | Loss: 5.8312 | Avg: 5.3333 | LR: 1.9e-06\n",
      "üìà Step  2950 | Loss: 5.5649 | Avg: 5.3618 | LR: 1.9e-06\n",
      "üìà Step  2975 | Loss: 4.5239 | Avg: 5.0945 | LR: 1.9e-06\n",
      "üìà Step  3000 | Loss: 5.0815 | Avg: 5.3453 | LR: 1.9e-06\n",
      "üìà Step  3025 | Loss: 5.5387 | Avg: 5.2068 | LR: 1.9e-06\n",
      "üìà Step  3050 | Loss: 5.4625 | Avg: 5.3986 | LR: 1.9e-06\n",
      "üìà Step  3075 | Loss: 6.1228 | Avg: 5.2811 | LR: 1.9e-06\n",
      "üìà Step  3100 | Loss: 5.7516 | Avg: 5.2209 | LR: 1.9e-06\n",
      "üìà Step  3125 | Loss: 5.3836 | Avg: 5.4751 | LR: 1.9e-06\n",
      "üìà Step  3150 | Loss: 5.4081 | Avg: 5.2222 | LR: 1.9e-06\n",
      "üìà Step  3175 | Loss: 4.5094 | Avg: 4.8463 | LR: 1.8e-06\n",
      "üìà Step  3200 | Loss: 5.8216 | Avg: 5.2066 | LR: 1.8e-06\n",
      "üìà Step  3225 | Loss: 4.6591 | Avg: 5.2527 | LR: 1.8e-06\n",
      "üìà Step  3250 | Loss: 4.2586 | Avg: 4.9362 | LR: 1.8e-06\n",
      "üìà Step  3275 | Loss: 6.2020 | Avg: 5.1799 | LR: 1.8e-06\n",
      "üìà Step  3300 | Loss: 4.7287 | Avg: 4.6754 | LR: 1.8e-06\n",
      "üìà Step  3325 | Loss: 5.5194 | Avg: 5.0262 | LR: 1.8e-06\n",
      "üìà Step  3350 | Loss: 4.5366 | Avg: 4.9998 | LR: 1.8e-06\n",
      "üìà Step  3375 | Loss: 5.2554 | Avg: 4.8874 | LR: 1.8e-06\n",
      "üìà Step  3400 | Loss: 5.3969 | Avg: 4.9296 | LR: 1.8e-06\n",
      "üìà Step  3425 | Loss: 4.6938 | Avg: 4.9831 | LR: 1.8e-06\n",
      "[2025-09-29 13:07:00,821] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "üíæ Stable checkpoint saved at 15% of epoch 1\n",
      "üìà Step  3450 | Loss: 6.2932 | Avg: 4.8560 | LR: 1.8e-06\n",
      "üìà Step  3475 | Loss: 4.5223 | Avg: 5.1304 | LR: 1.8e-06\n",
      "üìà Step  3500 | Loss: 5.1603 | Avg: 4.9459 | LR: 1.8e-06\n",
      "üìà Step  3525 | Loss: 4.6063 | Avg: 5.1142 | LR: 1.7e-06\n",
      "üìà Step  3550 | Loss: 5.1466 | Avg: 4.8992 | LR: 1.7e-06\n",
      "üìà Step  3575 | Loss: 4.8554 | Avg: 4.8671 | LR: 1.7e-06\n",
      "üìà Step  3600 | Loss: 4.1784 | Avg: 4.9279 | LR: 1.7e-06\n",
      "üìà Step  3625 | Loss: 4.5849 | Avg: 4.8652 | LR: 1.7e-06\n",
      "üìà Step  3650 | Loss: 5.0999 | Avg: 4.8436 | LR: 1.7e-06\n",
      "üìà Step  3675 | Loss: 4.6482 | Avg: 4.8595 | LR: 1.7e-06\n",
      "üìà Step  3700 | Loss: 5.4910 | Avg: 4.6598 | LR: 1.7e-06\n",
      "üìà Step  3725 | Loss: 4.6961 | Avg: 4.8047 | LR: 1.7e-06\n",
      "üìà Step  3750 | Loss: 4.6475 | Avg: 4.9745 | LR: 1.7e-06\n",
      "üìà Step  3775 | Loss: 4.4164 | Avg: 4.5831 | LR: 1.7e-06\n",
      "üìà Step  3800 | Loss: 5.4340 | Avg: 4.9067 | LR: 1.7e-06\n",
      "üìà Step  3825 | Loss: 6.3970 | Avg: 4.9284 | LR: 1.6e-06\n",
      "üìà Step  3850 | Loss: 3.8531 | Avg: 4.6361 | LR: 1.6e-06\n",
      "üìà Step  3875 | Loss: 4.6016 | Avg: 4.6888 | LR: 1.6e-06\n",
      "üìà Step  3900 | Loss: 4.0660 | Avg: 4.6944 | LR: 1.6e-06\n",
      "üìà Step  3925 | Loss: 3.5445 | Avg: 4.5169 | LR: 1.6e-06\n",
      "üìà Step  3950 | Loss: 4.3003 | Avg: 4.4204 | LR: 1.6e-06\n",
      "üìà Step  3975 | Loss: 5.9236 | Avg: 4.7576 | LR: 1.6e-06\n",
      "üìà Step  4000 | Loss: 5.5232 | Avg: 4.7661 | LR: 1.6e-06\n",
      "üìà Step  4025 | Loss: 4.8497 | Avg: 5.0426 | LR: 1.6e-06\n",
      "üìà Step  4050 | Loss: 4.4616 | Avg: 4.6316 | LR: 1.6e-06\n",
      "üìà Step  4075 | Loss: 4.9490 | Avg: 4.6586 | LR: 1.5e-06\n",
      "üìà Step  4100 | Loss: 4.9962 | Avg: 4.8414 | LR: 1.5e-06\n",
      "üìà Step  4125 | Loss: 3.1519 | Avg: 4.7382 | LR: 1.5e-06\n",
      "üìà Step  4150 | Loss: 5.3115 | Avg: 4.5457 | LR: 1.5e-06\n",
      "üìà Step  4175 | Loss: 4.1585 | Avg: 4.6326 | LR: 1.5e-06\n",
      "üìà Step  4200 | Loss: 4.5269 | Avg: 4.6786 | LR: 1.5e-06\n",
      "üìà Step  4225 | Loss: 4.8189 | Avg: 4.8112 | LR: 1.5e-06\n",
      "üìà Step  4250 | Loss: 4.3777 | Avg: 4.8817 | LR: 1.5e-06\n",
      "üìà Step  4275 | Loss: 5.3292 | Avg: 4.6573 | LR: 1.5e-06\n",
      "üìà Step  4300 | Loss: 4.1343 | Avg: 4.6106 | LR: 1.5e-06\n",
      "üìà Step  4325 | Loss: 4.5778 | Avg: 4.4281 | LR: 1.4e-06\n",
      "üìà Step  4350 | Loss: 4.5872 | Avg: 4.7531 | LR: 1.4e-06\n",
      "üìà Step  4375 | Loss: 4.0846 | Avg: 4.3601 | LR: 1.4e-06\n",
      "üìà Step  4400 | Loss: 4.9773 | Avg: 4.6189 | LR: 1.4e-06\n",
      "üìà Step  4425 | Loss: 4.9390 | Avg: 4.6385 | LR: 1.4e-06\n",
      "üìà Step  4450 | Loss: 4.5083 | Avg: 4.6853 | LR: 1.4e-06\n",
      "üìà Step  4475 | Loss: 4.1505 | Avg: 4.5251 | LR: 1.4e-06\n",
      "üìà Step  4500 | Loss: 4.9064 | Avg: 4.5697 | LR: 1.4e-06\n",
      "üìà Step  4525 | Loss: 4.5456 | Avg: 4.3600 | LR: 1.4e-06\n",
      "üìà Step  4550 | Loss: 4.0521 | Avg: 4.5649 | LR: 1.3e-06\n",
      "üìà Step  4575 | Loss: 4.4475 | Avg: 4.4170 | LR: 1.3e-06\n",
      "üìà Step  4600 | Loss: 4.0159 | Avg: 4.2874 | LR: 1.3e-06\n",
      "üìà Step  4625 | Loss: 5.1889 | Avg: 4.4221 | LR: 1.3e-06\n",
      "üìà Step  4650 | Loss: 3.7952 | Avg: 4.3501 | LR: 1.3e-06\n",
      "üìà Step  4675 | Loss: 4.8923 | Avg: 4.4032 | LR: 1.3e-06\n",
      "üìà Step  4700 | Loss: 4.6472 | Avg: 4.3646 | LR: 1.3e-06\n",
      "üìà Step  4725 | Loss: 3.7756 | Avg: 4.3920 | LR: 1.3e-06\n",
      "üìà Step  4750 | Loss: 4.8524 | Avg: 4.5462 | LR: 1.3e-06\n",
      "üìà Step  4775 | Loss: 4.2521 | Avg: 4.3930 | LR: 1.2e-06\n",
      "üìà Step  4800 | Loss: 4.4216 | Avg: 4.5608 | LR: 1.2e-06\n",
      "üìà Step  4825 | Loss: 1.9017 | Avg: 4.2299 | LR: 1.2e-06\n",
      "üìà Step  4850 | Loss: 3.8222 | Avg: 4.3716 | LR: 1.2e-06\n",
      "üìà Step  4875 | Loss: 5.5641 | Avg: 4.6310 | LR: 1.2e-06\n",
      "üìà Step  4900 | Loss: 5.1775 | Avg: 4.3967 | LR: 1.2e-06\n",
      "üìà Step  4925 | Loss: 3.6373 | Avg: 4.4099 | LR: 1.2e-06\n",
      "üìà Step  4950 | Loss: 4.4171 | Avg: 4.5717 | LR: 1.2e-06\n",
      "üìà Step  4975 | Loss: 4.7360 | Avg: 4.3836 | LR: 1.1e-06\n",
      "üìà Step  5000 | Loss: 3.8952 | Avg: 4.3760 | LR: 1.1e-06\n",
      "üìà Step  5025 | Loss: 4.2738 | Avg: 4.2475 | LR: 1.1e-06\n",
      "üìà Step  5050 | Loss: 4.0757 | Avg: 4.3595 | LR: 1.1e-06\n",
      "üìà Step  5075 | Loss: 3.7910 | Avg: 4.2521 | LR: 1.1e-06\n",
      "üìà Step  5100 | Loss: 4.0261 | Avg: 4.5537 | LR: 1.1e-06\n",
      "üìà Step  5125 | Loss: 3.2866 | Avg: 4.1450 | LR: 1.1e-06\n",
      "üìà Step  5150 | Loss: 4.2175 | Avg: 4.2372 | LR: 1.1e-06\n",
      "üìà Step  5175 | Loss: 3.6767 | Avg: 4.4925 | LR: 1.1e-06\n",
      "üìà Step  5200 | Loss: 4.3966 | Avg: 4.3047 | LR: 1.0e-06\n",
      "üìà Step  5225 | Loss: 4.2831 | Avg: 4.5764 | LR: 1.0e-06\n",
      "üìà Step  5250 | Loss: 4.5620 | Avg: 4.2748 | LR: 1.0e-06\n",
      "üìà Step  5275 | Loss: 2.8244 | Avg: 4.1103 | LR: 1.0e-06\n",
      "üìà Step  5300 | Loss: 3.9780 | Avg: 4.4316 | LR: 9.9e-07\n",
      "üìà Step  5325 | Loss: 5.5076 | Avg: 4.3159 | LR: 9.8e-07\n",
      "üìà Step  5350 | Loss: 5.0779 | Avg: 4.0958 | LR: 9.7e-07\n",
      "üìà Step  5375 | Loss: 4.7432 | Avg: 4.1794 | LR: 9.6e-07\n",
      "üìà Step  5400 | Loss: 4.1367 | Avg: 4.0598 | LR: 9.5e-07\n",
      "üìà Step  5425 | Loss: 4.9192 | Avg: 4.3408 | LR: 9.3e-07\n",
      "üìà Step  5450 | Loss: 4.0881 | Avg: 4.2067 | LR: 9.2e-07\n",
      "üìà Step  5475 | Loss: 4.4761 | Avg: 4.3642 | LR: 9.1e-07\n",
      "üìà Step  5500 | Loss: 3.8022 | Avg: 4.2534 | LR: 9.0e-07\n",
      "üìà Step  5525 | Loss: 4.3453 | Avg: 4.4967 | LR: 8.9e-07\n",
      "üìà Step  5550 | Loss: 3.8653 | Avg: 3.9738 | LR: 8.7e-07\n",
      "üìà Step  5575 | Loss: 4.9171 | Avg: 4.2835 | LR: 8.6e-07\n",
      "üìà Step  5600 | Loss: 3.1861 | Avg: 4.0487 | LR: 8.5e-07\n",
      "üìà Step  5625 | Loss: 3.5932 | Avg: 4.2387 | LR: 8.4e-07\n",
      "üìà Step  5650 | Loss: 4.3035 | Avg: 4.2902 | LR: 8.3e-07\n",
      "üìà Step  5675 | Loss: 5.5859 | Avg: 4.3105 | LR: 8.2e-07\n",
      "üìà Step  5700 | Loss: 3.5067 | Avg: 4.1473 | LR: 8.0e-07\n",
      "üìà Step  5725 | Loss: 4.7247 | Avg: 4.1555 | LR: 7.9e-07\n",
      "üìà Step  5750 | Loss: 4.1206 | Avg: 4.2414 | LR: 7.8e-07\n",
      "üìà Step  5775 | Loss: 4.0417 | Avg: 4.3097 | LR: 7.7e-07\n",
      "üìà Step  5800 | Loss: 4.3535 | Avg: 4.3893 | LR: 7.6e-07\n",
      "üìà Step  5825 | Loss: 3.5070 | Avg: 4.2073 | LR: 7.5e-07\n",
      "üìà Step  5850 | Loss: 4.1435 | Avg: 4.0893 | LR: 7.3e-07\n",
      "üìà Step  5875 | Loss: 3.8894 | Avg: 4.1274 | LR: 7.2e-07\n",
      "üìà Step  5900 | Loss: 4.7485 | Avg: 4.1774 | LR: 7.1e-07\n",
      "üìà Step  5925 | Loss: 4.2701 | Avg: 4.3407 | LR: 7.0e-07\n",
      "üìà Step  5950 | Loss: 5.3415 | Avg: 4.1485 | LR: 6.9e-07\n",
      "üìà Step  5975 | Loss: 3.6435 | Avg: 4.0174 | LR: 6.8e-07\n",
      "üìà Step  6000 | Loss: 4.9827 | Avg: 3.9834 | LR: 6.7e-07\n",
      "üìà Step  6025 | Loss: 3.1860 | Avg: 4.0028 | LR: 6.5e-07\n",
      "üìà Step  6050 | Loss: 5.6305 | Avg: 4.3285 | LR: 6.4e-07\n",
      "üìà Step  6075 | Loss: 3.8793 | Avg: 4.1049 | LR: 6.3e-07\n",
      "üìà Step  6100 | Loss: 3.8914 | Avg: 4.2801 | LR: 6.2e-07\n",
      "üìà Step  6125 | Loss: 4.8036 | Avg: 4.2812 | LR: 6.1e-07\n",
      "üìà Step  6150 | Loss: 3.7518 | Avg: 4.0118 | LR: 6.0e-07\n",
      "üìà Step  6175 | Loss: 4.0365 | Avg: 4.0729 | LR: 5.9e-07\n",
      "üìà Step  6200 | Loss: 4.0485 | Avg: 3.9078 | LR: 5.8e-07\n",
      "üìà Step  6225 | Loss: 4.2937 | Avg: 4.0390 | LR: 5.7e-07\n",
      "üìà Step  6250 | Loss: 3.9333 | Avg: 4.3964 | LR: 5.6e-07\n",
      "üìà Step  6275 | Loss: 2.3251 | Avg: 3.9547 | LR: 5.5e-07\n",
      "üìà Step  6300 | Loss: 4.0240 | Avg: 4.0276 | LR: 5.3e-07\n",
      "üìà Step  6325 | Loss: 4.7418 | Avg: 4.1091 | LR: 5.2e-07\n",
      "üìà Step  6350 | Loss: 3.7027 | Avg: 4.2777 | LR: 5.1e-07\n",
      "üìà Step  6375 | Loss: 3.7019 | Avg: 4.2899 | LR: 5.0e-07\n",
      "üìà Step  6400 | Loss: 3.7110 | Avg: 4.1319 | LR: 4.9e-07\n",
      "üìà Step  6425 | Loss: 4.1192 | Avg: 4.0945 | LR: 4.8e-07\n",
      "üìà Step  6450 | Loss: 3.8193 | Avg: 4.1131 | LR: 4.7e-07\n",
      "üìà Step  6475 | Loss: 3.1523 | Avg: 4.0410 | LR: 4.6e-07\n",
      "üìà Step  6500 | Loss: 3.7468 | Avg: 4.3016 | LR: 4.5e-07\n",
      "üìà Step  6525 | Loss: 3.9052 | Avg: 3.9577 | LR: 4.4e-07\n",
      "üìà Step  6550 | Loss: 4.4702 | Avg: 4.2578 | LR: 4.3e-07\n",
      "üìà Step  6575 | Loss: 4.3633 | Avg: 4.1517 | LR: 4.2e-07\n",
      "üìà Step  6600 | Loss: 3.8561 | Avg: 4.0143 | LR: 4.1e-07\n",
      "üìà Step  6625 | Loss: 3.0866 | Avg: 4.0681 | LR: 4.0e-07\n",
      "üìà Step  6650 | Loss: 3.8990 | Avg: 4.1532 | LR: 3.9e-07\n",
      "üìà Step  6675 | Loss: 4.4687 | Avg: 4.0181 | LR: 3.8e-07\n",
      "üìà Step  6700 | Loss: 4.2951 | Avg: 4.1730 | LR: 3.7e-07\n",
      "üìà Step  6725 | Loss: 3.3901 | Avg: 4.1680 | LR: 3.7e-07\n",
      "üìà Step  6750 | Loss: 5.0140 | Avg: 4.2178 | LR: 3.6e-07\n",
      "üìà Step  6775 | Loss: 4.7777 | Avg: 4.1084 | LR: 3.5e-07\n",
      "üìà Step  6800 | Loss: 2.3720 | Avg: 4.0039 | LR: 3.4e-07\n",
      "üìà Step  6825 | Loss: 2.6873 | Avg: 4.0691 | LR: 3.3e-07\n",
      "üìà Step  6850 | Loss: 3.9382 | Avg: 4.1532 | LR: 3.2e-07\n",
      "üíæ Stable checkpoint saved at 30% of epoch 1\n",
      "üìà Step  6875 | Loss: 3.1309 | Avg: 4.1771 | LR: 3.1e-07\n",
      "üìà Step  6900 | Loss: 3.8418 | Avg: 4.1872 | LR: 3.0e-07\n",
      "üìà Step  6925 | Loss: 3.4014 | Avg: 4.0902 | LR: 2.9e-07\n",
      "üìà Step  6950 | Loss: 4.4178 | Avg: 3.9480 | LR: 2.9e-07\n",
      "üìà Step  6975 | Loss: 3.5294 | Avg: 4.1178 | LR: 2.8e-07\n",
      "üìà Step  7000 | Loss: 4.1690 | Avg: 3.9923 | LR: 2.7e-07\n",
      "üìà Step  7025 | Loss: 3.8900 | Avg: 4.0336 | LR: 2.6e-07\n",
      "üìà Step  7050 | Loss: 4.6386 | Avg: 4.2473 | LR: 2.5e-07\n",
      "üìà Step  7075 | Loss: 4.3460 | Avg: 4.0211 | LR: 2.5e-07\n",
      "üìà Step  7100 | Loss: 4.6625 | Avg: 4.2639 | LR: 2.4e-07\n",
      "üìà Step  7125 | Loss: 4.4231 | Avg: 3.7983 | LR: 2.3e-07\n",
      "üìà Step  7150 | Loss: 4.9201 | Avg: 4.0769 | LR: 2.2e-07\n",
      "üìà Step  7175 | Loss: 4.5624 | Avg: 4.1931 | LR: 2.2e-07\n",
      "üìà Step  7200 | Loss: 3.5557 | Avg: 4.1458 | LR: 2.1e-07\n",
      "üìà Step  7225 | Loss: 4.5907 | Avg: 4.0976 | LR: 2.0e-07\n",
      "üìà Step  7250 | Loss: 4.2340 | Avg: 3.9281 | LR: 1.9e-07\n",
      "üìà Step  7275 | Loss: 4.5203 | Avg: 4.2787 | LR: 1.9e-07\n",
      "üìà Step  7300 | Loss: 4.4939 | Avg: 4.0601 | LR: 1.8e-07\n",
      "üìà Step  7325 | Loss: 3.3784 | Avg: 4.0372 | LR: 1.7e-07\n",
      "üìà Step  7350 | Loss: 3.6883 | Avg: 4.1908 | LR: 1.7e-07\n",
      "üìà Step  7375 | Loss: 4.7420 | Avg: 3.9698 | LR: 1.6e-07\n",
      "üìà Step  7400 | Loss: 3.9235 | Avg: 4.1601 | LR: 1.5e-07\n",
      "üìà Step  7425 | Loss: 3.8295 | Avg: 4.1057 | LR: 1.5e-07\n",
      "üìà Step  7450 | Loss: 4.1397 | Avg: 3.9401 | LR: 1.4e-07\n",
      "üìà Step  7475 | Loss: 4.5691 | Avg: 3.9232 | LR: 1.3e-07\n",
      "üìà Step  7500 | Loss: 4.4114 | Avg: 4.1746 | LR: 1.3e-07\n",
      "üìà Step  7525 | Loss: 3.9647 | Avg: 3.8349 | LR: 1.2e-07\n",
      "üìà Step  7550 | Loss: 5.5336 | Avg: 4.1751 | LR: 1.2e-07\n",
      "üìà Step  7575 | Loss: 4.5403 | Avg: 4.0913 | LR: 1.1e-07\n",
      "üìà Step  7600 | Loss: 4.1920 | Avg: 4.1737 | LR: 1.1e-07\n",
      "üìà Step  7625 | Loss: 2.9019 | Avg: 4.2801 | LR: 1.0e-07\n",
      "üìà Step  7650 | Loss: 4.4534 | Avg: 4.0550 | LR: 9.6e-08\n",
      "üìà Step  7675 | Loss: 4.7881 | Avg: 4.0963 | LR: 9.1e-08\n",
      "üìà Step  7700 | Loss: 3.9384 | Avg: 4.1452 | LR: 8.6e-08\n",
      "üìà Step  7725 | Loss: 3.7090 | Avg: 4.1211 | LR: 8.1e-08\n",
      "üìà Step  7750 | Loss: 3.8381 | Avg: 4.1895 | LR: 7.7e-08\n",
      "üìà Step  7775 | Loss: 3.0022 | Avg: 3.8937 | LR: 7.2e-08\n",
      "üìà Step  7800 | Loss: 4.2186 | Avg: 4.0806 | LR: 6.8e-08\n",
      "üìà Step  7825 | Loss: 4.4567 | Avg: 4.0556 | LR: 6.3e-08\n",
      "üìà Step  7850 | Loss: 3.8485 | Avg: 3.8651 | LR: 5.9e-08\n",
      "üìà Step  7875 | Loss: 3.3596 | Avg: 4.0093 | LR: 5.5e-08\n",
      "üìà Step  7900 | Loss: 4.6089 | Avg: 4.1434 | LR: 5.1e-08\n",
      "üìà Step  7925 | Loss: 4.6275 | Avg: 4.2193 | LR: 4.8e-08\n",
      "üìà Step  7950 | Loss: 4.7596 | Avg: 4.1896 | LR: 4.4e-08\n",
      "üìà Step  7975 | Loss: 4.6920 | Avg: 4.2162 | LR: 4.1e-08\n",
      "üìà Step  8000 | Loss: 3.9685 | Avg: 4.0535 | LR: 3.7e-08\n",
      "üìà Step  8025 | Loss: 3.9180 | Avg: 4.2403 | LR: 3.4e-08\n",
      "üìà Step  8050 | Loss: 3.4904 | Avg: 4.0049 | LR: 3.1e-08\n",
      "üìà Step  8075 | Loss: 4.3200 | Avg: 4.1579 | LR: 2.8e-08\n",
      "üìà Step  8100 | Loss: 4.8798 | Avg: 4.1169 | LR: 2.6e-08\n",
      "üìà Step  8125 | Loss: 3.5778 | Avg: 3.9672 | LR: 2.3e-08\n",
      "üìà Step  8150 | Loss: 4.7806 | Avg: 3.8466 | LR: 2.0e-08\n",
      "üìà Step  8175 | Loss: 3.9615 | Avg: 4.0198 | LR: 1.8e-08\n",
      "üìà Step  8200 | Loss: 4.1919 | Avg: 3.9484 | LR: 1.6e-08\n",
      "üìà Step  8225 | Loss: 3.4770 | Avg: 3.7309 | LR: 1.4e-08\n",
      "üìà Step  8250 | Loss: 4.4432 | Avg: 4.2898 | LR: 1.2e-08\n",
      "üìà Step  8275 | Loss: 4.3162 | Avg: 3.9618 | LR: 1.0e-08\n",
      "üìà Step  8300 | Loss: 4.4932 | Avg: 4.0817 | LR: 8.6e-09\n",
      "üìà Step  8325 | Loss: 3.7293 | Avg: 3.9561 | LR: 7.1e-09\n",
      "üìà Step  8350 | Loss: 3.3777 | Avg: 3.9489 | LR: 5.7e-09\n",
      "üìà Step  8375 | Loss: 4.6944 | Avg: 4.1246 | LR: 4.5e-09\n",
      "üìà Step  8400 | Loss: 4.1449 | Avg: 3.9616 | LR: 3.5e-09\n",
      "üìà Step  8425 | Loss: 3.4045 | Avg: 4.0265 | LR: 2.5e-09\n",
      "üìà Step  8450 | Loss: 3.9351 | Avg: 3.8333 | LR: 1.8e-09\n",
      "üìà Step  8475 | Loss: 3.5793 | Avg: 4.1259 | LR: 1.1e-09\n",
      "üìà Step  8500 | Loss: 4.9925 | Avg: 4.0970 | LR: 6.3e-10\n",
      "üìà Step  8525 | Loss: 3.8544 | Avg: 4.2813 | LR: 2.7e-10\n",
      "üìà Step  8550 | Loss: 4.3342 | Avg: 4.0706 | LR: 6.6e-11\n",
      "üìà Step  8575 | Loss: 2.8639 | Avg: 4.0471 | LR: 1.1e-13\n",
      "üìà Step  8600 | Loss: 4.1472 | Avg: 4.1891 | LR: 7.7e-11\n",
      "üìà Step  8625 | Loss: 4.6051 | Avg: 4.1488 | LR: 3.0e-10\n",
      "üìà Step  8650 | Loss: 4.7072 | Avg: 4.0409 | LR: 6.6e-10\n",
      "üìà Step  8675 | Loss: 4.0226 | Avg: 4.0304 | LR: 1.2e-09\n",
      "üìà Step  8700 | Loss: 5.2297 | Avg: 4.0285 | LR: 1.8e-09\n",
      "üìà Step  8725 | Loss: 3.3094 | Avg: 4.2959 | LR: 2.6e-09\n",
      "üìà Step  8750 | Loss: 4.2090 | Avg: 4.0553 | LR: 3.5e-09\n",
      "üìà Step  8775 | Loss: 4.0173 | Avg: 4.1872 | LR: 4.6e-09\n",
      "üìà Step  8800 | Loss: 4.8756 | Avg: 3.9511 | LR: 5.8e-09\n",
      "üìà Step  8825 | Loss: 4.8330 | Avg: 4.0639 | LR: 7.2e-09\n",
      "üìà Step  8850 | Loss: 3.7334 | Avg: 4.1270 | LR: 8.7e-09\n",
      "üìà Step  8875 | Loss: 3.6641 | Avg: 4.0863 | LR: 1.0e-08\n",
      "üìà Step  8900 | Loss: 3.3042 | Avg: 3.9616 | LR: 1.2e-08\n",
      "üìà Step  8925 | Loss: 5.1848 | Avg: 4.0150 | LR: 1.4e-08\n",
      "üìà Step  8950 | Loss: 4.3123 | Avg: 4.3104 | LR: 1.6e-08\n",
      "üìà Step  8975 | Loss: 3.7839 | Avg: 4.1135 | LR: 1.8e-08\n",
      "üìà Step  9000 | Loss: 4.6337 | Avg: 4.0307 | LR: 2.1e-08\n",
      "üìà Step  9025 | Loss: 3.1584 | Avg: 3.9491 | LR: 2.3e-08\n",
      "üìà Step  9050 | Loss: 3.5405 | Avg: 3.8369 | LR: 2.6e-08\n",
      "üìà Step  9075 | Loss: 3.6349 | Avg: 3.8869 | LR: 2.9e-08\n",
      "üìà Step  9100 | Loss: 4.4613 | Avg: 4.1167 | LR: 3.1e-08\n",
      "üìà Step  9125 | Loss: 3.5130 | Avg: 3.9653 | LR: 3.4e-08\n",
      "üìà Step  9150 | Loss: 4.3344 | Avg: 3.9864 | LR: 3.8e-08\n",
      "üìà Step  9175 | Loss: 3.7677 | Avg: 4.0808 | LR: 4.1e-08\n",
      "üìà Step  9200 | Loss: 4.6317 | Avg: 4.2346 | LR: 4.4e-08\n",
      "üìà Step  9225 | Loss: 4.2297 | Avg: 4.0331 | LR: 4.8e-08\n",
      "üìà Step  9250 | Loss: 3.5831 | Avg: 4.0864 | LR: 5.2e-08\n",
      "üìà Step  9275 | Loss: 4.7166 | Avg: 3.8167 | LR: 5.6e-08\n",
      "üìà Step  9300 | Loss: 4.3290 | Avg: 4.0315 | LR: 6.0e-08\n",
      "üìà Step  9325 | Loss: 4.0761 | Avg: 4.0270 | LR: 6.4e-08\n",
      "üìà Step  9350 | Loss: 2.9439 | Avg: 3.9321 | LR: 6.8e-08\n",
      "üìà Step  9375 | Loss: 3.6059 | Avg: 4.0482 | LR: 7.2e-08\n",
      "üìà Step  9400 | Loss: 4.7610 | Avg: 4.2889 | LR: 7.7e-08\n",
      "üìà Step  9425 | Loss: 4.1334 | Avg: 4.2293 | LR: 8.2e-08\n",
      "üìà Step  9450 | Loss: 4.1522 | Avg: 3.8859 | LR: 8.6e-08\n",
      "üìà Step  9475 | Loss: 3.8408 | Avg: 3.9374 | LR: 9.1e-08\n",
      "üìà Step  9500 | Loss: 3.4024 | Avg: 3.8410 | LR: 9.6e-08\n",
      "üìà Step  9525 | Loss: 4.2419 | Avg: 3.9831 | LR: 1.0e-07\n",
      "üìà Step  9550 | Loss: 5.0787 | Avg: 4.0814 | LR: 1.1e-07\n",
      "üìà Step  9575 | Loss: 4.0426 | Avg: 4.1087 | LR: 1.1e-07\n",
      "üìà Step  9600 | Loss: 4.4637 | Avg: 4.2033 | LR: 1.2e-07\n",
      "üìà Step  9625 | Loss: 4.2262 | Avg: 4.3178 | LR: 1.2e-07\n",
      "üìà Step  9650 | Loss: 4.3736 | Avg: 4.1603 | LR: 1.3e-07\n",
      "üìà Step  9675 | Loss: 3.3507 | Avg: 3.9554 | LR: 1.4e-07\n",
      "üìà Step  9700 | Loss: 4.1592 | Avg: 3.9734 | LR: 1.4e-07\n",
      "üìà Step  9725 | Loss: 3.6525 | Avg: 4.0253 | LR: 1.5e-07\n",
      "üìà Step  9750 | Loss: 2.9501 | Avg: 3.9856 | LR: 1.5e-07\n",
      "üìà Step  9775 | Loss: 3.7135 | Avg: 4.1611 | LR: 1.6e-07\n",
      "üìà Step  9800 | Loss: 3.0891 | Avg: 3.9162 | LR: 1.7e-07\n",
      "üìà Step  9825 | Loss: 3.6069 | Avg: 3.8863 | LR: 1.7e-07\n",
      "üìà Step  9850 | Loss: 3.4315 | Avg: 3.8959 | LR: 1.8e-07\n",
      "üìà Step  9875 | Loss: 3.4357 | Avg: 4.0661 | LR: 1.9e-07\n",
      "üìà Step  9900 | Loss: 4.5946 | Avg: 4.0681 | LR: 1.9e-07\n",
      "üìà Step  9925 | Loss: 4.8246 | Avg: 4.2368 | LR: 2.0e-07\n",
      "üìà Step  9950 | Loss: 2.8310 | Avg: 4.1162 | LR: 2.1e-07\n",
      "üìà Step  9975 | Loss: 3.7414 | Avg: 4.0593 | LR: 2.2e-07\n",
      "üìà Step 10000 | Loss: 4.2413 | Avg: 4.1542 | LR: 2.2e-07\n",
      "üìà Step 10025 | Loss: 4.2304 | Avg: 3.9650 | LR: 2.3e-07\n",
      "üìà Step 10050 | Loss: 3.5278 | Avg: 3.8974 | LR: 2.4e-07\n",
      "üìà Step 10075 | Loss: 4.7792 | Avg: 3.8731 | LR: 2.5e-07\n",
      "üìà Step 10100 | Loss: 3.5207 | Avg: 4.0578 | LR: 2.5e-07\n",
      "üìà Step 10125 | Loss: 3.9513 | Avg: 3.9040 | LR: 2.6e-07\n",
      "üìà Step 10150 | Loss: 3.1446 | Avg: 3.9644 | LR: 2.7e-07\n",
      "üìà Step 10175 | Loss: 5.0221 | Avg: 3.8814 | LR: 2.8e-07\n",
      "üìà Step 10200 | Loss: 5.1137 | Avg: 4.1111 | LR: 2.9e-07\n",
      "üìà Step 10225 | Loss: 4.2871 | Avg: 3.8101 | LR: 3.0e-07\n",
      "üìà Step 10250 | Loss: 4.7853 | Avg: 4.1893 | LR: 3.0e-07\n",
      "üìà Step 10275 | Loss: 4.3019 | Avg: 3.8306 | LR: 3.1e-07\n",
      "üíæ Stable checkpoint saved at 45% of epoch 1\n",
      "üìà Step 10300 | Loss: 4.1849 | Avg: 3.9275 | LR: 3.2e-07\n",
      "üìà Step 10325 | Loss: 4.0573 | Avg: 4.2401 | LR: 3.3e-07\n",
      "üìà Step 10350 | Loss: 4.0972 | Avg: 4.0197 | LR: 3.4e-07\n",
      "üìà Step 10375 | Loss: 4.7099 | Avg: 3.9635 | LR: 3.5e-07\n",
      "üìà Step 10400 | Loss: 4.7708 | Avg: 4.0372 | LR: 3.6e-07\n",
      "üìà Step 10425 | Loss: 3.1512 | Avg: 4.0009 | LR: 3.7e-07\n",
      "üìà Step 10450 | Loss: 4.4805 | Avg: 4.1019 | LR: 3.8e-07\n",
      "üìà Step 10475 | Loss: 3.6183 | Avg: 3.9297 | LR: 3.9e-07\n",
      "üìà Step 10500 | Loss: 3.8000 | Avg: 4.1654 | LR: 3.9e-07\n",
      "üìà Step 10525 | Loss: 3.4846 | Avg: 3.8739 | LR: 4.0e-07\n",
      "üìà Step 10550 | Loss: 3.2297 | Avg: 3.8839 | LR: 4.1e-07\n",
      "üìà Step 10575 | Loss: 3.9111 | Avg: 4.0178 | LR: 4.2e-07\n",
      "üìà Step 10600 | Loss: 4.4817 | Avg: 3.9819 | LR: 4.3e-07\n",
      "üìà Step 10625 | Loss: 4.4992 | Avg: 4.0976 | LR: 4.4e-07\n",
      "üìà Step 10650 | Loss: 4.5393 | Avg: 3.8396 | LR: 4.5e-07\n",
      "üìà Step 10675 | Loss: 4.6047 | Avg: 3.8178 | LR: 4.6e-07\n",
      "üìà Step 10700 | Loss: 4.2268 | Avg: 3.8023 | LR: 4.7e-07\n",
      "üìà Step 10725 | Loss: 3.4276 | Avg: 4.0877 | LR: 4.8e-07\n",
      "üìà Step 10750 | Loss: 4.5852 | Avg: 3.8972 | LR: 4.9e-07\n",
      "üìà Step 10775 | Loss: 2.9523 | Avg: 3.8961 | LR: 5.0e-07\n",
      "üìà Step 10800 | Loss: 4.9574 | Avg: 3.7618 | LR: 5.1e-07\n",
      "üìà Step 10825 | Loss: 3.9846 | Avg: 4.0405 | LR: 5.2e-07\n",
      "üìà Step 10850 | Loss: 3.1157 | Avg: 3.7003 | LR: 5.4e-07\n",
      "üìà Step 10875 | Loss: 3.5774 | Avg: 3.7394 | LR: 5.5e-07\n",
      "üìà Step 10900 | Loss: 4.6898 | Avg: 3.9765 | LR: 5.6e-07\n",
      "üìà Step 10925 | Loss: 3.6588 | Avg: 3.6998 | LR: 5.7e-07\n",
      "üìà Step 10950 | Loss: 3.8783 | Avg: 3.9038 | LR: 5.8e-07\n",
      "üìà Step 10975 | Loss: 4.0207 | Avg: 4.0757 | LR: 5.9e-07\n",
      "üìà Step 11000 | Loss: 5.0889 | Avg: 4.0057 | LR: 6.0e-07\n",
      "üìà Step 11025 | Loss: 3.9198 | Avg: 4.0641 | LR: 6.1e-07\n",
      "üìà Step 11050 | Loss: 5.1337 | Avg: 3.9400 | LR: 6.2e-07\n",
      "üìà Step 11075 | Loss: 4.3421 | Avg: 3.8173 | LR: 6.3e-07\n",
      "üìà Step 11100 | Loss: 3.9295 | Avg: 3.9960 | LR: 6.4e-07\n",
      "üìà Step 11125 | Loss: 3.5670 | Avg: 4.0214 | LR: 6.6e-07\n",
      "üìà Step 11150 | Loss: 3.3040 | Avg: 4.0371 | LR: 6.7e-07\n",
      "üìà Step 11175 | Loss: 3.5834 | Avg: 3.7131 | LR: 6.8e-07\n",
      "üìà Step 11200 | Loss: 4.5240 | Avg: 4.0286 | LR: 6.9e-07\n",
      "üìà Step 11225 | Loss: 4.2466 | Avg: 4.1054 | LR: 7.0e-07\n",
      "üìà Step 11250 | Loss: 4.5448 | Avg: 3.9131 | LR: 7.1e-07\n",
      "üìà Step 11275 | Loss: 4.3064 | Avg: 3.9065 | LR: 7.2e-07\n",
      "üìà Step 11300 | Loss: 3.8230 | Avg: 3.9673 | LR: 7.4e-07\n",
      "üìà Step 11325 | Loss: 4.1023 | Avg: 3.7897 | LR: 7.5e-07\n",
      "üìà Step 11350 | Loss: 3.8592 | Avg: 3.9086 | LR: 7.6e-07\n",
      "üìà Step 11375 | Loss: 3.5942 | Avg: 3.8159 | LR: 7.7e-07\n",
      "üìà Step 11400 | Loss: 3.3944 | Avg: 3.9321 | LR: 7.8e-07\n",
      "üìà Step 11425 | Loss: 3.6331 | Avg: 4.0967 | LR: 7.9e-07\n",
      "üìà Step 11450 | Loss: 3.0194 | Avg: 3.8089 | LR: 8.0e-07\n",
      "üìà Step 11475 | Loss: 4.2038 | Avg: 3.8428 | LR: 8.2e-07\n",
      "üìà Step 11500 | Loss: 3.4186 | Avg: 3.8529 | LR: 8.3e-07\n",
      "üìà Step 11525 | Loss: 3.9309 | Avg: 3.7971 | LR: 8.4e-07\n",
      "üìà Step 11550 | Loss: 3.8620 | Avg: 4.0244 | LR: 8.5e-07\n",
      "üìà Step 11575 | Loss: 3.6531 | Avg: 3.7909 | LR: 8.6e-07\n",
      "üìà Step 11600 | Loss: 4.0625 | Avg: 3.7644 | LR: 8.8e-07\n",
      "üìà Step 11625 | Loss: 3.3479 | Avg: 3.8819 | LR: 8.9e-07\n",
      "üìà Step 11650 | Loss: 4.0069 | Avg: 3.9188 | LR: 9.0e-07\n",
      "üìà Step 11675 | Loss: 4.3166 | Avg: 3.8433 | LR: 9.1e-07\n",
      "üìà Step 11700 | Loss: 2.9985 | Avg: 3.9383 | LR: 9.2e-07\n",
      "üìà Step 11725 | Loss: 2.8342 | Avg: 3.6496 | LR: 9.4e-07\n",
      "üìà Step 11750 | Loss: 2.8560 | Avg: 3.8986 | LR: 9.5e-07\n",
      "üìà Step 11775 | Loss: 3.0411 | Avg: 3.6743 | LR: 9.6e-07\n",
      "üìà Step 11800 | Loss: 3.5171 | Avg: 3.8801 | LR: 9.7e-07\n",
      "üìà Step 11825 | Loss: 4.8030 | Avg: 3.7707 | LR: 9.8e-07\n",
      "üìà Step 11850 | Loss: 3.8053 | Avg: 3.9022 | LR: 9.9e-07\n",
      "üìà Step 11875 | Loss: 4.3214 | Avg: 3.7635 | LR: 1.0e-06\n",
      "üìà Step 11900 | Loss: 4.5661 | Avg: 3.8094 | LR: 1.0e-06\n",
      "üìà Step 11925 | Loss: 3.6253 | Avg: 3.9625 | LR: 1.0e-06\n",
      "üìà Step 11950 | Loss: 3.6093 | Avg: 3.8298 | LR: 1.0e-06\n",
      "üìà Step 11975 | Loss: 3.0608 | Avg: 3.6263 | LR: 1.1e-06\n",
      "üìà Step 12000 | Loss: 2.9623 | Avg: 3.6436 | LR: 1.1e-06\n",
      "üìà Step 12025 | Loss: 4.1441 | Avg: 3.6421 | LR: 1.1e-06\n",
      "üìà Step 12050 | Loss: 4.6012 | Avg: 3.5762 | LR: 1.1e-06\n",
      "üìà Step 12075 | Loss: 2.7742 | Avg: 3.9540 | LR: 1.1e-06\n",
      "üìà Step 12100 | Loss: 3.5977 | Avg: 3.7172 | LR: 1.1e-06\n",
      "üìà Step 12125 | Loss: 3.4488 | Avg: 3.8321 | LR: 1.1e-06\n",
      "üìà Step 12150 | Loss: 3.5002 | Avg: 3.5199 | LR: 1.1e-06\n",
      "üìà Step 12175 | Loss: 4.3832 | Avg: 3.7358 | LR: 1.1e-06\n",
      "üìà Step 12200 | Loss: 3.4769 | Avg: 3.7096 | LR: 1.2e-06\n",
      "üìà Step 12225 | Loss: 4.1584 | Avg: 3.8191 | LR: 1.2e-06\n",
      "üìà Step 12250 | Loss: 2.9106 | Avg: 3.6630 | LR: 1.2e-06\n",
      "üìà Step 12275 | Loss: 3.3496 | Avg: 3.6731 | LR: 1.2e-06\n",
      "üìà Step 12300 | Loss: 3.8239 | Avg: 3.8051 | LR: 1.2e-06\n",
      "üìà Step 12325 | Loss: 4.1530 | Avg: 3.7129 | LR: 1.2e-06\n",
      "üìà Step 12350 | Loss: 5.0194 | Avg: 3.5372 | LR: 1.2e-06\n",
      "üìà Step 12375 | Loss: 4.2750 | Avg: 3.7564 | LR: 1.2e-06\n",
      "üìà Step 12400 | Loss: 4.1955 | Avg: 3.7409 | LR: 1.3e-06\n",
      "üìà Step 12425 | Loss: 3.6536 | Avg: 3.6046 | LR: 1.3e-06\n",
      "üìà Step 12450 | Loss: 3.1968 | Avg: 3.4697 | LR: 1.3e-06\n",
      "üìà Step 12475 | Loss: 3.4771 | Avg: 3.5282 | LR: 1.3e-06\n",
      "üìà Step 12500 | Loss: 4.9765 | Avg: 3.7262 | LR: 1.3e-06\n",
      "üìà Step 12525 | Loss: 3.9485 | Avg: 3.5695 | LR: 1.3e-06\n",
      "üìà Step 12550 | Loss: 4.5395 | Avg: 3.7382 | LR: 1.3e-06\n",
      "üìà Step 12575 | Loss: 3.1836 | Avg: 3.5586 | LR: 1.3e-06\n",
      "üìà Step 12600 | Loss: 4.1951 | Avg: 3.5055 | LR: 1.3e-06\n",
      "üìà Step 12625 | Loss: 4.0204 | Avg: 3.5747 | LR: 1.4e-06\n",
      "üìà Step 12650 | Loss: 2.7292 | Avg: 3.6248 | LR: 1.4e-06\n",
      "üìà Step 12675 | Loss: 2.4613 | Avg: 3.5413 | LR: 1.4e-06\n",
      "üìà Step 12700 | Loss: 3.3370 | Avg: 3.6406 | LR: 1.4e-06\n",
      "üìà Step 12725 | Loss: 4.3809 | Avg: 3.6851 | LR: 1.4e-06\n",
      "üìà Step 12750 | Loss: 3.9296 | Avg: 3.6203 | LR: 1.4e-06\n",
      "üìà Step 12775 | Loss: 3.6349 | Avg: 3.7028 | LR: 1.4e-06\n",
      "üìà Step 12800 | Loss: 3.7527 | Avg: 3.6102 | LR: 1.4e-06\n",
      "üìà Step 12825 | Loss: 4.4696 | Avg: 3.7128 | LR: 1.4e-06\n",
      "üìà Step 12850 | Loss: 3.3123 | Avg: 3.7166 | LR: 1.5e-06\n",
      "üìà Step 12875 | Loss: 3.4289 | Avg: 3.5444 | LR: 1.5e-06\n",
      "üìà Step 12900 | Loss: 4.1833 | Avg: 3.5632 | LR: 1.5e-06\n",
      "üìà Step 12925 | Loss: 2.8434 | Avg: 3.4727 | LR: 1.5e-06\n",
      "üìà Step 12950 | Loss: 2.8458 | Avg: 3.4862 | LR: 1.5e-06\n",
      "üìà Step 12975 | Loss: 3.0402 | Avg: 3.5006 | LR: 1.5e-06\n",
      "üìà Step 13000 | Loss: 3.2857 | Avg: 3.6076 | LR: 1.5e-06\n",
      "üìà Step 13025 | Loss: 4.0712 | Avg: 3.4912 | LR: 1.5e-06\n",
      "üìà Step 13050 | Loss: 4.0392 | Avg: 3.6244 | LR: 1.5e-06\n",
      "üìà Step 13075 | Loss: 3.7408 | Avg: 3.7133 | LR: 1.5e-06\n",
      "üìà Step 13100 | Loss: 3.0379 | Avg: 3.3701 | LR: 1.6e-06\n",
      "üìà Step 13125 | Loss: 4.0890 | Avg: 3.5549 | LR: 1.6e-06\n",
      "üìà Step 13150 | Loss: 3.5043 | Avg: 3.7435 | LR: 1.6e-06\n",
      "üìà Step 13175 | Loss: 2.7364 | Avg: 3.3469 | LR: 1.6e-06\n",
      "üìà Step 13200 | Loss: 3.2693 | Avg: 3.3774 | LR: 1.6e-06\n",
      "üìà Step 13225 | Loss: 3.7097 | Avg: 3.3088 | LR: 1.6e-06\n",
      "üìà Step 13250 | Loss: 4.9018 | Avg: 3.7333 | LR: 1.6e-06\n",
      "üìà Step 13275 | Loss: 3.4286 | Avg: 3.4600 | LR: 1.6e-06\n",
      "üìà Step 13300 | Loss: 3.4180 | Avg: 3.5188 | LR: 1.6e-06\n",
      "üìà Step 13325 | Loss: 2.2483 | Avg: 3.5076 | LR: 1.6e-06\n",
      "üìà Step 13350 | Loss: 3.1892 | Avg: 3.4129 | LR: 1.7e-06\n",
      "üìà Step 13375 | Loss: 3.4222 | Avg: 3.4900 | LR: 1.7e-06\n",
      "üìà Step 13400 | Loss: 3.9831 | Avg: 3.6497 | LR: 1.7e-06\n",
      "üìà Step 13425 | Loss: 3.5956 | Avg: 3.4981 | LR: 1.7e-06\n",
      "üìà Step 13450 | Loss: 3.5500 | Avg: 3.3413 | LR: 1.7e-06\n",
      "üìà Step 13475 | Loss: 3.4363 | Avg: 3.3431 | LR: 1.7e-06\n",
      "üìà Step 13500 | Loss: 3.2309 | Avg: 3.6825 | LR: 1.7e-06\n",
      "üìà Step 13525 | Loss: 2.7028 | Avg: 3.3466 | LR: 1.7e-06\n",
      "üìà Step 13550 | Loss: 2.4859 | Avg: 3.3268 | LR: 1.7e-06\n",
      "üìà Step 13575 | Loss: 3.3525 | Avg: 3.7000 | LR: 1.7e-06\n",
      "üìà Step 13600 | Loss: 2.9359 | Avg: 3.4120 | LR: 1.7e-06\n",
      "üìà Step 13625 | Loss: 3.2748 | Avg: 3.4757 | LR: 1.7e-06\n",
      "üìà Step 13650 | Loss: 3.2095 | Avg: 3.6951 | LR: 1.8e-06\n",
      "üìà Step 13675 | Loss: 4.0068 | Avg: 3.4294 | LR: 1.8e-06\n",
      "üìà Step 13700 | Loss: 4.0748 | Avg: 3.4357 | LR: 1.8e-06\n",
      "üíæ Stable checkpoint saved at 60% of epoch 1\n",
      "üìà Step 13725 | Loss: 4.1161 | Avg: 3.3942 | LR: 1.8e-06\n",
      "üìà Step 13750 | Loss: 3.0128 | Avg: 3.3566 | LR: 1.8e-06\n",
      "üìà Step 13775 | Loss: 3.0687 | Avg: 3.2742 | LR: 1.8e-06\n",
      "üìà Step 13800 | Loss: 3.5216 | Avg: 3.1480 | LR: 1.8e-06\n",
      "üìà Step 13825 | Loss: 2.8403 | Avg: 3.4099 | LR: 1.8e-06\n",
      "üìà Step 13850 | Loss: 3.4923 | Avg: 3.4567 | LR: 1.8e-06\n",
      "üìà Step 13875 | Loss: 2.7210 | Avg: 3.1210 | LR: 1.8e-06\n",
      "üìà Step 13900 | Loss: 3.6291 | Avg: 3.4135 | LR: 1.8e-06\n",
      "üìà Step 13925 | Loss: 3.2715 | Avg: 3.2265 | LR: 1.8e-06\n",
      "üìà Step 13950 | Loss: 2.8765 | Avg: 3.2559 | LR: 1.8e-06\n",
      "üìà Step 13975 | Loss: 2.6621 | Avg: 3.0410 | LR: 1.8e-06\n",
      "üìà Step 14000 | Loss: 2.3789 | Avg: 3.2607 | LR: 1.9e-06\n",
      "üìà Step 14025 | Loss: 3.5264 | Avg: 3.3527 | LR: 1.9e-06\n",
      "üìà Step 14050 | Loss: 3.4084 | Avg: 3.1569 | LR: 1.9e-06\n",
      "üìà Step 14075 | Loss: 3.0050 | Avg: 3.4626 | LR: 1.9e-06\n",
      "üìà Step 14100 | Loss: 4.1031 | Avg: 3.3720 | LR: 1.9e-06\n",
      "üìà Step 14125 | Loss: 4.3173 | Avg: 3.4033 | LR: 1.9e-06\n",
      "üìà Step 14150 | Loss: 3.5037 | Avg: 3.3263 | LR: 1.9e-06\n",
      "üìà Step 14175 | Loss: 2.6949 | Avg: 3.1645 | LR: 1.9e-06\n",
      "üìà Step 14200 | Loss: 3.6945 | Avg: 3.4548 | LR: 1.9e-06\n",
      "üìà Step 14225 | Loss: 2.6517 | Avg: 3.0868 | LR: 1.9e-06\n",
      "üìà Step 14250 | Loss: 2.8821 | Avg: 3.3388 | LR: 1.9e-06\n",
      "üìà Step 14275 | Loss: 3.7088 | Avg: 3.1544 | LR: 1.9e-06\n",
      "üìà Step 14300 | Loss: 2.2902 | Avg: 3.1230 | LR: 1.9e-06\n",
      "üìà Step 14325 | Loss: 3.6161 | Avg: 3.1904 | LR: 1.9e-06\n",
      "üìà Step 14350 | Loss: 3.3390 | Avg: 3.1882 | LR: 1.9e-06\n",
      "üìà Step 14375 | Loss: 4.1546 | Avg: 3.2926 | LR: 1.9e-06\n",
      "üìà Step 14400 | Loss: 3.8174 | Avg: 3.4178 | LR: 1.9e-06\n",
      "üìà Step 14425 | Loss: 3.1800 | Avg: 3.1305 | LR: 1.9e-06\n",
      "üìà Step 14450 | Loss: 3.7528 | Avg: 3.2526 | LR: 1.9e-06\n",
      "üìà Step 14475 | Loss: 3.1651 | Avg: 3.2539 | LR: 1.9e-06\n",
      "üìà Step 14500 | Loss: 3.0458 | Avg: 3.2621 | LR: 2.0e-06\n",
      "üìà Step 14525 | Loss: 3.2731 | Avg: 3.0451 | LR: 2.0e-06\n",
      "üìà Step 14550 | Loss: 3.2367 | Avg: 3.1952 | LR: 2.0e-06\n",
      "üìà Step 14575 | Loss: 3.7703 | Avg: 3.0293 | LR: 2.0e-06\n",
      "üìà Step 14600 | Loss: 2.4638 | Avg: 3.1544 | LR: 2.0e-06\n",
      "üìà Step 14625 | Loss: 2.7990 | Avg: 2.9816 | LR: 2.0e-06\n",
      "üìà Step 14650 | Loss: 3.4936 | Avg: 3.0334 | LR: 2.0e-06\n",
      "üìà Step 14675 | Loss: 3.5326 | Avg: 3.3675 | LR: 2.0e-06\n",
      "üìà Step 14700 | Loss: 2.7957 | Avg: 3.3567 | LR: 2.0e-06\n",
      "üìà Step 14725 | Loss: 3.5934 | Avg: 2.9907 | LR: 2.0e-06\n",
      "üìà Step 14750 | Loss: 2.6780 | Avg: 3.2005 | LR: 2.0e-06\n",
      "üìà Step 14775 | Loss: 3.8526 | Avg: 3.0287 | LR: 2.0e-06\n",
      "üìà Step 14800 | Loss: 3.4996 | Avg: 3.2023 | LR: 2.0e-06\n",
      "üìà Step 14825 | Loss: 2.5523 | Avg: 3.0448 | LR: 2.0e-06\n",
      "üìà Step 14850 | Loss: 2.5025 | Avg: 3.0879 | LR: 2.0e-06\n",
      "üìà Step 14875 | Loss: 2.8784 | Avg: 3.2420 | LR: 2.0e-06\n",
      "üìà Step 14900 | Loss: 2.9500 | Avg: 3.0674 | LR: 2.0e-06\n",
      "üìà Step 14925 | Loss: 3.1356 | Avg: 3.1173 | LR: 2.0e-06\n",
      "üìà Step 14950 | Loss: 3.9771 | Avg: 3.0104 | LR: 2.0e-06\n",
      "üìà Step 14975 | Loss: 3.0230 | Avg: 3.0628 | LR: 2.0e-06\n",
      "üìà Step 15000 | Loss: 3.2076 | Avg: 3.1199 | LR: 2.0e-06\n",
      "üìà Step 15025 | Loss: 2.9833 | Avg: 3.0997 | LR: 2.0e-06\n",
      "üìà Step 15050 | Loss: 3.1566 | Avg: 3.2162 | LR: 2.0e-06\n",
      "üìà Step 15075 | Loss: 2.3731 | Avg: 3.0746 | LR: 2.0e-06\n",
      "üìà Step 15100 | Loss: 3.0667 | Avg: 3.0575 | LR: 2.0e-06\n",
      "üìà Step 15125 | Loss: 3.2472 | Avg: 2.9906 | LR: 2.0e-06\n",
      "üìà Step 15150 | Loss: 2.7722 | Avg: 2.9291 | LR: 2.0e-06\n",
      "üìà Step 15175 | Loss: 2.6163 | Avg: 2.8061 | LR: 2.0e-06\n",
      "üìà Step 15200 | Loss: 2.3325 | Avg: 3.0427 | LR: 2.0e-06\n",
      "üìà Step 15225 | Loss: 2.9705 | Avg: 3.1883 | LR: 2.0e-06\n",
      "üìà Step 15250 | Loss: 3.0513 | Avg: 3.0564 | LR: 2.0e-06\n",
      "üìà Step 15275 | Loss: 3.8256 | Avg: 3.0844 | LR: 2.0e-06\n",
      "üìà Step 15300 | Loss: 2.8527 | Avg: 3.2935 | LR: 2.0e-06\n",
      "üìà Step 15325 | Loss: 3.3948 | Avg: 3.0918 | LR: 2.0e-06\n",
      "üìà Step 15350 | Loss: 2.4718 | Avg: 3.0934 | LR: 2.0e-06\n",
      "üìà Step 15375 | Loss: 3.0773 | Avg: 2.9447 | LR: 2.0e-06\n",
      "üìà Step 15400 | Loss: 2.9952 | Avg: 2.8552 | LR: 2.0e-06\n",
      "üìà Step 15425 | Loss: 2.0365 | Avg: 2.9276 | LR: 2.0e-06\n",
      "üìà Step 15450 | Loss: 2.4959 | Avg: 2.7913 | LR: 2.0e-06\n",
      "üìà Step 15475 | Loss: 2.5905 | Avg: 2.9931 | LR: 2.0e-06\n",
      "üìà Step 15500 | Loss: 2.8058 | Avg: 2.7247 | LR: 2.0e-06\n",
      "üìà Step 15525 | Loss: 2.3678 | Avg: 2.9454 | LR: 2.0e-06\n",
      "üìà Step 15550 | Loss: 3.4739 | Avg: 2.8572 | LR: 2.0e-06\n",
      "üìà Step 15575 | Loss: 3.6181 | Avg: 2.6761 | LR: 2.0e-06\n",
      "üìà Step 15600 | Loss: 3.0381 | Avg: 2.9550 | LR: 2.0e-06\n",
      "üìà Step 15625 | Loss: 2.4089 | Avg: 3.0147 | LR: 2.0e-06\n",
      "üìà Step 15650 | Loss: 3.8661 | Avg: 2.7955 | LR: 2.0e-06\n",
      "üìà Step 15675 | Loss: 3.5334 | Avg: 2.8104 | LR: 2.0e-06\n",
      "üìà Step 15700 | Loss: 2.5896 | Avg: 2.8599 | LR: 2.0e-06\n",
      "üìà Step 15725 | Loss: 4.0450 | Avg: 2.9843 | LR: 2.0e-06\n",
      "üìà Step 15750 | Loss: 2.4193 | Avg: 2.9404 | LR: 2.0e-06\n",
      "üìà Step 15775 | Loss: 3.5264 | Avg: 2.9477 | LR: 2.0e-06\n",
      "üìà Step 15800 | Loss: 2.7909 | Avg: 2.9104 | LR: 2.0e-06\n",
      "üìà Step 15825 | Loss: 2.5882 | Avg: 2.6334 | LR: 1.9e-06\n",
      "üìà Step 15850 | Loss: 2.9693 | Avg: 2.8492 | LR: 1.9e-06\n",
      "üìà Step 15875 | Loss: 3.1553 | Avg: 2.8399 | LR: 1.9e-06\n",
      "üìà Step 15900 | Loss: 3.0301 | Avg: 2.7304 | LR: 1.9e-06\n",
      "üìà Step 15925 | Loss: 3.3789 | Avg: 2.8125 | LR: 1.9e-06\n",
      "üìà Step 15950 | Loss: 3.3783 | Avg: 2.8847 | LR: 1.9e-06\n",
      "üìà Step 15975 | Loss: 3.2650 | Avg: 2.8433 | LR: 1.9e-06\n",
      "üìà Step 16000 | Loss: 2.9891 | Avg: 2.7917 | LR: 1.9e-06\n",
      "üìà Step 16025 | Loss: 2.4717 | Avg: 2.8684 | LR: 1.9e-06\n",
      "üìà Step 16050 | Loss: 1.9511 | Avg: 2.6656 | LR: 1.9e-06\n",
      "üìà Step 16075 | Loss: 2.5949 | Avg: 2.8113 | LR: 1.9e-06\n",
      "üìà Step 16100 | Loss: 2.8675 | Avg: 2.8150 | LR: 1.9e-06\n",
      "üìà Step 16125 | Loss: 2.3588 | Avg: 2.7896 | LR: 1.9e-06\n",
      "üìà Step 16150 | Loss: 2.4002 | Avg: 2.6278 | LR: 1.9e-06\n",
      "üìà Step 16175 | Loss: 2.7595 | Avg: 2.8509 | LR: 1.9e-06\n",
      "üìà Step 16200 | Loss: 2.3638 | Avg: 2.7565 | LR: 1.9e-06\n",
      "üìà Step 16225 | Loss: 2.8831 | Avg: 2.8486 | LR: 1.9e-06\n",
      "üìà Step 16250 | Loss: 3.2908 | Avg: 2.8019 | LR: 1.9e-06\n",
      "üìà Step 16275 | Loss: 3.2338 | Avg: 2.8013 | LR: 1.9e-06\n",
      "üìà Step 16300 | Loss: 2.6902 | Avg: 2.8093 | LR: 1.9e-06\n",
      "üìà Step 16325 | Loss: 1.4818 | Avg: 2.7093 | LR: 1.8e-06\n",
      "üìà Step 16350 | Loss: 2.4163 | Avg: 2.7038 | LR: 1.8e-06\n",
      "üìà Step 16375 | Loss: 1.5556 | Avg: 2.8973 | LR: 1.8e-06\n",
      "üìà Step 16400 | Loss: 4.4205 | Avg: 3.0578 | LR: 1.8e-06\n",
      "üìà Step 16425 | Loss: 3.0639 | Avg: 2.8237 | LR: 1.8e-06\n",
      "üìà Step 16450 | Loss: 1.9750 | Avg: 2.6151 | LR: 1.8e-06\n",
      "üìà Step 16475 | Loss: 2.1943 | Avg: 2.7411 | LR: 1.8e-06\n",
      "üìà Step 16500 | Loss: 3.3962 | Avg: 2.7012 | LR: 1.8e-06\n",
      "üìà Step 16525 | Loss: 2.7435 | Avg: 2.7203 | LR: 1.8e-06\n",
      "üìà Step 16550 | Loss: 2.8582 | Avg: 2.8555 | LR: 1.8e-06\n",
      "üìà Step 16575 | Loss: 2.6647 | Avg: 2.6147 | LR: 1.8e-06\n",
      "üìà Step 16600 | Loss: 3.0954 | Avg: 2.7557 | LR: 1.8e-06\n",
      "üìà Step 16625 | Loss: 2.3325 | Avg: 2.7312 | LR: 1.8e-06\n",
      "üìà Step 16650 | Loss: 2.2542 | Avg: 2.4950 | LR: 1.8e-06\n",
      "üìà Step 16675 | Loss: 2.7972 | Avg: 2.7113 | LR: 1.7e-06\n",
      "üìà Step 16700 | Loss: 2.4735 | Avg: 2.5457 | LR: 1.7e-06\n",
      "üìà Step 16725 | Loss: 2.1682 | Avg: 2.6734 | LR: 1.7e-06\n",
      "üìà Step 16750 | Loss: 2.5212 | Avg: 2.6923 | LR: 1.7e-06\n",
      "üìà Step 16775 | Loss: 2.0615 | Avg: 2.5357 | LR: 1.7e-06\n",
      "üìà Step 16800 | Loss: 2.8101 | Avg: 2.7510 | LR: 1.7e-06\n",
      "üìà Step 16825 | Loss: 2.7573 | Avg: 2.5916 | LR: 1.7e-06\n",
      "üìà Step 16850 | Loss: 2.3347 | Avg: 2.8029 | LR: 1.7e-06\n",
      "üìà Step 16875 | Loss: 3.7422 | Avg: 2.6302 | LR: 1.7e-06\n",
      "üìà Step 16900 | Loss: 3.3992 | Avg: 2.7145 | LR: 1.7e-06\n",
      "üìà Step 16925 | Loss: 2.4102 | Avg: 2.6225 | LR: 1.7e-06\n",
      "üìà Step 16950 | Loss: 2.2697 | Avg: 2.7679 | LR: 1.7e-06\n",
      "üìà Step 16975 | Loss: 1.9582 | Avg: 2.6754 | LR: 1.6e-06\n",
      "üìà Step 17000 | Loss: 2.8451 | Avg: 2.7705 | LR: 1.6e-06\n",
      "üìà Step 17025 | Loss: 1.9295 | Avg: 2.6999 | LR: 1.6e-06\n",
      "üìà Step 17050 | Loss: 3.0501 | Avg: 2.6716 | LR: 1.6e-06\n",
      "üìà Step 17075 | Loss: 1.8675 | Avg: 2.5387 | LR: 1.6e-06\n",
      "üìà Step 17100 | Loss: 3.6038 | Avg: 2.7016 | LR: 1.6e-06\n",
      "üìà Step 17125 | Loss: 2.4024 | Avg: 2.6505 | LR: 1.6e-06\n",
      "üíæ Stable checkpoint saved at 75% of epoch 1\n",
      "üìà Step 17150 | Loss: 2.9649 | Avg: 2.5972 | LR: 1.6e-06\n",
      "üìà Step 17175 | Loss: 2.1712 | Avg: 2.4380 | LR: 1.6e-06\n",
      "üìà Step 17200 | Loss: 2.3862 | Avg: 2.8404 | LR: 1.6e-06\n",
      "üìà Step 17225 | Loss: 2.7581 | Avg: 2.8344 | LR: 1.5e-06\n",
      "üìà Step 17250 | Loss: 2.7800 | Avg: 2.6624 | LR: 1.5e-06\n",
      "üìà Step 17275 | Loss: 2.0856 | Avg: 2.8054 | LR: 1.5e-06\n",
      "üìà Step 17300 | Loss: 2.2652 | Avg: 2.6850 | LR: 1.5e-06\n",
      "üìà Step 17325 | Loss: 2.2696 | Avg: 2.6762 | LR: 1.5e-06\n",
      "üìà Step 17350 | Loss: 2.7501 | Avg: 2.6336 | LR: 1.5e-06\n",
      "üìà Step 17375 | Loss: 2.6609 | Avg: 2.5366 | LR: 1.5e-06\n",
      "üìà Step 17400 | Loss: 2.2427 | Avg: 2.7792 | LR: 1.5e-06\n",
      "üìà Step 17425 | Loss: 2.6996 | Avg: 2.6171 | LR: 1.5e-06\n",
      "üìà Step 17450 | Loss: 2.7078 | Avg: 2.7003 | LR: 1.5e-06\n",
      "üìà Step 17475 | Loss: 2.2106 | Avg: 2.7028 | LR: 1.4e-06\n",
      "üìà Step 17500 | Loss: 3.5740 | Avg: 2.6173 | LR: 1.4e-06\n",
      "üìà Step 17525 | Loss: 1.8774 | Avg: 2.5271 | LR: 1.4e-06\n",
      "üìà Step 17550 | Loss: 2.3579 | Avg: 2.6013 | LR: 1.4e-06\n",
      "üìà Step 17575 | Loss: 2.3359 | Avg: 2.7701 | LR: 1.4e-06\n",
      "üìà Step 17600 | Loss: 2.4910 | Avg: 2.4465 | LR: 1.4e-06\n",
      "üìà Step 17625 | Loss: 3.3912 | Avg: 2.5653 | LR: 1.4e-06\n",
      "üìà Step 17650 | Loss: 3.0243 | Avg: 2.6860 | LR: 1.4e-06\n",
      "üìà Step 17675 | Loss: 2.4280 | Avg: 2.6773 | LR: 1.4e-06\n",
      "üìà Step 17700 | Loss: 2.5620 | Avg: 2.6969 | LR: 1.3e-06\n",
      "üìà Step 17725 | Loss: 2.5182 | Avg: 3.0135 | LR: 1.3e-06\n",
      "üìà Step 17750 | Loss: 2.0701 | Avg: 2.5479 | LR: 1.3e-06\n",
      "üìà Step 17775 | Loss: 3.8078 | Avg: 2.9133 | LR: 1.3e-06\n",
      "üìà Step 17800 | Loss: 1.6789 | Avg: 2.5587 | LR: 1.3e-06\n",
      "üìà Step 17825 | Loss: 2.6990 | Avg: 2.8490 | LR: 1.3e-06\n",
      "üìà Step 17850 | Loss: 2.5492 | Avg: 2.8502 | LR: 1.3e-06\n",
      "üìà Step 17875 | Loss: 2.0976 | Avg: 2.5204 | LR: 1.3e-06\n",
      "üìà Step 17900 | Loss: 2.9290 | Avg: 2.7259 | LR: 1.3e-06\n",
      "üìà Step 17925 | Loss: 2.6748 | Avg: 2.6272 | LR: 1.2e-06\n",
      "üìà Step 17950 | Loss: 2.7975 | Avg: 2.5578 | LR: 1.2e-06\n",
      "üìà Step 17975 | Loss: 2.5718 | Avg: 2.6387 | LR: 1.2e-06\n",
      "üìà Step 18000 | Loss: 2.1843 | Avg: 2.5132 | LR: 1.2e-06\n",
      "üìà Step 18025 | Loss: 4.1004 | Avg: 2.7022 | LR: 1.2e-06\n",
      "üìà Step 18050 | Loss: 2.1388 | Avg: 2.6063 | LR: 1.2e-06\n",
      "üìà Step 18075 | Loss: 2.1697 | Avg: 2.4558 | LR: 1.2e-06\n",
      "üìà Step 18100 | Loss: 2.3475 | Avg: 2.5574 | LR: 1.2e-06\n",
      "üìà Step 18125 | Loss: 2.2671 | Avg: 2.4651 | LR: 1.1e-06\n",
      "üìà Step 18150 | Loss: 2.2001 | Avg: 2.5098 | LR: 1.1e-06\n",
      "üìà Step 18175 | Loss: 2.3284 | Avg: 2.7303 | LR: 1.1e-06\n",
      "üìà Step 18200 | Loss: 2.1891 | Avg: 2.7038 | LR: 1.1e-06\n",
      "üìà Step 18225 | Loss: 3.2600 | Avg: 2.5308 | LR: 1.1e-06\n",
      "üìà Step 18250 | Loss: 2.3049 | Avg: 2.6027 | LR: 1.1e-06\n",
      "üìà Step 18275 | Loss: 2.4460 | Avg: 2.4568 | LR: 1.1e-06\n",
      "üìà Step 18300 | Loss: 2.1432 | Avg: 2.5837 | LR: 1.1e-06\n",
      "üìà Step 18325 | Loss: 3.2349 | Avg: 2.6832 | LR: 1.1e-06\n",
      "üìà Step 18350 | Loss: 3.4287 | Avg: 2.6371 | LR: 1.0e-06\n",
      "üìà Step 18375 | Loss: 2.9762 | Avg: 2.5362 | LR: 1.0e-06\n",
      "üìà Step 18400 | Loss: 1.7756 | Avg: 2.4189 | LR: 1.0e-06\n",
      "üìà Step 18425 | Loss: 3.0163 | Avg: 2.6319 | LR: 1.0e-06\n",
      "üìà Step 18450 | Loss: 2.6007 | Avg: 2.3969 | LR: 9.9e-07\n",
      "üìà Step 18475 | Loss: 1.8918 | Avg: 2.6513 | LR: 9.8e-07\n",
      "üìà Step 18500 | Loss: 2.7241 | Avg: 2.5667 | LR: 9.7e-07\n",
      "üìà Step 18525 | Loss: 2.5524 | Avg: 2.6958 | LR: 9.6e-07\n",
      "üìà Step 18550 | Loss: 2.0756 | Avg: 2.4485 | LR: 9.5e-07\n",
      "üìà Step 18575 | Loss: 1.3731 | Avg: 2.6146 | LR: 9.3e-07\n",
      "üìà Step 18600 | Loss: 2.5194 | Avg: 2.6693 | LR: 9.2e-07\n",
      "üìà Step 18625 | Loss: 2.0292 | Avg: 2.6426 | LR: 9.1e-07\n",
      "üìà Step 18650 | Loss: 2.1692 | Avg: 2.6318 | LR: 9.0e-07\n",
      "üìà Step 18675 | Loss: 3.2462 | Avg: 2.6500 | LR: 8.9e-07\n",
      "üìà Step 18700 | Loss: 3.2249 | Avg: 2.7019 | LR: 8.7e-07\n",
      "üìà Step 18725 | Loss: 2.6695 | Avg: 2.7529 | LR: 8.6e-07\n",
      "üìà Step 18750 | Loss: 2.7088 | Avg: 2.7039 | LR: 8.5e-07\n",
      "üìà Step 18775 | Loss: 1.9393 | Avg: 2.6850 | LR: 8.4e-07\n",
      "üìà Step 18800 | Loss: 2.4987 | Avg: 2.7292 | LR: 8.3e-07\n",
      "üìà Step 18825 | Loss: 1.8600 | Avg: 2.4916 | LR: 8.1e-07\n",
      "üìà Step 18850 | Loss: 3.9231 | Avg: 2.6192 | LR: 8.0e-07\n",
      "üìà Step 18875 | Loss: 3.2784 | Avg: 2.4361 | LR: 7.9e-07\n",
      "üìà Step 18900 | Loss: 2.6548 | Avg: 2.4873 | LR: 7.8e-07\n",
      "üìà Step 18925 | Loss: 2.6438 | Avg: 2.5034 | LR: 7.7e-07\n",
      "üìà Step 18950 | Loss: 3.4459 | Avg: 2.8233 | LR: 7.6e-07\n",
      "üìà Step 18975 | Loss: 2.4441 | Avg: 2.5694 | LR: 7.4e-07\n",
      "üìà Step 19000 | Loss: 3.8724 | Avg: 2.5841 | LR: 7.3e-07\n",
      "üìà Step 19025 | Loss: 1.2750 | Avg: 2.4579 | LR: 7.2e-07\n",
      "üìà Step 19050 | Loss: 2.7883 | Avg: 2.7250 | LR: 7.1e-07\n",
      "üìà Step 19075 | Loss: 2.9050 | Avg: 2.6627 | LR: 7.0e-07\n",
      "üìà Step 19100 | Loss: 2.9743 | Avg: 2.6920 | LR: 6.9e-07\n",
      "üìà Step 19125 | Loss: 2.4547 | Avg: 2.5346 | LR: 6.8e-07\n",
      "üìà Step 19150 | Loss: 2.5618 | Avg: 2.4231 | LR: 6.6e-07\n",
      "üìà Step 19175 | Loss: 1.6704 | Avg: 2.8299 | LR: 6.5e-07\n",
      "üìà Step 19200 | Loss: 2.7117 | Avg: 2.3936 | LR: 6.4e-07\n",
      "üìà Step 19225 | Loss: 1.9899 | Avg: 2.4094 | LR: 6.3e-07\n",
      "üìà Step 19250 | Loss: 2.1453 | Avg: 2.5723 | LR: 6.2e-07\n",
      "üìà Step 19275 | Loss: 3.1802 | Avg: 2.5467 | LR: 6.1e-07\n",
      "üìà Step 19300 | Loss: 2.4678 | Avg: 2.5851 | LR: 6.0e-07\n",
      "üìà Step 19325 | Loss: 1.9129 | Avg: 2.5248 | LR: 5.9e-07\n",
      "üìà Step 19350 | Loss: 2.4366 | Avg: 2.6507 | LR: 5.8e-07\n",
      "üìà Step 19375 | Loss: 2.9560 | Avg: 2.5884 | LR: 5.7e-07\n",
      "üìà Step 19400 | Loss: 2.8030 | Avg: 2.4918 | LR: 5.6e-07\n",
      "üìà Step 19425 | Loss: 4.1095 | Avg: 2.6438 | LR: 5.4e-07\n",
      "üìà Step 19450 | Loss: 2.4622 | Avg: 2.4171 | LR: 5.3e-07\n",
      "üìà Step 19475 | Loss: 2.3843 | Avg: 2.6181 | LR: 5.2e-07\n",
      "üìà Step 19500 | Loss: 2.2496 | Avg: 2.6132 | LR: 5.1e-07\n",
      "üìà Step 19525 | Loss: 2.5823 | Avg: 2.4726 | LR: 5.0e-07\n",
      "üìà Step 19550 | Loss: 2.5324 | Avg: 2.6812 | LR: 4.9e-07\n",
      "üìà Step 19575 | Loss: 2.7110 | Avg: 2.4921 | LR: 4.8e-07\n",
      "üìà Step 19600 | Loss: 1.9818 | Avg: 2.7403 | LR: 4.7e-07\n",
      "üìà Step 19625 | Loss: 3.1316 | Avg: 2.6934 | LR: 4.6e-07\n",
      "üìà Step 19650 | Loss: 2.7947 | Avg: 2.6173 | LR: 4.5e-07\n",
      "üìà Step 19675 | Loss: 2.2049 | Avg: 2.4470 | LR: 4.4e-07\n",
      "üìà Step 19700 | Loss: 2.4217 | Avg: 2.7510 | LR: 4.3e-07\n",
      "üìà Step 19725 | Loss: 2.5024 | Avg: 2.7004 | LR: 4.2e-07\n",
      "üìà Step 19750 | Loss: 2.5058 | Avg: 2.4938 | LR: 4.1e-07\n",
      "üìà Step 19775 | Loss: 2.7187 | Avg: 2.7056 | LR: 4.0e-07\n",
      "üìà Step 19800 | Loss: 2.4377 | Avg: 2.3703 | LR: 3.9e-07\n",
      "üìà Step 19825 | Loss: 2.8329 | Avg: 2.6472 | LR: 3.8e-07\n",
      "üìà Step 19850 | Loss: 2.1202 | Avg: 2.5166 | LR: 3.7e-07\n",
      "üìà Step 19875 | Loss: 1.8701 | Avg: 2.4694 | LR: 3.6e-07\n",
      "üìà Step 19900 | Loss: 2.3602 | Avg: 2.5591 | LR: 3.6e-07\n",
      "üìà Step 19925 | Loss: 3.1128 | Avg: 2.5990 | LR: 3.5e-07\n",
      "üìà Step 19950 | Loss: 2.6566 | Avg: 2.5606 | LR: 3.4e-07\n",
      "üìà Step 19975 | Loss: 2.5603 | Avg: 2.7189 | LR: 3.3e-07\n",
      "üìà Step 20000 | Loss: 2.9656 | Avg: 2.5796 | LR: 3.2e-07\n",
      "üìà Step 20025 | Loss: 2.5674 | Avg: 2.5649 | LR: 3.1e-07\n",
      "üìà Step 20050 | Loss: 1.9781 | Avg: 2.5478 | LR: 3.0e-07\n",
      "üìà Step 20075 | Loss: 3.2361 | Avg: 2.4686 | LR: 2.9e-07\n",
      "üìà Step 20100 | Loss: 1.6597 | Avg: 2.3691 | LR: 2.9e-07\n",
      "üìà Step 20125 | Loss: 3.0203 | Avg: 2.5900 | LR: 2.8e-07\n",
      "üìà Step 20150 | Loss: 3.1219 | Avg: 2.7102 | LR: 2.7e-07\n",
      "üìà Step 20175 | Loss: 2.4061 | Avg: 2.5778 | LR: 2.6e-07\n",
      "üìà Step 20200 | Loss: 2.1366 | Avg: 2.4423 | LR: 2.5e-07\n",
      "üìà Step 20225 | Loss: 2.6515 | Avg: 2.5499 | LR: 2.5e-07\n",
      "üìà Step 20250 | Loss: 1.6907 | Avg: 2.3337 | LR: 2.4e-07\n",
      "üìà Step 20275 | Loss: 4.0815 | Avg: 2.4927 | LR: 2.3e-07\n",
      "üìà Step 20300 | Loss: 2.7104 | Avg: 2.6517 | LR: 2.2e-07\n",
      "üìà Step 20325 | Loss: 2.2545 | Avg: 2.4299 | LR: 2.1e-07\n",
      "üìà Step 20350 | Loss: 2.0850 | Avg: 2.6016 | LR: 2.1e-07\n",
      "üìà Step 20375 | Loss: 3.9311 | Avg: 2.7449 | LR: 2.0e-07\n",
      "üìà Step 20400 | Loss: 2.1188 | Avg: 2.4849 | LR: 1.9e-07\n",
      "üìà Step 20425 | Loss: 2.5579 | Avg: 2.5348 | LR: 1.9e-07\n",
      "üìà Step 20450 | Loss: 2.1612 | Avg: 2.3481 | LR: 1.8e-07\n",
      "üìà Step 20475 | Loss: 2.2451 | Avg: 2.5243 | LR: 1.7e-07\n",
      "üìà Step 20500 | Loss: 2.1848 | Avg: 2.5533 | LR: 1.7e-07\n",
      "üìà Step 20525 | Loss: 2.5306 | Avg: 2.7373 | LR: 1.6e-07\n",
      "üìà Step 20550 | Loss: 2.9621 | Avg: 2.3880 | LR: 1.5e-07\n",
      "üíæ Stable checkpoint saved at 90% of epoch 1\n",
      "üìà Step 20575 | Loss: 2.0329 | Avg: 2.5548 | LR: 1.5e-07\n",
      "üìà Step 20600 | Loss: 2.9969 | Avg: 2.5749 | LR: 1.4e-07\n",
      "üìà Step 20625 | Loss: 3.0243 | Avg: 2.4935 | LR: 1.3e-07\n",
      "üìà Step 20650 | Loss: 2.3280 | Avg: 2.7452 | LR: 1.3e-07\n",
      "üìà Step 20675 | Loss: 2.3689 | Avg: 2.5841 | LR: 1.2e-07\n",
      "üìà Step 20700 | Loss: 3.2599 | Avg: 2.6510 | LR: 1.2e-07\n",
      "üìà Step 20725 | Loss: 2.4379 | Avg: 2.6877 | LR: 1.1e-07\n",
      "üìà Step 20750 | Loss: 2.5491 | Avg: 2.8839 | LR: 1.1e-07\n",
      "üìà Step 20775 | Loss: 2.9342 | Avg: 2.4961 | LR: 1.0e-07\n",
      "üìà Step 20800 | Loss: 2.8807 | Avg: 2.4813 | LR: 9.6e-08\n",
      "üìà Step 20825 | Loss: 2.5949 | Avg: 2.5655 | LR: 9.0e-08\n",
      "üìà Step 20850 | Loss: 2.9631 | Avg: 2.4855 | LR: 8.6e-08\n",
      "üìà Step 20875 | Loss: 2.6095 | Avg: 2.4415 | LR: 8.1e-08\n",
      "üìà Step 20900 | Loss: 3.0323 | Avg: 2.5935 | LR: 7.6e-08\n",
      "üìà Step 20925 | Loss: 2.7481 | Avg: 2.3600 | LR: 7.2e-08\n",
      "üìà Step 20950 | Loss: 2.7276 | Avg: 2.4822 | LR: 6.7e-08\n",
      "üìà Step 20975 | Loss: 2.4118 | Avg: 2.7915 | LR: 6.3e-08\n",
      "üìà Step 21000 | Loss: 2.3773 | Avg: 2.5438 | LR: 5.9e-08\n",
      "üìà Step 21025 | Loss: 1.8705 | Avg: 2.3542 | LR: 5.5e-08\n",
      "üìà Step 21050 | Loss: 1.9706 | Avg: 2.3315 | LR: 5.1e-08\n",
      "üìà Step 21075 | Loss: 2.6427 | Avg: 2.5566 | LR: 4.7e-08\n",
      "üìà Step 21100 | Loss: 3.9729 | Avg: 2.5231 | LR: 4.4e-08\n",
      "üìà Step 21125 | Loss: 3.1693 | Avg: 2.5948 | LR: 4.0e-08\n",
      "üìà Step 21150 | Loss: 2.4255 | Avg: 2.5536 | LR: 3.7e-08\n",
      "üìà Step 21175 | Loss: 2.8340 | Avg: 2.4378 | LR: 3.4e-08\n",
      "üìà Step 21200 | Loss: 1.8418 | Avg: 2.5936 | LR: 3.1e-08\n",
      "üìà Step 21225 | Loss: 1.8739 | Avg: 2.7510 | LR: 2.8e-08\n",
      "üìà Step 21250 | Loss: 2.5153 | Avg: 2.6111 | LR: 2.5e-08\n",
      "üìà Step 21275 | Loss: 1.7068 | Avg: 2.5862 | LR: 2.3e-08\n",
      "üìà Step 21300 | Loss: 2.4068 | Avg: 2.7344 | LR: 2.0e-08\n",
      "üìà Step 21325 | Loss: 2.1954 | Avg: 2.5218 | LR: 1.8e-08\n",
      "üìà Step 21350 | Loss: 2.3680 | Avg: 2.4728 | LR: 1.6e-08\n",
      "üìà Step 21375 | Loss: 1.9412 | Avg: 2.5857 | LR: 1.4e-08\n",
      "üìà Step 21400 | Loss: 2.5045 | Avg: 2.5346 | LR: 1.2e-08\n",
      "üìà Step 21425 | Loss: 2.0840 | Avg: 2.5868 | LR: 1.0e-08\n",
      "üìà Step 21450 | Loss: 1.9798 | Avg: 2.6021 | LR: 8.4e-09\n",
      "üìà Step 21475 | Loss: 2.2744 | Avg: 2.2833 | LR: 7.0e-09\n",
      "üìà Step 21500 | Loss: 3.3696 | Avg: 2.4986 | LR: 5.6e-09\n",
      "üìà Step 21525 | Loss: 1.5727 | Avg: 2.5551 | LR: 4.4e-09\n",
      "üìà Step 21550 | Loss: 2.7134 | Avg: 2.6557 | LR: 3.4e-09\n",
      "üìà Step 21575 | Loss: 2.4632 | Avg: 2.3958 | LR: 2.5e-09\n",
      "üìà Step 21600 | Loss: 2.2873 | Avg: 2.4776 | LR: 1.7e-09\n",
      "üìà Step 21625 | Loss: 2.8422 | Avg: 2.5712 | LR: 1.1e-09\n",
      "üìà Step 21650 | Loss: 2.8321 | Avg: 2.6697 | LR: 5.9e-10\n",
      "üìà Step 21675 | Loss: 2.7728 | Avg: 2.6558 | LR: 2.5e-10\n",
      "üìà Step 21700 | Loss: 2.8543 | Avg: 2.4962 | LR: 5.5e-11\n",
      "üìà Step 21725 | Loss: 2.7899 | Avg: 2.5236 | LR: 1.0e-12\n",
      "üìà Step 21750 | Loss: 2.0961 | Avg: 2.7298 | LR: 9.0e-11\n",
      "üìà Step 21775 | Loss: 2.2188 | Avg: 2.5020 | LR: 3.2e-10\n",
      "üìà Step 21800 | Loss: 4.3595 | Avg: 2.5894 | LR: 6.9e-10\n",
      "üìà Step 21825 | Loss: 3.2564 | Avg: 2.4424 | LR: 1.2e-09\n",
      "üìà Step 21850 | Loss: 2.6333 | Avg: 2.4851 | LR: 1.9e-09\n",
      "üìà Step 21875 | Loss: 2.8916 | Avg: 2.6023 | LR: 2.7e-09\n",
      "üìà Step 21900 | Loss: 2.9147 | Avg: 2.4981 | LR: 3.6e-09\n",
      "üìà Step 21925 | Loss: 2.3828 | Avg: 2.6417 | LR: 4.7e-09\n",
      "üìà Step 21950 | Loss: 2.2045 | Avg: 2.6142 | LR: 5.9e-09\n",
      "üìà Step 21975 | Loss: 1.6367 | Avg: 2.4198 | LR: 7.3e-09\n",
      "üìà Step 22000 | Loss: 2.1563 | Avg: 2.5599 | LR: 8.8e-09\n",
      "üìà Step 22025 | Loss: 2.7594 | Avg: 2.4630 | LR: 1.0e-08\n",
      "üìà Step 22050 | Loss: 2.0546 | Avg: 2.6173 | LR: 1.2e-08\n",
      "üìà Step 22075 | Loss: 1.5226 | Avg: 2.7413 | LR: 1.4e-08\n",
      "üìà Step 22100 | Loss: 2.2938 | Avg: 2.5441 | LR: 1.6e-08\n",
      "üìà Step 22125 | Loss: 3.1676 | Avg: 2.7964 | LR: 1.8e-08\n",
      "üìà Step 22150 | Loss: 2.6458 | Avg: 2.4364 | LR: 2.1e-08\n",
      "üìà Step 22175 | Loss: 2.7586 | Avg: 2.5480 | LR: 2.3e-08\n",
      "üìà Step 22200 | Loss: 3.4283 | Avg: 2.7527 | LR: 2.6e-08\n",
      "üìà Step 22225 | Loss: 2.1775 | Avg: 2.6274 | LR: 2.9e-08\n",
      "üìà Step 22250 | Loss: 2.1517 | Avg: 2.5596 | LR: 3.2e-08\n",
      "üìà Step 22275 | Loss: 3.0714 | Avg: 2.7440 | LR: 3.5e-08\n",
      "üìà Step 22300 | Loss: 2.9980 | Avg: 2.3527 | LR: 3.8e-08\n",
      "üìà Step 22325 | Loss: 4.1524 | Avg: 2.4385 | LR: 4.1e-08\n",
      "üìà Step 22350 | Loss: 2.9984 | Avg: 2.5478 | LR: 4.5e-08\n",
      "üìà Step 22375 | Loss: 1.9174 | Avg: 2.5083 | LR: 4.8e-08\n",
      "üìà Step 22400 | Loss: 2.5263 | Avg: 2.2902 | LR: 5.2e-08\n",
      "üìà Step 22425 | Loss: 2.1351 | Avg: 2.6436 | LR: 5.6e-08\n",
      "üìà Step 22450 | Loss: 3.0676 | Avg: 2.6253 | LR: 6.0e-08\n",
      "üìà Step 22475 | Loss: 3.7065 | Avg: 2.5322 | LR: 6.4e-08\n",
      "üìà Step 22500 | Loss: 3.0644 | Avg: 2.5271 | LR: 6.8e-08\n",
      "üìà Step 22525 | Loss: 2.5585 | Avg: 2.5657 | LR: 7.3e-08\n",
      "üìà Step 22550 | Loss: 1.8802 | Avg: 2.4474 | LR: 7.7e-08\n",
      "üìà Step 22575 | Loss: 2.8783 | Avg: 2.6081 | LR: 8.2e-08\n",
      "üìà Step 22600 | Loss: 2.0163 | Avg: 2.5231 | LR: 8.7e-08\n",
      "üìà Step 22625 | Loss: 2.3753 | Avg: 2.6186 | LR: 9.2e-08\n",
      "üìà Step 22650 | Loss: 3.4364 | Avg: 2.6125 | LR: 9.7e-08\n",
      "üìà Step 22675 | Loss: 2.8128 | Avg: 2.4999 | LR: 1.0e-07\n",
      "üìà Step 22700 | Loss: 3.1177 | Avg: 2.7490 | LR: 1.1e-07\n",
      "üìà Step 22725 | Loss: 3.2210 | Avg: 2.5139 | LR: 1.1e-07\n",
      "üìà Step 22750 | Loss: 2.1124 | Avg: 2.4032 | LR: 1.2e-07\n",
      "üìà Step 22775 | Loss: 2.8436 | Avg: 2.4351 | LR: 1.2e-07\n",
      "üìà Step 22800 | Loss: 2.5688 | Avg: 2.5481 | LR: 1.3e-07\n",
      "üìà Step 22825 | Loss: 2.1508 | Avg: 2.6895 | LR: 1.4e-07\n",
      "üìà Step 22850 | Loss: 2.5630 | Avg: 2.6808 | LR: 1.4e-07\n",
      "üìä Epoch 1 - Average train loss: 3.8794 (Stable!)\n",
      "üíæ End-of-epoch checkpoint saved: epoch1_final\n",
      "üîç Running validation for epoch 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76429120f51d4dfea3b5d0d25cf1aa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 1: train_loss=3.8794 val_loss=2.2446\n",
      "‚úÖ Best checkpoint updated!\n",
      "==== Epoch 2/3 (Ultra-Stable Mode) ====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e6ee027ba34585a7bbe862b1d2b57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 - Stable:   0%|          | 0/22861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Step    25 | Loss: 3.2407 | Avg: 2.6725 | LR: 1.5e-07\n",
      "üìà Step    50 | Loss: 2.2645 | Avg: 2.4665 | LR: 1.6e-07\n",
      "üìà Step    75 | Loss: 2.3702 | Avg: 2.5575 | LR: 1.6e-07\n",
      "üìà Step   100 | Loss: 2.1984 | Avg: 2.5338 | LR: 1.7e-07\n",
      "üìà Step   125 | Loss: 3.3773 | Avg: 2.6119 | LR: 1.8e-07\n",
      "üìà Step   150 | Loss: 3.7514 | Avg: 2.6584 | LR: 1.8e-07\n",
      "üìà Step   175 | Loss: 2.7201 | Avg: 2.7567 | LR: 1.9e-07\n",
      "üìà Step   200 | Loss: 2.5715 | Avg: 2.6099 | LR: 2.0e-07\n",
      "üìà Step   225 | Loss: 1.9143 | Avg: 2.5596 | LR: 2.1e-07\n",
      "üìà Step   250 | Loss: 1.7183 | Avg: 2.4486 | LR: 2.1e-07\n",
      "üìà Step   275 | Loss: 2.4030 | Avg: 2.7347 | LR: 2.2e-07\n",
      "üìà Step   300 | Loss: 1.8046 | Avg: 2.4877 | LR: 2.3e-07\n",
      "üìà Step   325 | Loss: 1.9950 | Avg: 2.5506 | LR: 2.3e-07\n",
      "üìà Step   350 | Loss: 2.8187 | Avg: 2.6090 | LR: 2.4e-07\n",
      "üìà Step   375 | Loss: 3.4929 | Avg: 2.4743 | LR: 2.5e-07\n",
      "üìà Step   400 | Loss: 2.8538 | Avg: 2.6624 | LR: 2.6e-07\n",
      "üìà Step   425 | Loss: 2.2952 | Avg: 2.5963 | LR: 2.7e-07\n",
      "üìà Step   450 | Loss: 2.1937 | Avg: 2.2483 | LR: 2.7e-07\n",
      "üìà Step   475 | Loss: 2.8166 | Avg: 2.3522 | LR: 2.8e-07\n",
      "üìà Step   500 | Loss: 2.6840 | Avg: 2.6203 | LR: 2.9e-07\n",
      "üìà Step   525 | Loss: 3.1259 | Avg: 2.6764 | LR: 3.0e-07\n",
      "üìà Step   550 | Loss: 2.3588 | Avg: 2.4662 | LR: 3.1e-07\n",
      "üìà Step   575 | Loss: 2.3231 | Avg: 2.6050 | LR: 3.2e-07\n",
      "üìà Step   600 | Loss: 2.3772 | Avg: 2.3071 | LR: 3.3e-07\n",
      "üìà Step   625 | Loss: 2.7960 | Avg: 2.6758 | LR: 3.3e-07\n",
      "üìà Step   650 | Loss: 2.0901 | Avg: 2.5826 | LR: 3.4e-07\n",
      "üìà Step   675 | Loss: 2.9060 | Avg: 2.5646 | LR: 3.5e-07\n",
      "üìà Step   700 | Loss: 3.4867 | Avg: 2.3928 | LR: 3.6e-07\n",
      "üìà Step   725 | Loss: 2.2985 | Avg: 2.3857 | LR: 3.7e-07\n",
      "üìà Step   750 | Loss: 2.5270 | Avg: 2.6187 | LR: 3.8e-07\n",
      "üìà Step   775 | Loss: 2.0600 | Avg: 2.4081 | LR: 3.9e-07\n",
      "üìà Step   800 | Loss: 1.9777 | Avg: 2.6966 | LR: 4.0e-07\n",
      "üìà Step   825 | Loss: 3.1123 | Avg: 2.4920 | LR: 4.1e-07\n",
      "üìà Step   850 | Loss: 3.1744 | Avg: 2.6261 | LR: 4.2e-07\n",
      "üìà Step   875 | Loss: 2.2519 | Avg: 2.4838 | LR: 4.3e-07\n",
      "üìà Step   900 | Loss: 2.4152 | Avg: 2.6597 | LR: 4.4e-07\n",
      "üìà Step   925 | Loss: 2.9066 | Avg: 2.5153 | LR: 4.5e-07\n",
      "üìà Step   950 | Loss: 2.3552 | Avg: 2.3612 | LR: 4.6e-07\n",
      "üìà Step   975 | Loss: 2.2002 | Avg: 2.4061 | LR: 4.7e-07\n",
      "üìà Step  1000 | Loss: 1.8687 | Avg: 2.7015 | LR: 4.8e-07\n",
      "üìà Step  1025 | Loss: 2.6597 | Avg: 2.5349 | LR: 4.9e-07\n",
      "üìà Step  1050 | Loss: 2.1184 | Avg: 2.4443 | LR: 5.0e-07\n",
      "üìà Step  1075 | Loss: 1.8522 | Avg: 2.7298 | LR: 5.1e-07\n",
      "üìà Step  1100 | Loss: 1.7911 | Avg: 2.2793 | LR: 5.2e-07\n",
      "üìà Step  1125 | Loss: 2.9741 | Avg: 2.2883 | LR: 5.3e-07\n",
      "üìà Step  1150 | Loss: 2.7849 | Avg: 2.5506 | LR: 5.4e-07\n",
      "üìà Step  1175 | Loss: 1.6350 | Avg: 2.4231 | LR: 5.5e-07\n",
      "üìà Step  1200 | Loss: 2.9401 | Avg: 2.6565 | LR: 5.6e-07\n",
      "üìà Step  1225 | Loss: 1.8172 | Avg: 2.5557 | LR: 5.7e-07\n",
      "üìà Step  1250 | Loss: 1.3516 | Avg: 2.6020 | LR: 5.8e-07\n",
      "üìà Step  1275 | Loss: 1.6311 | Avg: 2.4667 | LR: 5.9e-07\n",
      "üìà Step  1300 | Loss: 3.1336 | Avg: 2.5690 | LR: 6.1e-07\n",
      "üìà Step  1325 | Loss: 2.5070 | Avg: 2.4684 | LR: 6.2e-07\n",
      "üìà Step  1350 | Loss: 3.7272 | Avg: 2.4050 | LR: 6.3e-07\n",
      "üìà Step  1375 | Loss: 2.7272 | Avg: 2.4420 | LR: 6.4e-07\n",
      "üìà Step  1400 | Loss: 2.6030 | Avg: 2.4934 | LR: 6.5e-07\n",
      "üìà Step  1425 | Loss: 2.5962 | Avg: 2.5223 | LR: 6.6e-07\n",
      "üìà Step  1450 | Loss: 2.9722 | Avg: 2.4220 | LR: 6.7e-07\n",
      "üìà Step  1475 | Loss: 2.6111 | Avg: 2.6474 | LR: 6.8e-07\n",
      "üìà Step  1500 | Loss: 1.9963 | Avg: 2.4589 | LR: 7.0e-07\n",
      "üìà Step  1525 | Loss: 2.6860 | Avg: 2.6421 | LR: 7.1e-07\n",
      "üìà Step  1550 | Loss: 3.0354 | Avg: 2.4967 | LR: 7.2e-07\n",
      "üìà Step  1575 | Loss: 1.5135 | Avg: 2.5569 | LR: 7.3e-07\n",
      "üìà Step  1600 | Loss: 2.4644 | Avg: 2.5692 | LR: 7.4e-07\n",
      "üìà Step  1625 | Loss: 2.2754 | Avg: 2.2844 | LR: 7.5e-07\n",
      "üìà Step  1650 | Loss: 2.4858 | Avg: 2.4991 | LR: 7.6e-07\n",
      "üìà Step  1675 | Loss: 2.8370 | Avg: 2.5954 | LR: 7.8e-07\n",
      "üìà Step  1700 | Loss: 2.0550 | Avg: 2.4613 | LR: 7.9e-07\n",
      "üìà Step  1725 | Loss: 2.6184 | Avg: 2.3831 | LR: 8.0e-07\n",
      "üìà Step  1750 | Loss: 2.3181 | Avg: 2.2938 | LR: 8.1e-07\n",
      "üìà Step  1775 | Loss: 2.8261 | Avg: 2.4220 | LR: 8.2e-07\n",
      "üìà Step  1800 | Loss: 1.5777 | Avg: 2.3779 | LR: 8.3e-07\n",
      "üìà Step  1825 | Loss: 3.0349 | Avg: 2.4866 | LR: 8.5e-07\n",
      "üìà Step  1850 | Loss: 2.4776 | Avg: 2.5277 | LR: 8.6e-07\n",
      "üìà Step  1875 | Loss: 2.4800 | Avg: 2.5039 | LR: 8.7e-07\n",
      "üìà Step  1900 | Loss: 2.0441 | Avg: 2.6568 | LR: 8.8e-07\n",
      "üìà Step  1925 | Loss: 1.2989 | Avg: 2.2888 | LR: 8.9e-07\n",
      "üìà Step  1950 | Loss: 2.1703 | Avg: 2.5630 | LR: 9.1e-07\n",
      "üìà Step  1975 | Loss: 2.5864 | Avg: 2.2914 | LR: 9.2e-07\n",
      "üìà Step  2000 | Loss: 2.0895 | Avg: 2.4402 | LR: 9.3e-07\n",
      "üìà Step  2025 | Loss: 1.5372 | Avg: 2.6483 | LR: 9.4e-07\n",
      "üìà Step  2050 | Loss: 2.5005 | Avg: 2.3666 | LR: 9.5e-07\n",
      "üìà Step  2075 | Loss: 1.9619 | Avg: 2.5380 | LR: 9.7e-07\n",
      "üìà Step  2100 | Loss: 2.5847 | Avg: 2.6580 | LR: 9.8e-07\n",
      "üìà Step  2125 | Loss: 2.6702 | Avg: 2.5719 | LR: 9.9e-07\n",
      "üìà Step  2150 | Loss: 2.9478 | Avg: 2.6894 | LR: 1.0e-06\n",
      "üìà Step  2175 | Loss: 1.7405 | Avg: 2.2607 | LR: 1.0e-06\n",
      "üìà Step  2200 | Loss: 3.1791 | Avg: 2.6059 | LR: 1.0e-06\n",
      "üìà Step  2225 | Loss: 2.6405 | Avg: 2.3895 | LR: 1.0e-06\n",
      "üìà Step  2250 | Loss: 1.8806 | Avg: 2.5489 | LR: 1.0e-06\n",
      "üìà Step  2275 | Loss: 1.9196 | Avg: 2.4531 | LR: 1.1e-06\n",
      "üìà Step  2300 | Loss: 2.1228 | Avg: 2.4958 | LR: 1.1e-06\n",
      "üìà Step  2325 | Loss: 1.5724 | Avg: 2.5325 | LR: 1.1e-06\n",
      "üìà Step  2350 | Loss: 2.6950 | Avg: 2.5358 | LR: 1.1e-06\n",
      "üìà Step  2375 | Loss: 2.3173 | Avg: 2.3580 | LR: 1.1e-06\n",
      "üìà Step  2400 | Loss: 1.8859 | Avg: 2.3824 | LR: 1.1e-06\n",
      "üìà Step  2425 | Loss: 3.1205 | Avg: 2.6280 | LR: 1.1e-06\n",
      "üìà Step  2450 | Loss: 2.2072 | Avg: 2.7008 | LR: 1.1e-06\n",
      "üìà Step  2475 | Loss: 3.1197 | Avg: 2.4702 | LR: 1.2e-06\n",
      "üìà Step  2500 | Loss: 2.5946 | Avg: 2.4551 | LR: 1.2e-06\n",
      "üìà Step  2525 | Loss: 2.4118 | Avg: 2.5292 | LR: 1.2e-06\n",
      "üìà Step  2550 | Loss: 2.3409 | Avg: 2.4409 | LR: 1.2e-06\n",
      "üìà Step  2575 | Loss: 2.5973 | Avg: 2.3830 | LR: 1.2e-06\n",
      "üìà Step  2600 | Loss: 2.9072 | Avg: 2.5032 | LR: 1.2e-06\n",
      "üìà Step  2625 | Loss: 2.4072 | Avg: 2.3058 | LR: 1.2e-06\n",
      "üìà Step  2650 | Loss: 2.7606 | Avg: 2.3032 | LR: 1.2e-06\n",
      "üìà Step  2675 | Loss: 3.8905 | Avg: 2.2843 | LR: 1.2e-06\n",
      "üìà Step  2700 | Loss: 1.6520 | Avg: 2.3941 | LR: 1.3e-06\n",
      "üìà Step  2725 | Loss: 2.5557 | Avg: 2.4269 | LR: 1.3e-06\n",
      "üìà Step  2750 | Loss: 1.7207 | Avg: 2.4682 | LR: 1.3e-06\n",
      "üìà Step  2775 | Loss: 2.6901 | Avg: 2.4024 | LR: 1.3e-06\n",
      "üìà Step  2800 | Loss: 2.1181 | Avg: 2.2176 | LR: 1.3e-06\n",
      "üìà Step  2825 | Loss: 1.8433 | Avg: 2.4241 | LR: 1.3e-06\n",
      "üìà Step  2850 | Loss: 2.9460 | Avg: 2.2953 | LR: 1.3e-06\n",
      "üìà Step  2875 | Loss: 1.9705 | Avg: 2.6342 | LR: 1.3e-06\n",
      "üìà Step  2900 | Loss: 1.9035 | Avg: 2.4270 | LR: 1.4e-06\n",
      "üìà Step  2925 | Loss: 3.3654 | Avg: 2.4768 | LR: 1.4e-06\n",
      "üìà Step  2950 | Loss: 2.3516 | Avg: 2.5453 | LR: 1.4e-06\n",
      "üìà Step  2975 | Loss: 2.1153 | Avg: 2.3196 | LR: 1.4e-06\n",
      "üìà Step  3000 | Loss: 2.4437 | Avg: 2.2756 | LR: 1.4e-06\n",
      "üìà Step  3025 | Loss: 2.6859 | Avg: 2.4501 | LR: 1.4e-06\n",
      "üìà Step  3050 | Loss: 2.9238 | Avg: 2.5563 | LR: 1.4e-06\n",
      "üìà Step  3075 | Loss: 2.2910 | Avg: 2.3651 | LR: 1.4e-06\n",
      "üìà Step  3100 | Loss: 2.3319 | Avg: 2.4589 | LR: 1.4e-06\n",
      "üìà Step  3125 | Loss: 2.6139 | Avg: 2.4722 | LR: 1.5e-06\n",
      "üìà Step  3150 | Loss: 3.4410 | Avg: 2.5126 | LR: 1.5e-06\n",
      "üìà Step  3175 | Loss: 2.1644 | Avg: 2.2799 | LR: 1.5e-06\n",
      "üìà Step  3200 | Loss: 2.3164 | Avg: 2.3238 | LR: 1.5e-06\n",
      "üìà Step  3225 | Loss: 3.0642 | Avg: 2.4951 | LR: 1.5e-06\n",
      "üìà Step  3250 | Loss: 2.8501 | Avg: 2.3102 | LR: 1.5e-06\n",
      "üìà Step  3275 | Loss: 2.4913 | Avg: 2.3337 | LR: 1.5e-06\n",
      "üìà Step  3300 | Loss: 2.4369 | Avg: 2.6840 | LR: 1.5e-06\n",
      "üìà Step  3325 | Loss: 1.9284 | Avg: 2.3648 | LR: 1.5e-06\n",
      "üìà Step  3350 | Loss: 2.1451 | Avg: 2.3858 | LR: 1.5e-06\n",
      "üìà Step  3375 | Loss: 2.2659 | Avg: 2.4007 | LR: 1.6e-06\n",
      "üìà Step  3400 | Loss: 2.9652 | Avg: 2.4261 | LR: 1.6e-06\n",
      "üìà Step  3425 | Loss: 3.2399 | Avg: 2.4785 | LR: 1.6e-06\n",
      "üíæ Stable checkpoint saved at 15% of epoch 2\n",
      "üìà Step  3450 | Loss: 2.8071 | Avg: 2.4234 | LR: 1.6e-06\n",
      "üìà Step  3475 | Loss: 1.8316 | Avg: 2.4157 | LR: 1.6e-06\n",
      "üìà Step  3500 | Loss: 2.7030 | Avg: 2.2729 | LR: 1.6e-06\n",
      "üìà Step  3525 | Loss: 3.2621 | Avg: 2.5305 | LR: 1.6e-06\n",
      "üìà Step  3550 | Loss: 2.5839 | Avg: 2.1674 | LR: 1.6e-06\n",
      "üìà Step  3575 | Loss: 2.7415 | Avg: 2.2972 | LR: 1.6e-06\n",
      "üìà Step  3600 | Loss: 2.2780 | Avg: 2.3936 | LR: 1.6e-06\n",
      "üìà Step  3625 | Loss: 2.2283 | Avg: 2.4637 | LR: 1.6e-06\n",
      "üìà Step  3650 | Loss: 1.6771 | Avg: 2.3676 | LR: 1.7e-06\n",
      "üìà Step  3675 | Loss: 2.6406 | Avg: 2.3660 | LR: 1.7e-06\n",
      "üìà Step  3700 | Loss: 2.4679 | Avg: 2.4559 | LR: 1.7e-06\n",
      "üìà Step  3725 | Loss: 2.3393 | Avg: 2.3563 | LR: 1.7e-06\n",
      "üìà Step  3750 | Loss: 2.0803 | Avg: 2.3551 | LR: 1.7e-06\n",
      "üìà Step  3775 | Loss: 2.3607 | Avg: 2.2730 | LR: 1.7e-06\n",
      "üìà Step  3800 | Loss: 2.0201 | Avg: 2.3035 | LR: 1.7e-06\n",
      "üìà Step  3825 | Loss: 1.6673 | Avg: 2.1770 | LR: 1.7e-06\n",
      "üìà Step  3850 | Loss: 2.1033 | Avg: 2.3856 | LR: 1.7e-06\n",
      "üìà Step  3875 | Loss: 2.2681 | Avg: 2.4221 | LR: 1.7e-06\n",
      "üìà Step  3900 | Loss: 2.1016 | Avg: 2.5619 | LR: 1.7e-06\n",
      "üìà Step  3925 | Loss: 2.4066 | Avg: 2.3061 | LR: 1.8e-06\n",
      "üìà Step  3950 | Loss: 2.8062 | Avg: 2.5018 | LR: 1.8e-06\n",
      "üìà Step  3975 | Loss: 3.0856 | Avg: 2.3006 | LR: 1.8e-06\n",
      "üìà Step  4000 | Loss: 2.2722 | Avg: 2.3272 | LR: 1.8e-06\n",
      "üìà Step  4025 | Loss: 2.3884 | Avg: 2.4843 | LR: 1.8e-06\n",
      "üìà Step  4050 | Loss: 1.8424 | Avg: 2.1643 | LR: 1.8e-06\n",
      "üìà Step  4075 | Loss: 1.8093 | Avg: 2.4391 | LR: 1.8e-06\n",
      "üìà Step  4100 | Loss: 1.8064 | Avg: 2.2869 | LR: 1.8e-06\n",
      "üìà Step  4125 | Loss: 2.7041 | Avg: 2.1272 | LR: 1.8e-06\n",
      "üìà Step  4150 | Loss: 3.1655 | Avg: 2.3497 | LR: 1.8e-06\n",
      "üìà Step  4175 | Loss: 1.7784 | Avg: 2.3688 | LR: 1.8e-06\n",
      "üìà Step  4200 | Loss: 1.8446 | Avg: 2.3485 | LR: 1.8e-06\n",
      "üìà Step  4225 | Loss: 1.7104 | Avg: 2.5005 | LR: 1.8e-06\n",
      "üìà Step  4250 | Loss: 2.1721 | Avg: 2.2577 | LR: 1.8e-06\n",
      "üìà Step  4275 | Loss: 2.4399 | Avg: 2.4416 | LR: 1.9e-06\n",
      "üìà Step  4300 | Loss: 1.6698 | Avg: 2.4489 | LR: 1.9e-06\n",
      "üìà Step  4325 | Loss: 2.8066 | Avg: 2.3629 | LR: 1.9e-06\n",
      "üìà Step  4350 | Loss: 3.0546 | Avg: 2.4692 | LR: 1.9e-06\n",
      "üìà Step  4375 | Loss: 1.8747 | Avg: 2.3509 | LR: 1.9e-06\n",
      "üìà Step  4400 | Loss: 2.3658 | Avg: 2.3550 | LR: 1.9e-06\n",
      "üìà Step  4425 | Loss: 1.6468 | Avg: 2.3888 | LR: 1.9e-06\n",
      "üìà Step  4450 | Loss: 2.3256 | Avg: 2.4646 | LR: 1.9e-06\n",
      "üìà Step  4475 | Loss: 2.3094 | Avg: 2.3910 | LR: 1.9e-06\n",
      "üìà Step  4500 | Loss: 2.1291 | Avg: 2.2719 | LR: 1.9e-06\n",
      "üìà Step  4525 | Loss: 1.7468 | Avg: 2.2807 | LR: 1.9e-06\n",
      "üìà Step  4550 | Loss: 1.4804 | Avg: 2.0494 | LR: 1.9e-06\n",
      "üìà Step  4575 | Loss: 3.0727 | Avg: 2.4565 | LR: 1.9e-06\n",
      "üìà Step  4600 | Loss: 1.2002 | Avg: 2.1649 | LR: 1.9e-06\n",
      "üìà Step  4625 | Loss: 2.3599 | Avg: 2.3015 | LR: 1.9e-06\n",
      "üìà Step  4650 | Loss: 2.7758 | Avg: 2.3785 | LR: 1.9e-06\n",
      "üìà Step  4675 | Loss: 2.4535 | Avg: 2.0505 | LR: 1.9e-06\n",
      "üìà Step  4700 | Loss: 3.1112 | Avg: 2.1604 | LR: 1.9e-06\n",
      "üìà Step  4725 | Loss: 3.1141 | Avg: 2.2987 | LR: 1.9e-06\n",
      "üìà Step  4750 | Loss: 1.5545 | Avg: 2.3185 | LR: 1.9e-06\n",
      "üìà Step  4775 | Loss: 3.0784 | Avg: 2.5306 | LR: 2.0e-06\n",
      "üìà Step  4800 | Loss: 2.4996 | Avg: 2.2361 | LR: 2.0e-06\n",
      "üìà Step  4825 | Loss: 1.8748 | Avg: 2.4415 | LR: 2.0e-06\n",
      "üìà Step  4850 | Loss: 2.3200 | Avg: 2.4211 | LR: 2.0e-06\n",
      "üìà Step  4875 | Loss: 2.7231 | Avg: 2.1858 | LR: 2.0e-06\n",
      "üìà Step  4900 | Loss: 2.9929 | Avg: 2.3773 | LR: 2.0e-06\n",
      "üìà Step  4925 | Loss: 2.0364 | Avg: 2.3211 | LR: 2.0e-06\n",
      "üìà Step  4950 | Loss: 1.9562 | Avg: 2.2253 | LR: 2.0e-06\n",
      "üìà Step  4975 | Loss: 2.0953 | Avg: 2.2824 | LR: 2.0e-06\n",
      "üìà Step  5000 | Loss: 3.1914 | Avg: 2.1531 | LR: 2.0e-06\n",
      "üìà Step  5025 | Loss: 2.3627 | Avg: 2.2888 | LR: 2.0e-06\n",
      "üìà Step  5050 | Loss: 2.3479 | Avg: 2.2770 | LR: 2.0e-06\n",
      "üìà Step  5075 | Loss: 2.6580 | Avg: 2.1942 | LR: 2.0e-06\n",
      "üìà Step  5100 | Loss: 2.2475 | Avg: 2.1525 | LR: 2.0e-06\n",
      "üìà Step  5125 | Loss: 2.0240 | Avg: 2.2902 | LR: 2.0e-06\n",
      "üìà Step  5150 | Loss: 2.6732 | Avg: 2.2640 | LR: 2.0e-06\n",
      "üìà Step  5175 | Loss: 2.2339 | Avg: 2.1775 | LR: 2.0e-06\n",
      "üìà Step  5200 | Loss: 2.3315 | Avg: 2.3858 | LR: 2.0e-06\n",
      "üìà Step  5225 | Loss: 2.5315 | Avg: 1.9742 | LR: 2.0e-06\n",
      "üìà Step  5250 | Loss: 2.4569 | Avg: 2.2530 | LR: 2.0e-06\n",
      "üìà Step  5275 | Loss: 1.9235 | Avg: 2.2529 | LR: 2.0e-06\n",
      "üìà Step  5300 | Loss: 1.7826 | Avg: 2.1627 | LR: 2.0e-06\n",
      "üìà Step  5325 | Loss: 1.7242 | Avg: 2.2129 | LR: 2.0e-06\n",
      "üìà Step  5350 | Loss: 1.4730 | Avg: 2.2819 | LR: 2.0e-06\n",
      "üìà Step  5375 | Loss: 1.5969 | Avg: 2.0064 | LR: 2.0e-06\n",
      "üìà Step  5400 | Loss: 3.4140 | Avg: 2.2553 | LR: 2.0e-06\n",
      "üìà Step  5425 | Loss: 1.8492 | Avg: 2.0085 | LR: 2.0e-06\n",
      "üìà Step  5450 | Loss: 2.4406 | Avg: 2.1630 | LR: 2.0e-06\n",
      "üìà Step  5475 | Loss: 2.9085 | Avg: 2.1546 | LR: 2.0e-06\n",
      "üìà Step  5500 | Loss: 2.5297 | Avg: 2.2961 | LR: 2.0e-06\n",
      "üìà Step  5525 | Loss: 1.2920 | Avg: 2.3280 | LR: 2.0e-06\n",
      "üìà Step  5550 | Loss: 2.6228 | Avg: 2.1317 | LR: 2.0e-06\n",
      "üìà Step  5575 | Loss: 3.2251 | Avg: 2.2424 | LR: 2.0e-06\n",
      "üìà Step  5600 | Loss: 2.2090 | Avg: 2.2283 | LR: 2.0e-06\n",
      "üìà Step  5625 | Loss: 1.0338 | Avg: 2.1996 | LR: 2.0e-06\n",
      "üìà Step  5650 | Loss: 2.0226 | Avg: 2.2567 | LR: 2.0e-06\n",
      "üìà Step  5675 | Loss: 2.3283 | Avg: 2.1091 | LR: 2.0e-06\n",
      "üìà Step  5700 | Loss: 2.3346 | Avg: 2.3374 | LR: 2.0e-06\n",
      "üìà Step  5725 | Loss: 0.9396 | Avg: 2.0941 | LR: 2.0e-06\n",
      "üìà Step  5750 | Loss: 1.5522 | Avg: 2.3528 | LR: 2.0e-06\n",
      "üìà Step  5775 | Loss: 2.7464 | Avg: 2.4723 | LR: 2.0e-06\n",
      "üìà Step  5800 | Loss: 1.6709 | Avg: 2.1776 | LR: 2.0e-06\n",
      "üìà Step  5825 | Loss: 2.7950 | Avg: 2.4094 | LR: 2.0e-06\n",
      "üìà Step  5850 | Loss: 2.0396 | Avg: 2.0630 | LR: 2.0e-06\n",
      "üìà Step  5875 | Loss: 1.8377 | Avg: 2.1905 | LR: 2.0e-06\n",
      "üìà Step  5900 | Loss: 1.7027 | Avg: 2.2365 | LR: 2.0e-06\n",
      "üìà Step  5925 | Loss: 2.5471 | Avg: 2.0811 | LR: 2.0e-06\n",
      "üìà Step  5950 | Loss: 2.0366 | Avg: 2.1565 | LR: 2.0e-06\n",
      "üìà Step  5975 | Loss: 1.6082 | Avg: 2.1360 | LR: 2.0e-06\n",
      "üìà Step  6000 | Loss: 3.0622 | Avg: 2.2689 | LR: 2.0e-06\n",
      "üìà Step  6025 | Loss: 2.8345 | Avg: 2.1062 | LR: 2.0e-06\n",
      "üìà Step  6050 | Loss: 2.2299 | Avg: 2.3103 | LR: 2.0e-06\n",
      "üìà Step  6075 | Loss: 1.8128 | Avg: 2.1505 | LR: 2.0e-06\n",
      "üìà Step  6100 | Loss: 3.1477 | Avg: 2.0467 | LR: 1.9e-06\n",
      "üìà Step  6125 | Loss: 2.3882 | Avg: 2.2638 | LR: 1.9e-06\n",
      "üìà Step  6150 | Loss: 3.0322 | Avg: 2.3483 | LR: 1.9e-06\n",
      "üìà Step  6175 | Loss: 2.0552 | Avg: 2.2528 | LR: 1.9e-06\n",
      "üìà Step  6200 | Loss: 1.5470 | Avg: 2.1323 | LR: 1.9e-06\n",
      "üìà Step  6225 | Loss: 2.0012 | Avg: 2.3441 | LR: 1.9e-06\n",
      "üìà Step  6250 | Loss: 1.8513 | Avg: 2.2379 | LR: 1.9e-06\n",
      "üìà Step  6275 | Loss: 2.5767 | Avg: 2.2854 | LR: 1.9e-06\n",
      "üìà Step  6300 | Loss: 1.5394 | Avg: 2.2424 | LR: 1.9e-06\n",
      "üìà Step  6325 | Loss: 2.2819 | Avg: 2.2235 | LR: 1.9e-06\n",
      "üìà Step  6350 | Loss: 1.2190 | Avg: 2.0590 | LR: 1.9e-06\n",
      "üìà Step  6375 | Loss: 1.3723 | Avg: 2.1084 | LR: 1.9e-06\n",
      "üìà Step  6400 | Loss: 1.9301 | Avg: 2.0823 | LR: 1.9e-06\n",
      "üìà Step  6425 | Loss: 1.9620 | Avg: 2.0563 | LR: 1.9e-06\n",
      "üìà Step  6450 | Loss: 2.1660 | Avg: 2.0499 | LR: 1.9e-06\n",
      "üìà Step  6475 | Loss: 1.8045 | Avg: 2.1354 | LR: 1.9e-06\n",
      "üìà Step  6500 | Loss: 2.5787 | Avg: 2.2525 | LR: 1.9e-06\n",
      "üìà Step  6525 | Loss: 1.6376 | Avg: 2.0411 | LR: 1.9e-06\n",
      "üìà Step  6550 | Loss: 3.5007 | Avg: 2.1304 | LR: 1.9e-06\n",
      "üìà Step  6575 | Loss: 3.1061 | Avg: 2.2844 | LR: 1.9e-06\n",
      "üìà Step  6600 | Loss: 2.0772 | Avg: 1.9644 | LR: 1.8e-06\n",
      "üìà Step  6625 | Loss: 2.3763 | Avg: 2.0722 | LR: 1.8e-06\n",
      "üìà Step  6650 | Loss: 2.5084 | Avg: 2.2121 | LR: 1.8e-06\n",
      "üìà Step  6675 | Loss: 2.4207 | Avg: 2.2091 | LR: 1.8e-06\n",
      "üìà Step  6700 | Loss: 2.2905 | Avg: 2.0384 | LR: 1.8e-06\n",
      "üìà Step  6725 | Loss: 1.5129 | Avg: 1.9788 | LR: 1.8e-06\n",
      "üìà Step  6750 | Loss: 2.1927 | Avg: 2.1100 | LR: 1.8e-06\n",
      "üìà Step  6775 | Loss: 2.6976 | Avg: 2.2247 | LR: 1.8e-06\n",
      "üìà Step  6800 | Loss: 1.7024 | Avg: 2.0044 | LR: 1.8e-06\n",
      "üìà Step  6825 | Loss: 1.5228 | Avg: 2.0296 | LR: 1.8e-06\n",
      "üìà Step  6850 | Loss: 1.9794 | Avg: 2.0922 | LR: 1.8e-06\n",
      "üíæ Stable checkpoint saved at 30% of epoch 2\n",
      "üìà Step  6875 | Loss: 3.0413 | Avg: 2.1624 | LR: 1.8e-06\n",
      "üìà Step  6900 | Loss: 1.6077 | Avg: 2.2034 | LR: 1.8e-06\n",
      "üìà Step  6925 | Loss: 2.0387 | Avg: 1.9875 | LR: 1.8e-06\n",
      "üìà Step  6950 | Loss: 2.5053 | Avg: 2.0538 | LR: 1.7e-06\n",
      "üìà Step  6975 | Loss: 2.7917 | Avg: 2.0007 | LR: 1.7e-06\n",
      "üìà Step  7000 | Loss: 1.0803 | Avg: 1.8993 | LR: 1.7e-06\n",
      "üìà Step  7025 | Loss: 1.7639 | Avg: 1.9960 | LR: 1.7e-06\n",
      "üìà Step  7050 | Loss: 1.9847 | Avg: 1.9104 | LR: 1.7e-06\n",
      "üìà Step  7075 | Loss: 2.4099 | Avg: 2.2473 | LR: 1.7e-06\n",
      "üìà Step  7100 | Loss: 2.7273 | Avg: 2.1617 | LR: 1.7e-06\n",
      "üìà Step  7125 | Loss: 1.0778 | Avg: 1.9553 | LR: 1.7e-06\n",
      "üìà Step  7150 | Loss: 2.4419 | Avg: 2.1433 | LR: 1.7e-06\n",
      "üìà Step  7175 | Loss: 1.8245 | Avg: 2.2120 | LR: 1.7e-06\n",
      "üìà Step  7200 | Loss: 1.6949 | Avg: 2.2202 | LR: 1.7e-06\n",
      "üìà Step  7225 | Loss: 2.3242 | Avg: 1.9792 | LR: 1.7e-06\n",
      "üìà Step  7250 | Loss: 1.9058 | Avg: 1.9866 | LR: 1.6e-06\n",
      "üìà Step  7275 | Loss: 1.6574 | Avg: 2.1026 | LR: 1.6e-06\n",
      "üìà Step  7300 | Loss: 2.1153 | Avg: 2.0047 | LR: 1.6e-06\n",
      "üìà Step  7325 | Loss: 1.6014 | Avg: 2.0098 | LR: 1.6e-06\n",
      "üìà Step  7350 | Loss: 1.8891 | Avg: 1.9411 | LR: 1.6e-06\n",
      "üìà Step  7375 | Loss: 2.2118 | Avg: 2.3421 | LR: 1.6e-06\n",
      "üìà Step  7400 | Loss: 2.2826 | Avg: 2.2218 | LR: 1.6e-06\n",
      "üìà Step  7425 | Loss: 2.3941 | Avg: 2.0873 | LR: 1.6e-06\n",
      "üìà Step  7450 | Loss: 1.3031 | Avg: 1.9972 | LR: 1.6e-06\n",
      "üìà Step  7475 | Loss: 2.3969 | Avg: 2.0752 | LR: 1.6e-06\n",
      "üìà Step  7500 | Loss: 1.9749 | Avg: 1.8983 | LR: 1.6e-06\n",
      "üìà Step  7525 | Loss: 3.1441 | Avg: 2.0345 | LR: 1.5e-06\n",
      "üìà Step  7550 | Loss: 2.4744 | Avg: 2.0088 | LR: 1.5e-06\n",
      "üìà Step  7575 | Loss: 3.3660 | Avg: 2.2587 | LR: 1.5e-06\n",
      "üìà Step  7600 | Loss: 2.0634 | Avg: 2.0453 | LR: 1.5e-06\n",
      "üìà Step  7625 | Loss: 1.0983 | Avg: 1.9600 | LR: 1.5e-06\n",
      "üìà Step  7650 | Loss: 2.3442 | Avg: 1.9863 | LR: 1.5e-06\n",
      "üìà Step  7675 | Loss: 2.1924 | Avg: 2.1793 | LR: 1.5e-06\n",
      "üìà Step  7700 | Loss: 1.9619 | Avg: 2.1006 | LR: 1.5e-06\n",
      "üìà Step  7725 | Loss: 1.5525 | Avg: 1.9180 | LR: 1.5e-06\n",
      "üìà Step  7750 | Loss: 2.0911 | Avg: 2.1303 | LR: 1.4e-06\n",
      "üìà Step  7775 | Loss: 1.7652 | Avg: 1.9169 | LR: 1.4e-06\n",
      "üìà Step  7800 | Loss: 2.9391 | Avg: 1.9792 | LR: 1.4e-06\n",
      "üìà Step  7825 | Loss: 2.9767 | Avg: 2.0521 | LR: 1.4e-06\n",
      "üìà Step  7850 | Loss: 2.0812 | Avg: 2.2195 | LR: 1.4e-06\n",
      "üìà Step  7875 | Loss: 1.8139 | Avg: 2.0856 | LR: 1.4e-06\n",
      "üìà Step  7900 | Loss: 1.3691 | Avg: 2.1491 | LR: 1.4e-06\n",
      "üìà Step  7925 | Loss: 3.0132 | Avg: 1.9885 | LR: 1.4e-06\n",
      "üìà Step  7950 | Loss: 2.3932 | Avg: 2.0249 | LR: 1.4e-06\n",
      "üìà Step  7975 | Loss: 2.3367 | Avg: 1.8557 | LR: 1.3e-06\n",
      "üìà Step  8000 | Loss: 0.8697 | Avg: 2.0133 | LR: 1.3e-06\n",
      "üìà Step  8025 | Loss: 2.6368 | Avg: 2.0946 | LR: 1.3e-06\n",
      "üìà Step  8050 | Loss: 2.0664 | Avg: 2.0960 | LR: 1.3e-06\n",
      "üìà Step  8075 | Loss: 2.3505 | Avg: 2.0165 | LR: 1.3e-06\n",
      "üìà Step  8100 | Loss: 1.2042 | Avg: 2.1169 | LR: 1.3e-06\n",
      "üìà Step  8125 | Loss: 1.6459 | Avg: 2.1372 | LR: 1.3e-06\n",
      "üìà Step  8150 | Loss: 1.7145 | Avg: 2.1366 | LR: 1.3e-06\n",
      "üìà Step  8175 | Loss: 3.3272 | Avg: 2.1600 | LR: 1.3e-06\n",
      "üìà Step  8200 | Loss: 0.9715 | Avg: 1.9930 | LR: 1.2e-06\n",
      "üìà Step  8225 | Loss: 0.9101 | Avg: 1.8719 | LR: 1.2e-06\n",
      "üìà Step  8250 | Loss: 1.7085 | Avg: 2.0457 | LR: 1.2e-06\n",
      "üìà Step  8275 | Loss: 2.5329 | Avg: 1.9916 | LR: 1.2e-06\n",
      "üìà Step  8300 | Loss: 1.7147 | Avg: 2.1427 | LR: 1.2e-06\n",
      "üìà Step  8325 | Loss: 1.9548 | Avg: 2.0345 | LR: 1.2e-06\n",
      "üìà Step  8350 | Loss: 2.6157 | Avg: 2.1891 | LR: 1.2e-06\n",
      "üìà Step  8375 | Loss: 1.8765 | Avg: 1.8419 | LR: 1.2e-06\n",
      "üìà Step  8400 | Loss: 2.3108 | Avg: 2.0440 | LR: 1.2e-06\n",
      "üìà Step  8425 | Loss: 1.3136 | Avg: 1.7580 | LR: 1.1e-06\n",
      "üìà Step  8450 | Loss: 0.7359 | Avg: 1.9072 | LR: 1.1e-06\n",
      "üìà Step  8475 | Loss: 2.1317 | Avg: 2.1221 | LR: 1.1e-06\n",
      "üìà Step  8500 | Loss: 2.5982 | Avg: 1.9553 | LR: 1.1e-06\n",
      "üìà Step  8525 | Loss: 2.2461 | Avg: 1.9155 | LR: 1.1e-06\n",
      "üìà Step  8550 | Loss: 1.8844 | Avg: 2.0569 | LR: 1.1e-06\n",
      "üìà Step  8575 | Loss: 1.6162 | Avg: 2.0180 | LR: 1.1e-06\n",
      "üìà Step  8600 | Loss: 2.3309 | Avg: 2.0866 | LR: 1.1e-06\n",
      "üìà Step  8625 | Loss: 1.5744 | Avg: 2.0170 | LR: 1.0e-06\n",
      "üìà Step  8650 | Loss: 2.2174 | Avg: 2.1377 | LR: 1.0e-06\n",
      "üìà Step  8675 | Loss: 1.4626 | Avg: 1.8434 | LR: 1.0e-06\n",
      "üìà Step  8700 | Loss: 3.2514 | Avg: 1.9888 | LR: 1.0e-06\n",
      "üìà Step  8725 | Loss: 1.9496 | Avg: 1.9563 | LR: 1.0e-06\n",
      "üìà Step  8750 | Loss: 2.3950 | Avg: 2.0810 | LR: 9.9e-07\n",
      "üìà Step  8775 | Loss: 2.2662 | Avg: 2.1374 | LR: 9.7e-07\n",
      "üìà Step  8800 | Loss: 2.8575 | Avg: 2.0287 | LR: 9.6e-07\n",
      "üìà Step  8825 | Loss: 2.1188 | Avg: 2.1617 | LR: 9.5e-07\n",
      "üìà Step  8850 | Loss: 2.2617 | Avg: 2.0187 | LR: 9.4e-07\n",
      "üìà Step  8875 | Loss: 2.2589 | Avg: 2.0443 | LR: 9.3e-07\n",
      "üìà Step  8900 | Loss: 1.4572 | Avg: 2.0894 | LR: 9.2e-07\n",
      "üìà Step  8925 | Loss: 1.8700 | Avg: 1.9545 | LR: 9.0e-07\n",
      "üìà Step  8950 | Loss: 1.5285 | Avg: 2.1510 | LR: 8.9e-07\n",
      "üìà Step  8975 | Loss: 1.0288 | Avg: 1.9187 | LR: 8.8e-07\n",
      "üìà Step  9000 | Loss: 1.7351 | Avg: 1.9941 | LR: 8.7e-07\n",
      "üìà Step  9025 | Loss: 2.5011 | Avg: 2.0199 | LR: 8.6e-07\n",
      "üìà Step  9050 | Loss: 1.7201 | Avg: 2.0315 | LR: 8.4e-07\n",
      "üìà Step  9075 | Loss: 1.4729 | Avg: 2.2187 | LR: 8.3e-07\n",
      "üìà Step  9100 | Loss: 2.0570 | Avg: 2.1136 | LR: 8.2e-07\n",
      "üìà Step  9125 | Loss: 1.6553 | Avg: 1.8555 | LR: 8.1e-07\n",
      "üìà Step  9150 | Loss: 1.8605 | Avg: 2.0256 | LR: 8.0e-07\n",
      "üìà Step  9175 | Loss: 1.6400 | Avg: 1.8855 | LR: 7.9e-07\n",
      "üìà Step  9200 | Loss: 1.5421 | Avg: 1.9961 | LR: 7.7e-07\n",
      "üìà Step  9225 | Loss: 1.5882 | Avg: 1.9862 | LR: 7.6e-07\n",
      "üìà Step  9250 | Loss: 2.5874 | Avg: 1.9699 | LR: 7.5e-07\n",
      "üìà Step  9275 | Loss: 1.1861 | Avg: 1.9563 | LR: 7.4e-07\n",
      "üìà Step  9300 | Loss: 1.8442 | Avg: 1.9735 | LR: 7.3e-07\n",
      "üìà Step  9325 | Loss: 1.0314 | Avg: 2.1887 | LR: 7.2e-07\n",
      "üìà Step  9350 | Loss: 2.3054 | Avg: 1.9803 | LR: 7.0e-07\n",
      "üìà Step  9375 | Loss: 2.3597 | Avg: 2.2301 | LR: 6.9e-07\n",
      "üìà Step  9400 | Loss: 1.0695 | Avg: 1.9085 | LR: 6.8e-07\n",
      "üìà Step  9425 | Loss: 1.7034 | Avg: 1.9605 | LR: 6.7e-07\n",
      "üìà Step  9450 | Loss: 1.6937 | Avg: 1.8643 | LR: 6.6e-07\n",
      "üìà Step  9475 | Loss: 1.8065 | Avg: 1.7837 | LR: 6.5e-07\n",
      "üìà Step  9500 | Loss: 2.7587 | Avg: 2.0525 | LR: 6.4e-07\n",
      "üìà Step  9525 | Loss: 2.9503 | Avg: 2.0456 | LR: 6.3e-07\n",
      "üìà Step  9550 | Loss: 2.3811 | Avg: 1.9559 | LR: 6.1e-07\n",
      "üìà Step  9575 | Loss: 1.8511 | Avg: 2.1195 | LR: 6.0e-07\n",
      "üìà Step  9600 | Loss: 1.9289 | Avg: 1.8512 | LR: 5.9e-07\n",
      "üìà Step  9625 | Loss: 1.1997 | Avg: 1.8841 | LR: 5.8e-07\n",
      "üìà Step  9650 | Loss: 2.0047 | Avg: 2.1202 | LR: 5.7e-07\n",
      "üìà Step  9675 | Loss: 2.2739 | Avg: 1.8380 | LR: 5.6e-07\n",
      "üìà Step  9700 | Loss: 1.5101 | Avg: 1.8275 | LR: 5.5e-07\n",
      "üìà Step  9725 | Loss: 1.8606 | Avg: 1.8701 | LR: 5.4e-07\n",
      "üìà Step  9750 | Loss: 2.1657 | Avg: 2.1630 | LR: 5.3e-07\n",
      "üìà Step  9775 | Loss: 2.1788 | Avg: 1.9832 | LR: 5.2e-07\n",
      "üìà Step  9800 | Loss: 1.8901 | Avg: 1.9106 | LR: 5.1e-07\n",
      "üìà Step  9825 | Loss: 2.8702 | Avg: 1.9772 | LR: 5.0e-07\n",
      "üìà Step  9850 | Loss: 1.5949 | Avg: 2.0512 | LR: 4.9e-07\n",
      "üìà Step  9875 | Loss: 2.6749 | Avg: 1.8474 | LR: 4.8e-07\n",
      "üìà Step  9900 | Loss: 1.3977 | Avg: 2.0218 | LR: 4.7e-07\n",
      "üìà Step  9925 | Loss: 2.7416 | Avg: 1.8994 | LR: 4.6e-07\n",
      "üìà Step  9950 | Loss: 1.4763 | Avg: 2.0093 | LR: 4.5e-07\n",
      "üìà Step  9975 | Loss: 1.4436 | Avg: 2.0475 | LR: 4.4e-07\n",
      "üìà Step 10000 | Loss: 2.9551 | Avg: 2.2050 | LR: 4.3e-07\n",
      "üìà Step 10025 | Loss: 1.6174 | Avg: 1.9736 | LR: 4.2e-07\n",
      "üìà Step 10050 | Loss: 1.6103 | Avg: 2.1156 | LR: 4.1e-07\n",
      "üìà Step 10075 | Loss: 2.1883 | Avg: 1.9612 | LR: 4.0e-07\n",
      "üìà Step 10100 | Loss: 1.8680 | Avg: 1.9415 | LR: 3.9e-07\n",
      "üìà Step 10125 | Loss: 2.5801 | Avg: 2.2240 | LR: 3.8e-07\n",
      "üìà Step 10150 | Loss: 2.1133 | Avg: 2.0518 | LR: 3.7e-07\n",
      "üìà Step 10175 | Loss: 1.5818 | Avg: 1.9268 | LR: 3.6e-07\n",
      "üìà Step 10200 | Loss: 2.8739 | Avg: 2.0875 | LR: 3.5e-07\n",
      "üìà Step 10225 | Loss: 1.7593 | Avg: 1.8536 | LR: 3.4e-07\n",
      "üìà Step 10250 | Loss: 1.8767 | Avg: 2.1382 | LR: 3.3e-07\n",
      "üìà Step 10275 | Loss: 2.0164 | Avg: 1.8959 | LR: 3.2e-07\n",
      "üíæ Stable checkpoint saved at 45% of epoch 2\n",
      "üìà Step 10300 | Loss: 0.8551 | Avg: 1.9299 | LR: 3.2e-07\n",
      "üìà Step 10325 | Loss: 1.2416 | Avg: 1.7355 | LR: 3.1e-07\n",
      "üìà Step 10350 | Loss: 1.5995 | Avg: 2.0166 | LR: 3.0e-07\n",
      "üìà Step 10375 | Loss: 1.8606 | Avg: 2.1115 | LR: 2.9e-07\n",
      "üìà Step 10400 | Loss: 1.7929 | Avg: 1.9273 | LR: 2.8e-07\n",
      "üìà Step 10425 | Loss: 2.6172 | Avg: 2.2023 | LR: 2.7e-07\n",
      "üìà Step 10450 | Loss: 3.0009 | Avg: 2.1794 | LR: 2.6e-07\n",
      "üìà Step 10475 | Loss: 1.3457 | Avg: 1.9360 | LR: 2.6e-07\n",
      "üìà Step 10500 | Loss: 1.0776 | Avg: 1.9997 | LR: 2.5e-07\n",
      "üìà Step 10525 | Loss: 0.9123 | Avg: 1.8807 | LR: 2.4e-07\n",
      "üìà Step 10550 | Loss: 2.0980 | Avg: 2.0756 | LR: 2.3e-07\n",
      "üìà Step 10575 | Loss: 1.1193 | Avg: 1.8381 | LR: 2.3e-07\n",
      "üìà Step 10600 | Loss: 2.2507 | Avg: 2.0713 | LR: 2.2e-07\n",
      "üìà Step 10625 | Loss: 1.5821 | Avg: 1.7774 | LR: 2.1e-07\n",
      "üìà Step 10650 | Loss: 2.8442 | Avg: 2.1521 | LR: 2.0e-07\n",
      "üìà Step 10675 | Loss: 1.4781 | Avg: 1.9646 | LR: 2.0e-07\n",
      "üìà Step 10700 | Loss: 2.4172 | Avg: 2.1525 | LR: 1.9e-07\n",
      "üìà Step 10725 | Loss: 2.1174 | Avg: 1.9334 | LR: 1.8e-07\n",
      "üìà Step 10750 | Loss: 2.2454 | Avg: 1.8642 | LR: 1.8e-07\n",
      "üìà Step 10775 | Loss: 2.0339 | Avg: 1.8595 | LR: 1.7e-07\n",
      "üìà Step 10800 | Loss: 2.7851 | Avg: 1.8748 | LR: 1.6e-07\n",
      "üìà Step 10825 | Loss: 1.3293 | Avg: 1.9057 | LR: 1.6e-07\n",
      "üìà Step 10850 | Loss: 2.7768 | Avg: 1.9097 | LR: 1.5e-07\n",
      "üìà Step 10875 | Loss: 1.7170 | Avg: 2.0401 | LR: 1.4e-07\n",
      "üìà Step 10900 | Loss: 2.9841 | Avg: 2.1154 | LR: 1.4e-07\n",
      "üìà Step 10925 | Loss: 2.5793 | Avg: 2.0093 | LR: 1.3e-07\n",
      "üìà Step 10950 | Loss: 1.9303 | Avg: 1.9202 | LR: 1.3e-07\n",
      "üìà Step 10975 | Loss: 1.6828 | Avg: 2.0450 | LR: 1.2e-07\n",
      "üìà Step 11000 | Loss: 1.8969 | Avg: 1.9744 | LR: 1.1e-07\n",
      "üìà Step 11025 | Loss: 1.5207 | Avg: 1.7737 | LR: 1.1e-07\n",
      "üìà Step 11050 | Loss: 1.1464 | Avg: 1.8981 | LR: 1.0e-07\n",
      "üìà Step 11075 | Loss: 1.7215 | Avg: 1.9857 | LR: 9.8e-08\n",
      "üìà Step 11100 | Loss: 1.9290 | Avg: 2.0341 | LR: 9.3e-08\n",
      "üìà Step 11125 | Loss: 1.8766 | Avg: 1.9103 | LR: 8.8e-08\n",
      "üìà Step 11150 | Loss: 1.2757 | Avg: 1.9522 | LR: 8.3e-08\n",
      "üìà Step 11175 | Loss: 1.2496 | Avg: 2.0863 | LR: 7.8e-08\n",
      "üìà Step 11200 | Loss: 1.8974 | Avg: 2.1985 | LR: 7.4e-08\n",
      "üìà Step 11225 | Loss: 1.6170 | Avg: 2.1179 | LR: 6.9e-08\n",
      "üìà Step 11250 | Loss: 2.1699 | Avg: 1.9935 | LR: 6.5e-08\n",
      "üìà Step 11275 | Loss: 2.2749 | Avg: 2.1875 | LR: 6.1e-08\n",
      "üìà Step 11300 | Loss: 1.3849 | Avg: 2.0978 | LR: 5.7e-08\n",
      "üìà Step 11325 | Loss: 2.4188 | Avg: 2.1520 | LR: 5.3e-08\n",
      "üìà Step 11350 | Loss: 2.1456 | Avg: 1.9553 | LR: 4.9e-08\n",
      "üìà Step 11375 | Loss: 1.9693 | Avg: 2.0722 | LR: 4.6e-08\n",
      "üìà Step 11400 | Loss: 2.4147 | Avg: 2.1836 | LR: 4.2e-08\n",
      "üìà Step 11425 | Loss: 3.6316 | Avg: 1.9874 | LR: 3.9e-08\n",
      "üìà Step 11450 | Loss: 1.7655 | Avg: 2.0768 | LR: 3.5e-08\n",
      "üìà Step 11475 | Loss: 1.6015 | Avg: 1.8391 | LR: 3.2e-08\n",
      "üìà Step 11500 | Loss: 1.5598 | Avg: 1.9765 | LR: 2.9e-08\n",
      "üìà Step 11525 | Loss: 1.1300 | Avg: 1.7853 | LR: 2.7e-08\n",
      "üìà Step 11550 | Loss: 2.0273 | Avg: 1.9732 | LR: 2.4e-08\n",
      "üìà Step 11575 | Loss: 1.4696 | Avg: 1.9322 | LR: 2.1e-08\n",
      "üìà Step 11600 | Loss: 1.7874 | Avg: 1.9052 | LR: 1.9e-08\n",
      "üìà Step 11625 | Loss: 2.4786 | Avg: 2.0516 | LR: 1.7e-08\n",
      "üìà Step 11650 | Loss: 1.4557 | Avg: 1.9658 | LR: 1.5e-08\n",
      "üìà Step 11675 | Loss: 2.6090 | Avg: 1.9062 | LR: 1.3e-08\n",
      "üìà Step 11700 | Loss: 2.2255 | Avg: 1.8908 | LR: 1.1e-08\n",
      "üìà Step 11725 | Loss: 1.6306 | Avg: 1.8014 | LR: 9.2e-09\n",
      "üìà Step 11750 | Loss: 3.1478 | Avg: 1.8742 | LR: 7.6e-09\n",
      "üìà Step 11775 | Loss: 1.5295 | Avg: 2.2095 | LR: 6.2e-09\n",
      "üìà Step 11800 | Loss: 2.0217 | Avg: 2.0417 | LR: 5.0e-09\n",
      "üìà Step 11825 | Loss: 1.7290 | Avg: 1.9232 | LR: 3.9e-09\n",
      "üìà Step 11850 | Loss: 2.2562 | Avg: 2.0122 | LR: 2.9e-09\n",
      "üìà Step 11875 | Loss: 1.6135 | Avg: 2.0269 | LR: 2.0e-09\n",
      "üìà Step 11900 | Loss: 1.5148 | Avg: 1.9910 | LR: 1.4e-09\n",
      "üìà Step 11925 | Loss: 1.6526 | Avg: 2.0860 | LR: 8.1e-10\n",
      "üìà Step 11950 | Loss: 1.4372 | Avg: 2.0680 | LR: 4.0e-10\n",
      "üìà Step 11975 | Loss: 1.6339 | Avg: 1.9534 | LR: 1.3e-10\n",
      "üìà Step 12000 | Loss: 1.4439 | Avg: 1.9761 | LR: 9.2e-12\n",
      "üìà Step 12025 | Loss: 1.9962 | Avg: 2.0258 | LR: 2.9e-11\n",
      "üìà Step 12050 | Loss: 2.3464 | Avg: 2.0047 | LR: 1.9e-10\n",
      "üìà Step 12075 | Loss: 2.0628 | Avg: 2.1109 | LR: 5.0e-10\n",
      "üìà Step 12100 | Loss: 1.1712 | Avg: 2.1860 | LR: 9.5e-10\n",
      "üìà Step 12125 | Loss: 2.7216 | Avg: 1.8889 | LR: 1.5e-09\n",
      "üìà Step 12150 | Loss: 1.5355 | Avg: 1.9762 | LR: 2.3e-09\n",
      "üìà Step 12175 | Loss: 2.0086 | Avg: 1.9569 | LR: 3.1e-09\n",
      "üìà Step 12200 | Loss: 1.5661 | Avg: 1.8702 | LR: 4.2e-09\n",
      "üìà Step 12225 | Loss: 1.7797 | Avg: 2.0679 | LR: 5.3e-09\n",
      "üìà Step 12250 | Loss: 1.9391 | Avg: 2.0268 | LR: 6.6e-09\n",
      "üìà Step 12275 | Loss: 2.4666 | Avg: 1.9428 | LR: 8.1e-09\n",
      "üìà Step 12300 | Loss: 1.4613 | Avg: 1.9622 | LR: 9.7e-09\n",
      "üìà Step 12325 | Loss: 2.1412 | Avg: 2.1655 | LR: 1.1e-08\n",
      "üìà Step 12350 | Loss: 2.4566 | Avg: 1.9157 | LR: 1.3e-08\n",
      "üìà Step 12375 | Loss: 2.8496 | Avg: 1.9955 | LR: 1.5e-08\n",
      "üìà Step 12400 | Loss: 2.3650 | Avg: 2.0725 | LR: 1.7e-08\n",
      "üìà Step 12425 | Loss: 1.1731 | Avg: 1.7997 | LR: 2.0e-08\n",
      "üìà Step 12450 | Loss: 1.9384 | Avg: 2.0095 | LR: 2.2e-08\n",
      "üìà Step 12475 | Loss: 2.3538 | Avg: 2.0571 | LR: 2.5e-08\n",
      "üìà Step 12500 | Loss: 2.8010 | Avg: 2.0282 | LR: 2.7e-08\n",
      "üìà Step 12525 | Loss: 2.3639 | Avg: 1.9190 | LR: 3.0e-08\n",
      "üìà Step 12550 | Loss: 1.8440 | Avg: 1.9271 | LR: 3.3e-08\n",
      "üìà Step 12575 | Loss: 2.5221 | Avg: 2.3188 | LR: 3.6e-08\n",
      "üìà Step 12600 | Loss: 2.5662 | Avg: 2.1144 | LR: 4.0e-08\n",
      "üìà Step 12625 | Loss: 1.7299 | Avg: 1.9087 | LR: 4.3e-08\n",
      "üìà Step 12650 | Loss: 2.2257 | Avg: 2.0094 | LR: 4.7e-08\n",
      "üìà Step 12675 | Loss: 1.2534 | Avg: 1.9937 | LR: 5.0e-08\n",
      "üìà Step 12700 | Loss: 1.9633 | Avg: 2.0854 | LR: 5.4e-08\n",
      "üìà Step 12725 | Loss: 2.0513 | Avg: 2.1888 | LR: 5.8e-08\n",
      "üìà Step 12750 | Loss: 2.9008 | Avg: 1.7544 | LR: 6.2e-08\n",
      "üìà Step 12775 | Loss: 2.0384 | Avg: 1.9147 | LR: 6.6e-08\n",
      "üìà Step 12800 | Loss: 1.8786 | Avg: 1.9632 | LR: 7.1e-08\n",
      "üìà Step 12825 | Loss: 1.4544 | Avg: 1.9645 | LR: 7.5e-08\n",
      "üìà Step 12850 | Loss: 1.9250 | Avg: 2.0413 | LR: 8.0e-08\n",
      "üìà Step 12875 | Loss: 2.3372 | Avg: 1.8135 | LR: 8.4e-08\n",
      "üìà Step 12900 | Loss: 2.0378 | Avg: 1.9741 | LR: 8.9e-08\n",
      "üìà Step 12925 | Loss: 2.9168 | Avg: 2.0561 | LR: 9.4e-08\n",
      "üìà Step 12950 | Loss: 1.9152 | Avg: 1.9802 | LR: 9.9e-08\n",
      "üìà Step 12975 | Loss: 3.1334 | Avg: 1.9775 | LR: 1.0e-07\n",
      "üìà Step 13000 | Loss: 1.5791 | Avg: 1.7972 | LR: 1.1e-07\n",
      "üìà Step 13025 | Loss: 1.7666 | Avg: 1.9912 | LR: 1.2e-07\n",
      "üìà Step 13050 | Loss: 1.7623 | Avg: 2.0622 | LR: 1.2e-07\n",
      "üìà Step 13075 | Loss: 1.5597 | Avg: 1.8515 | LR: 1.3e-07\n",
      "üìà Step 13100 | Loss: 0.8997 | Avg: 1.8504 | LR: 1.3e-07\n",
      "üìà Step 13125 | Loss: 1.5548 | Avg: 1.9573 | LR: 1.4e-07\n",
      "üìà Step 13150 | Loss: 2.5837 | Avg: 1.9927 | LR: 1.5e-07\n",
      "üìà Step 13175 | Loss: 1.9571 | Avg: 1.9226 | LR: 1.5e-07\n",
      "üìà Step 13200 | Loss: 2.6878 | Avg: 2.0914 | LR: 1.6e-07\n",
      "üìà Step 13225 | Loss: 1.9782 | Avg: 1.8711 | LR: 1.6e-07\n",
      "üìà Step 13250 | Loss: 2.1607 | Avg: 2.0398 | LR: 1.7e-07\n",
      "üìà Step 13275 | Loss: 2.3583 | Avg: 2.2703 | LR: 1.8e-07\n",
      "üìà Step 13300 | Loss: 2.6115 | Avg: 1.7870 | LR: 1.8e-07\n",
      "üìà Step 13325 | Loss: 1.8093 | Avg: 1.8907 | LR: 1.9e-07\n",
      "üìà Step 13350 | Loss: 1.7721 | Avg: 1.9898 | LR: 2.0e-07\n",
      "üìà Step 13375 | Loss: 1.7527 | Avg: 1.9482 | LR: 2.1e-07\n",
      "üìà Step 13400 | Loss: 2.6621 | Avg: 1.9318 | LR: 2.1e-07\n",
      "üìà Step 13425 | Loss: 2.0553 | Avg: 1.9141 | LR: 2.2e-07\n",
      "üìà Step 13450 | Loss: 1.9665 | Avg: 1.9871 | LR: 2.3e-07\n",
      "üìà Step 13475 | Loss: 3.0301 | Avg: 2.0714 | LR: 2.4e-07\n",
      "üìà Step 13500 | Loss: 1.6346 | Avg: 2.0002 | LR: 2.4e-07\n",
      "üìà Step 13525 | Loss: 0.8431 | Avg: 2.1679 | LR: 2.5e-07\n",
      "üìà Step 13550 | Loss: 0.8191 | Avg: 2.0374 | LR: 2.6e-07\n",
      "üìà Step 13575 | Loss: 1.6632 | Avg: 1.8648 | LR: 2.7e-07\n",
      "üìà Step 13600 | Loss: 1.9482 | Avg: 1.9118 | LR: 2.8e-07\n",
      "üìà Step 13625 | Loss: 2.0712 | Avg: 2.0160 | LR: 2.8e-07\n",
      "üìà Step 13650 | Loss: 1.9776 | Avg: 2.2176 | LR: 2.9e-07\n",
      "üìà Step 13675 | Loss: 2.1674 | Avg: 1.8872 | LR: 3.0e-07\n",
      "üìà Step 13700 | Loss: 1.4222 | Avg: 2.0517 | LR: 3.1e-07\n",
      "üíæ Stable checkpoint saved at 60% of epoch 2\n",
      "üìà Step 13725 | Loss: 2.4735 | Avg: 1.8239 | LR: 3.2e-07\n",
      "üìà Step 13750 | Loss: 1.5301 | Avg: 1.8024 | LR: 3.3e-07\n",
      "üìà Step 13775 | Loss: 3.2282 | Avg: 2.0804 | LR: 3.4e-07\n",
      "üìà Step 13800 | Loss: 1.3803 | Avg: 2.0271 | LR: 3.4e-07\n",
      "üìà Step 13825 | Loss: 1.9608 | Avg: 2.0287 | LR: 3.5e-07\n",
      "üìà Step 13850 | Loss: 3.0593 | Avg: 1.8823 | LR: 3.6e-07\n",
      "üìà Step 13875 | Loss: 1.0851 | Avg: 2.2401 | LR: 3.7e-07\n",
      "üìà Step 13900 | Loss: 3.3992 | Avg: 2.0059 | LR: 3.8e-07\n",
      "üìà Step 13925 | Loss: 2.2743 | Avg: 1.9393 | LR: 3.9e-07\n",
      "üìà Step 13950 | Loss: 2.5820 | Avg: 2.0037 | LR: 4.0e-07\n",
      "üìà Step 13975 | Loss: 1.9561 | Avg: 1.8866 | LR: 4.1e-07\n",
      "üìà Step 14000 | Loss: 1.3221 | Avg: 2.1879 | LR: 4.2e-07\n",
      "üìà Step 14025 | Loss: 1.5297 | Avg: 1.8946 | LR: 4.3e-07\n",
      "üìà Step 14050 | Loss: 1.8102 | Avg: 1.9557 | LR: 4.4e-07\n",
      "üìà Step 14075 | Loss: 2.1772 | Avg: 2.0188 | LR: 4.5e-07\n",
      "üìà Step 14100 | Loss: 2.6453 | Avg: 1.9426 | LR: 4.6e-07\n",
      "üìà Step 14125 | Loss: 2.1754 | Avg: 2.0490 | LR: 4.7e-07\n",
      "üìà Step 14150 | Loss: 2.3331 | Avg: 2.1037 | LR: 4.8e-07\n",
      "üìà Step 14175 | Loss: 1.8792 | Avg: 1.7923 | LR: 4.9e-07\n",
      "üìà Step 14200 | Loss: 2.3168 | Avg: 2.0901 | LR: 5.0e-07\n",
      "üìà Step 14225 | Loss: 1.9932 | Avg: 1.9209 | LR: 5.1e-07\n",
      "üìà Step 14250 | Loss: 1.7393 | Avg: 2.0707 | LR: 5.2e-07\n",
      "üìà Step 14275 | Loss: 2.0393 | Avg: 2.0244 | LR: 5.3e-07\n",
      "üìà Step 14300 | Loss: 1.5904 | Avg: 2.0659 | LR: 5.4e-07\n",
      "üìà Step 14325 | Loss: 0.8470 | Avg: 2.1723 | LR: 5.5e-07\n",
      "üìà Step 14350 | Loss: 2.9443 | Avg: 2.0797 | LR: 5.6e-07\n",
      "üìà Step 14375 | Loss: 1.9068 | Avg: 1.8677 | LR: 5.7e-07\n",
      "üìà Step 14400 | Loss: 2.0278 | Avg: 1.8464 | LR: 5.8e-07\n",
      "üìà Step 14425 | Loss: 0.7890 | Avg: 2.0292 | LR: 6.0e-07\n",
      "üìà Step 14450 | Loss: 1.8329 | Avg: 2.0541 | LR: 6.1e-07\n",
      "üìà Step 14475 | Loss: 2.0658 | Avg: 2.0151 | LR: 6.2e-07\n",
      "üìà Step 14500 | Loss: 1.3284 | Avg: 1.9279 | LR: 6.3e-07\n",
      "üìà Step 14525 | Loss: 2.5217 | Avg: 2.0639 | LR: 6.4e-07\n",
      "üìà Step 14550 | Loss: 1.2711 | Avg: 2.0157 | LR: 6.5e-07\n",
      "üìà Step 14575 | Loss: 2.2522 | Avg: 2.0832 | LR: 6.6e-07\n",
      "üìà Step 14600 | Loss: 2.4746 | Avg: 1.9889 | LR: 6.7e-07\n",
      "üìà Step 14625 | Loss: 1.8388 | Avg: 1.8826 | LR: 6.8e-07\n",
      "üìà Step 14650 | Loss: 1.8006 | Avg: 1.9297 | LR: 7.0e-07\n",
      "üìà Step 14675 | Loss: 2.1254 | Avg: 1.8345 | LR: 7.1e-07\n",
      "üìà Step 14700 | Loss: 1.3816 | Avg: 1.9828 | LR: 7.2e-07\n",
      "üìà Step 14725 | Loss: 2.1041 | Avg: 2.0890 | LR: 7.3e-07\n",
      "üìà Step 14750 | Loss: 3.3765 | Avg: 2.1668 | LR: 7.4e-07\n",
      "üìà Step 14775 | Loss: 2.0350 | Avg: 1.9292 | LR: 7.5e-07\n",
      "üìà Step 14800 | Loss: 1.2585 | Avg: 1.9631 | LR: 7.7e-07\n",
      "üìà Step 14825 | Loss: 2.3437 | Avg: 1.7462 | LR: 7.8e-07\n",
      "üìà Step 14850 | Loss: 1.7792 | Avg: 1.7940 | LR: 7.9e-07\n",
      "üìà Step 14875 | Loss: 2.7550 | Avg: 2.1917 | LR: 8.0e-07\n",
      "üìà Step 14900 | Loss: 2.5359 | Avg: 2.0976 | LR: 8.1e-07\n",
      "üìà Step 14925 | Loss: 1.7117 | Avg: 1.7347 | LR: 8.2e-07\n",
      "üìà Step 14950 | Loss: 1.2651 | Avg: 1.9563 | LR: 8.4e-07\n",
      "üìà Step 14975 | Loss: 2.2559 | Avg: 1.8874 | LR: 8.5e-07\n",
      "üìà Step 15000 | Loss: 1.8614 | Avg: 2.1042 | LR: 8.6e-07\n",
      "üìà Step 15025 | Loss: 1.3150 | Avg: 2.1264 | LR: 8.7e-07\n",
      "üìà Step 15050 | Loss: 1.5894 | Avg: 1.8567 | LR: 8.8e-07\n",
      "üìà Step 15075 | Loss: 1.5010 | Avg: 1.8022 | LR: 8.9e-07\n",
      "üìà Step 15100 | Loss: 1.3633 | Avg: 1.8458 | LR: 9.1e-07\n",
      "üìà Step 15125 | Loss: 2.7617 | Avg: 1.9902 | LR: 9.2e-07\n",
      "üìà Step 15150 | Loss: 2.2956 | Avg: 1.8815 | LR: 9.3e-07\n",
      "üìà Step 15175 | Loss: 2.8844 | Avg: 2.0948 | LR: 9.4e-07\n",
      "üìà Step 15200 | Loss: 2.1446 | Avg: 1.7661 | LR: 9.5e-07\n",
      "üìà Step 15225 | Loss: 1.6190 | Avg: 1.8493 | LR: 9.7e-07\n",
      "üìà Step 15250 | Loss: 2.1237 | Avg: 2.0220 | LR: 9.8e-07\n",
      "üìà Step 15275 | Loss: 1.9950 | Avg: 2.0328 | LR: 9.9e-07\n",
      "üìà Step 15300 | Loss: 2.1779 | Avg: 2.0030 | LR: 1.0e-06\n",
      "üìà Step 15325 | Loss: 1.9179 | Avg: 1.9910 | LR: 1.0e-06\n",
      "üìà Step 15350 | Loss: 2.3237 | Avg: 1.8880 | LR: 1.0e-06\n",
      "üìà Step 15375 | Loss: 1.6844 | Avg: 1.9088 | LR: 1.0e-06\n",
      "üìà Step 15400 | Loss: 1.2724 | Avg: 1.8812 | LR: 1.0e-06\n",
      "üìà Step 15425 | Loss: 3.3284 | Avg: 2.0582 | LR: 1.1e-06\n",
      "üìà Step 15450 | Loss: 2.4788 | Avg: 2.1656 | LR: 1.1e-06\n",
      "üìà Step 15475 | Loss: 1.9093 | Avg: 1.9780 | LR: 1.1e-06\n",
      "üìà Step 15500 | Loss: 2.2350 | Avg: 2.1773 | LR: 1.1e-06\n",
      "üìà Step 15525 | Loss: 2.5309 | Avg: 1.9102 | LR: 1.1e-06\n",
      "üìà Step 15550 | Loss: 1.1980 | Avg: 1.8451 | LR: 1.1e-06\n",
      "üìà Step 15575 | Loss: 1.8021 | Avg: 2.0167 | LR: 1.1e-06\n",
      "üìà Step 15600 | Loss: 2.6306 | Avg: 1.8173 | LR: 1.1e-06\n",
      "üìà Step 15625 | Loss: 2.8299 | Avg: 1.7852 | LR: 1.2e-06\n",
      "üìà Step 15650 | Loss: 1.8345 | Avg: 1.7889 | LR: 1.2e-06\n",
      "üìà Step 15675 | Loss: 1.9553 | Avg: 1.9573 | LR: 1.2e-06\n",
      "üìà Step 15700 | Loss: 1.1701 | Avg: 1.8728 | LR: 1.2e-06\n",
      "üìà Step 15725 | Loss: 2.4973 | Avg: 1.8154 | LR: 1.2e-06\n",
      "üìà Step 15750 | Loss: 1.9905 | Avg: 1.8654 | LR: 1.2e-06\n",
      "üìà Step 15775 | Loss: 1.0548 | Avg: 1.8892 | LR: 1.2e-06\n",
      "üìà Step 15800 | Loss: 2.0342 | Avg: 2.0960 | LR: 1.2e-06\n",
      "üìà Step 15825 | Loss: 1.7745 | Avg: 1.9505 | LR: 1.3e-06\n",
      "üìà Step 15850 | Loss: 1.7627 | Avg: 1.8274 | LR: 1.3e-06\n",
      "üìà Step 15875 | Loss: 3.0542 | Avg: 2.0780 | LR: 1.3e-06\n",
      "üìà Step 15900 | Loss: 2.7131 | Avg: 2.0838 | LR: 1.3e-06\n",
      "üìà Step 15925 | Loss: 2.4076 | Avg: 1.9387 | LR: 1.3e-06\n",
      "üìà Step 15950 | Loss: 1.9298 | Avg: 1.9054 | LR: 1.3e-06\n",
      "üìà Step 15975 | Loss: 1.8371 | Avg: 1.7027 | LR: 1.3e-06\n",
      "üìà Step 16000 | Loss: 1.2288 | Avg: 1.8346 | LR: 1.3e-06\n",
      "üìà Step 16025 | Loss: 2.3834 | Avg: 1.8897 | LR: 1.3e-06\n",
      "üìà Step 16050 | Loss: 1.1295 | Avg: 1.9900 | LR: 1.4e-06\n",
      "üìà Step 16075 | Loss: 1.3016 | Avg: 1.9748 | LR: 1.4e-06\n",
      "üìà Step 16100 | Loss: 2.5090 | Avg: 1.9481 | LR: 1.4e-06\n",
      "üìà Step 16125 | Loss: 3.3120 | Avg: 1.9124 | LR: 1.4e-06\n",
      "üìà Step 16150 | Loss: 1.7199 | Avg: 1.7701 | LR: 1.4e-06\n",
      "üìà Step 16175 | Loss: 2.5724 | Avg: 1.8102 | LR: 1.4e-06\n",
      "üìà Step 16200 | Loss: 0.8249 | Avg: 1.9569 | LR: 1.4e-06\n",
      "üìà Step 16225 | Loss: 1.5136 | Avg: 1.9291 | LR: 1.4e-06\n",
      "üìà Step 16250 | Loss: 1.0940 | Avg: 2.0380 | LR: 1.4e-06\n",
      "üìà Step 16275 | Loss: 2.4354 | Avg: 1.8648 | LR: 1.5e-06\n",
      "üìà Step 16300 | Loss: 2.3245 | Avg: 1.7970 | LR: 1.5e-06\n",
      "üìà Step 16325 | Loss: 1.7006 | Avg: 2.1830 | LR: 1.5e-06\n",
      "üìà Step 16350 | Loss: 1.5864 | Avg: 1.9823 | LR: 1.5e-06\n",
      "üìà Step 16375 | Loss: 2.3121 | Avg: 1.9958 | LR: 1.5e-06\n",
      "üìà Step 16400 | Loss: 2.8553 | Avg: 1.9053 | LR: 1.5e-06\n",
      "üìà Step 16425 | Loss: 2.1312 | Avg: 2.0100 | LR: 1.5e-06\n",
      "üìà Step 16450 | Loss: 1.7550 | Avg: 2.0688 | LR: 1.5e-06\n",
      "üìà Step 16475 | Loss: 1.3580 | Avg: 1.8950 | LR: 1.5e-06\n",
      "üìà Step 16500 | Loss: 2.2590 | Avg: 2.1224 | LR: 1.5e-06\n",
      "üìà Step 16525 | Loss: 1.5791 | Avg: 1.7387 | LR: 1.6e-06\n",
      "üìà Step 16550 | Loss: 1.4040 | Avg: 1.8728 | LR: 1.6e-06\n",
      "üìà Step 16575 | Loss: 1.9136 | Avg: 1.7974 | LR: 1.6e-06\n",
      "üìà Step 16600 | Loss: 2.5574 | Avg: 1.8364 | LR: 1.6e-06\n",
      "üìà Step 16625 | Loss: 1.6642 | Avg: 1.9579 | LR: 1.6e-06\n",
      "üìà Step 16650 | Loss: 1.3954 | Avg: 1.8532 | LR: 1.6e-06\n",
      "üìà Step 16675 | Loss: 1.2680 | Avg: 1.8260 | LR: 1.6e-06\n",
      "üìà Step 16700 | Loss: 1.6792 | Avg: 1.8314 | LR: 1.6e-06\n",
      "üìà Step 16725 | Loss: 2.2108 | Avg: 1.9682 | LR: 1.6e-06\n",
      "üìà Step 16750 | Loss: 2.6715 | Avg: 1.8684 | LR: 1.6e-06\n",
      "üìà Step 16775 | Loss: 1.8558 | Avg: 2.1972 | LR: 1.6e-06\n",
      "üìà Step 16800 | Loss: 1.0384 | Avg: 1.7408 | LR: 1.7e-06\n",
      "üìà Step 16825 | Loss: 1.0366 | Avg: 2.0218 | LR: 1.7e-06\n",
      "üìà Step 16850 | Loss: 2.2431 | Avg: 1.8788 | LR: 1.7e-06\n",
      "üìà Step 16875 | Loss: 1.5281 | Avg: 1.6865 | LR: 1.7e-06\n",
      "üìà Step 16900 | Loss: 1.9940 | Avg: 1.9896 | LR: 1.7e-06\n",
      "üìà Step 16925 | Loss: 1.9856 | Avg: 1.9059 | LR: 1.7e-06\n",
      "üìà Step 16950 | Loss: 1.2243 | Avg: 1.9234 | LR: 1.7e-06\n",
      "üìà Step 16975 | Loss: 2.1019 | Avg: 1.8646 | LR: 1.7e-06\n",
      "üìà Step 17000 | Loss: 2.5351 | Avg: 1.9955 | LR: 1.7e-06\n",
      "üìà Step 17025 | Loss: 3.1429 | Avg: 1.9576 | LR: 1.7e-06\n",
      "üìà Step 17050 | Loss: 1.5495 | Avg: 1.9552 | LR: 1.7e-06\n",
      "üìà Step 17075 | Loss: 1.7248 | Avg: 1.7054 | LR: 1.8e-06\n",
      "üìà Step 17100 | Loss: 1.8294 | Avg: 1.9599 | LR: 1.8e-06\n",
      "üìà Step 17125 | Loss: 2.3221 | Avg: 1.8695 | LR: 1.8e-06\n",
      "üíæ Stable checkpoint saved at 75% of epoch 2\n",
      "üìà Step 17150 | Loss: 2.3544 | Avg: 2.1441 | LR: 1.8e-06\n",
      "üìà Step 17175 | Loss: 2.0324 | Avg: 1.9926 | LR: 1.8e-06\n",
      "üìà Step 17200 | Loss: 2.4692 | Avg: 1.9998 | LR: 1.8e-06\n",
      "üìà Step 17225 | Loss: 3.0034 | Avg: 1.9516 | LR: 1.8e-06\n",
      "üìà Step 17250 | Loss: 2.1276 | Avg: 1.7815 | LR: 1.8e-06\n",
      "üìà Step 17275 | Loss: 1.8336 | Avg: 2.0660 | LR: 1.8e-06\n",
      "üìà Step 17300 | Loss: 2.2457 | Avg: 1.8338 | LR: 1.8e-06\n",
      "üìà Step 17325 | Loss: 3.1992 | Avg: 2.0511 | LR: 1.8e-06\n",
      "üìà Step 17350 | Loss: 2.5922 | Avg: 1.9542 | LR: 1.8e-06\n",
      "üìà Step 17375 | Loss: 1.9150 | Avg: 1.9623 | LR: 1.8e-06\n",
      "üìà Step 17400 | Loss: 2.3463 | Avg: 2.0998 | LR: 1.8e-06\n",
      "üìà Step 17425 | Loss: 1.4466 | Avg: 1.8704 | LR: 1.9e-06\n",
      "üìà Step 17450 | Loss: 1.8614 | Avg: 2.0838 | LR: 1.9e-06\n",
      "üìà Step 17475 | Loss: 2.2113 | Avg: 1.8231 | LR: 1.9e-06\n",
      "üìà Step 17500 | Loss: 2.6338 | Avg: 2.0405 | LR: 1.9e-06\n",
      "üìà Step 17525 | Loss: 1.5014 | Avg: 1.8968 | LR: 1.9e-06\n",
      "üìà Step 17550 | Loss: 2.5717 | Avg: 2.0055 | LR: 1.9e-06\n",
      "üìà Step 17575 | Loss: 2.1511 | Avg: 1.9782 | LR: 1.9e-06\n",
      "üìà Step 17600 | Loss: 2.8798 | Avg: 1.8631 | LR: 1.9e-06\n",
      "üìà Step 17625 | Loss: 2.1250 | Avg: 1.9496 | LR: 1.9e-06\n",
      "üìà Step 17650 | Loss: 1.7902 | Avg: 2.0242 | LR: 1.9e-06\n",
      "üìà Step 17675 | Loss: 1.9533 | Avg: 2.0154 | LR: 1.9e-06\n",
      "üìà Step 17700 | Loss: 2.6485 | Avg: 2.0529 | LR: 1.9e-06\n",
      "üìà Step 17725 | Loss: 2.9070 | Avg: 1.9735 | LR: 1.9e-06\n",
      "üìà Step 17750 | Loss: 2.4045 | Avg: 2.0167 | LR: 1.9e-06\n",
      "üìà Step 17775 | Loss: 1.5887 | Avg: 1.9542 | LR: 1.9e-06\n",
      "üìà Step 17800 | Loss: 2.0901 | Avg: 1.8574 | LR: 1.9e-06\n",
      "üìà Step 17825 | Loss: 1.8291 | Avg: 1.8743 | LR: 1.9e-06\n",
      "üìà Step 17850 | Loss: 1.7843 | Avg: 1.9942 | LR: 1.9e-06\n",
      "üìà Step 17875 | Loss: 2.9517 | Avg: 2.0024 | LR: 1.9e-06\n",
      "üìà Step 17900 | Loss: 1.6253 | Avg: 1.9097 | LR: 1.9e-06\n",
      "üìà Step 17925 | Loss: 2.2144 | Avg: 1.9675 | LR: 2.0e-06\n",
      "üìà Step 17950 | Loss: 2.3567 | Avg: 1.8566 | LR: 2.0e-06\n",
      "üìà Step 17975 | Loss: 2.0606 | Avg: 1.6997 | LR: 2.0e-06\n",
      "üìà Step 18000 | Loss: 1.3057 | Avg: 1.9504 | LR: 2.0e-06\n",
      "üìà Step 18025 | Loss: 3.0115 | Avg: 1.9381 | LR: 2.0e-06\n",
      "üìà Step 18050 | Loss: 2.8747 | Avg: 1.7787 | LR: 2.0e-06\n",
      "üìà Step 18075 | Loss: 2.7774 | Avg: 1.9991 | LR: 2.0e-06\n",
      "üìà Step 18100 | Loss: 1.4062 | Avg: 1.9381 | LR: 2.0e-06\n",
      "üìà Step 18125 | Loss: 1.1013 | Avg: 1.7084 | LR: 2.0e-06\n",
      "üìà Step 18150 | Loss: 1.6608 | Avg: 2.2025 | LR: 2.0e-06\n",
      "üìà Step 18175 | Loss: 1.8641 | Avg: 1.8734 | LR: 2.0e-06\n",
      "üìà Step 18200 | Loss: 1.9367 | Avg: 1.9417 | LR: 2.0e-06\n",
      "üìà Step 18225 | Loss: 1.0574 | Avg: 1.7802 | LR: 2.0e-06\n",
      "üìà Step 18250 | Loss: 2.3656 | Avg: 1.9074 | LR: 2.0e-06\n",
      "üìà Step 18275 | Loss: 2.0789 | Avg: 1.6471 | LR: 2.0e-06\n",
      "üìà Step 18300 | Loss: 1.6852 | Avg: 1.8694 | LR: 2.0e-06\n",
      "üìà Step 18325 | Loss: 1.9522 | Avg: 2.0355 | LR: 2.0e-06\n",
      "üìà Step 18350 | Loss: 1.7813 | Avg: 1.8851 | LR: 2.0e-06\n",
      "üìà Step 18375 | Loss: 0.9205 | Avg: 1.8169 | LR: 2.0e-06\n",
      "üìà Step 18400 | Loss: 2.5936 | Avg: 1.8836 | LR: 2.0e-06\n",
      "üìà Step 18425 | Loss: 1.3078 | Avg: 1.7334 | LR: 2.0e-06\n",
      "üìà Step 18450 | Loss: 2.3809 | Avg: 1.8410 | LR: 2.0e-06\n",
      "üìà Step 18475 | Loss: 2.3433 | Avg: 1.6534 | LR: 2.0e-06\n",
      "üìà Step 18500 | Loss: 2.5776 | Avg: 1.8688 | LR: 2.0e-06\n",
      "üìà Step 18525 | Loss: 2.5170 | Avg: 1.8227 | LR: 2.0e-06\n",
      "üìà Step 18550 | Loss: 1.4814 | Avg: 1.7258 | LR: 2.0e-06\n",
      "üìà Step 18575 | Loss: 2.0745 | Avg: 2.0146 | LR: 2.0e-06\n",
      "üìà Step 18600 | Loss: 2.0344 | Avg: 1.7782 | LR: 2.0e-06\n",
      "üìà Step 18625 | Loss: 3.0368 | Avg: 1.8399 | LR: 2.0e-06\n",
      "üìà Step 18650 | Loss: 2.8084 | Avg: 1.9935 | LR: 2.0e-06\n",
      "üìà Step 18675 | Loss: 1.0932 | Avg: 1.6439 | LR: 2.0e-06\n",
      "üìà Step 18700 | Loss: 1.4995 | Avg: 1.6967 | LR: 2.0e-06\n",
      "üìà Step 18725 | Loss: 2.3065 | Avg: 1.7630 | LR: 2.0e-06\n",
      "üìà Step 18750 | Loss: 1.2486 | Avg: 1.9045 | LR: 2.0e-06\n",
      "üìà Step 18775 | Loss: 1.8711 | Avg: 1.6817 | LR: 2.0e-06\n",
      "üìà Step 18800 | Loss: 2.5312 | Avg: 1.9216 | LR: 2.0e-06\n",
      "üìà Step 18825 | Loss: 2.6043 | Avg: 1.9811 | LR: 2.0e-06\n",
      "üìà Step 18850 | Loss: 2.6846 | Avg: 1.8644 | LR: 2.0e-06\n",
      "üìà Step 18875 | Loss: 1.7357 | Avg: 1.7590 | LR: 2.0e-06\n",
      "üìà Step 18900 | Loss: 1.6901 | Avg: 1.7950 | LR: 2.0e-06\n",
      "üìà Step 18925 | Loss: 1.7193 | Avg: 1.7755 | LR: 2.0e-06\n",
      "üìà Step 18950 | Loss: 2.0149 | Avg: 1.8628 | LR: 2.0e-06\n",
      "üìà Step 18975 | Loss: 2.0309 | Avg: 1.8427 | LR: 2.0e-06\n",
      "üìà Step 19000 | Loss: 1.9692 | Avg: 1.8312 | LR: 2.0e-06\n",
      "üìà Step 19025 | Loss: 2.4027 | Avg: 1.8891 | LR: 2.0e-06\n",
      "üìà Step 19050 | Loss: 2.0290 | Avg: 1.9461 | LR: 2.0e-06\n",
      "üìà Step 19075 | Loss: 1.8958 | Avg: 1.8226 | LR: 2.0e-06\n",
      "üìà Step 19100 | Loss: 1.9808 | Avg: 1.9473 | LR: 2.0e-06\n",
      "üìà Step 19125 | Loss: 1.4554 | Avg: 1.8691 | LR: 2.0e-06\n",
      "üìà Step 19150 | Loss: 2.5428 | Avg: 1.8829 | LR: 2.0e-06\n",
      "üìà Step 19175 | Loss: 2.2133 | Avg: 1.7709 | LR: 2.0e-06\n",
      "üìà Step 19200 | Loss: 1.5628 | Avg: 1.7601 | LR: 2.0e-06\n",
      "üìà Step 19225 | Loss: 2.0361 | Avg: 1.8408 | LR: 2.0e-06\n",
      "üìà Step 19250 | Loss: 1.5581 | Avg: 1.8894 | LR: 1.9e-06\n",
      "üìà Step 19275 | Loss: 0.6015 | Avg: 1.7754 | LR: 1.9e-06\n",
      "üìà Step 19300 | Loss: 2.4077 | Avg: 1.9906 | LR: 1.9e-06\n",
      "üìà Step 19325 | Loss: 1.0939 | Avg: 1.8030 | LR: 1.9e-06\n",
      "üìà Step 19350 | Loss: 1.6509 | Avg: 1.6529 | LR: 1.9e-06\n",
      "üìà Step 19375 | Loss: 2.2993 | Avg: 1.8913 | LR: 1.9e-06\n",
      "üìà Step 19400 | Loss: 1.3210 | Avg: 1.6910 | LR: 1.9e-06\n",
      "üìà Step 19425 | Loss: 1.8362 | Avg: 1.7456 | LR: 1.9e-06\n",
      "üìà Step 19450 | Loss: 1.0371 | Avg: 1.9376 | LR: 1.9e-06\n",
      "üìà Step 19475 | Loss: 2.4093 | Avg: 1.9656 | LR: 1.9e-06\n",
      "üìà Step 19500 | Loss: 2.2994 | Avg: 2.1092 | LR: 1.9e-06\n",
      "üìà Step 19525 | Loss: 3.0814 | Avg: 1.9382 | LR: 1.9e-06\n",
      "üìà Step 19550 | Loss: 1.3732 | Avg: 2.0223 | LR: 1.9e-06\n",
      "üìà Step 19575 | Loss: 1.2219 | Avg: 1.7249 | LR: 1.9e-06\n",
      "üìà Step 19600 | Loss: 2.5545 | Avg: 2.0041 | LR: 1.9e-06\n",
      "üìà Step 19625 | Loss: 1.8056 | Avg: 1.9532 | LR: 1.9e-06\n",
      "üìà Step 19650 | Loss: 1.2567 | Avg: 1.7133 | LR: 1.9e-06\n",
      "üìà Step 19675 | Loss: 1.5784 | Avg: 1.8878 | LR: 1.9e-06\n",
      "üìà Step 19700 | Loss: 1.7277 | Avg: 1.7802 | LR: 1.9e-06\n",
      "üìà Step 19725 | Loss: 0.9408 | Avg: 1.7923 | LR: 1.9e-06\n",
      "üìà Step 19750 | Loss: 1.3526 | Avg: 1.8558 | LR: 1.8e-06\n",
      "üìà Step 19775 | Loss: 1.8617 | Avg: 1.7451 | LR: 1.8e-06\n",
      "üìà Step 19800 | Loss: 1.5225 | Avg: 1.7894 | LR: 1.8e-06\n",
      "üìà Step 19825 | Loss: 1.4863 | Avg: 1.6436 | LR: 1.8e-06\n",
      "üìà Step 19850 | Loss: 1.5848 | Avg: 1.8743 | LR: 1.8e-06\n",
      "üìà Step 19875 | Loss: 2.5121 | Avg: 1.7485 | LR: 1.8e-06\n",
      "üìà Step 19900 | Loss: 1.3160 | Avg: 1.8333 | LR: 1.8e-06\n",
      "üìà Step 19925 | Loss: 1.5124 | Avg: 1.8811 | LR: 1.8e-06\n",
      "üìà Step 19950 | Loss: 2.4971 | Avg: 1.7096 | LR: 1.8e-06\n",
      "üìà Step 19975 | Loss: 2.4140 | Avg: 1.9142 | LR: 1.8e-06\n",
      "üìà Step 20000 | Loss: 3.3660 | Avg: 1.6751 | LR: 1.8e-06\n",
      "üìà Step 20025 | Loss: 2.2718 | Avg: 1.8479 | LR: 1.8e-06\n",
      "üìà Step 20050 | Loss: 1.6555 | Avg: 1.9373 | LR: 1.8e-06\n",
      "üìà Step 20075 | Loss: 2.7801 | Avg: 1.7911 | LR: 1.8e-06\n",
      "üìà Step 20100 | Loss: 1.6362 | Avg: 1.9824 | LR: 1.7e-06\n",
      "üìà Step 20125 | Loss: 3.2158 | Avg: 1.7835 | LR: 1.7e-06\n",
      "üìà Step 20150 | Loss: 1.7068 | Avg: 1.7014 | LR: 1.7e-06\n",
      "üìà Step 20175 | Loss: 0.7931 | Avg: 1.9769 | LR: 1.7e-06\n",
      "üìà Step 20200 | Loss: 1.4136 | Avg: 1.6501 | LR: 1.7e-06\n",
      "üìà Step 20225 | Loss: 1.4688 | Avg: 1.7937 | LR: 1.7e-06\n",
      "üìà Step 20250 | Loss: 1.1801 | Avg: 1.7088 | LR: 1.7e-06\n",
      "üìà Step 20275 | Loss: 0.8447 | Avg: 1.6526 | LR: 1.7e-06\n",
      "üìà Step 20300 | Loss: 2.2154 | Avg: 1.8208 | LR: 1.7e-06\n",
      "üìà Step 20325 | Loss: 1.2977 | Avg: 1.7633 | LR: 1.7e-06\n",
      "üìà Step 20350 | Loss: 2.3306 | Avg: 1.6726 | LR: 1.7e-06\n",
      "üìà Step 20375 | Loss: 1.9447 | Avg: 1.7711 | LR: 1.7e-06\n",
      "üìà Step 20400 | Loss: 1.3938 | Avg: 1.7674 | LR: 1.6e-06\n",
      "üìà Step 20425 | Loss: 1.1175 | Avg: 1.6556 | LR: 1.6e-06\n",
      "üìà Step 20450 | Loss: 1.0194 | Avg: 1.8120 | LR: 1.6e-06\n",
      "üìà Step 20475 | Loss: 2.1597 | Avg: 1.6468 | LR: 1.6e-06\n",
      "üìà Step 20500 | Loss: 2.1634 | Avg: 1.9029 | LR: 1.6e-06\n",
      "üìà Step 20525 | Loss: 1.2630 | Avg: 1.8958 | LR: 1.6e-06\n",
      "üìà Step 20550 | Loss: 1.7215 | Avg: 1.7775 | LR: 1.6e-06\n",
      "üíæ Stable checkpoint saved at 90% of epoch 2\n",
      "üìà Step 20575 | Loss: 1.6624 | Avg: 1.6935 | LR: 1.6e-06\n",
      "üìà Step 20600 | Loss: 1.2129 | Avg: 1.7476 | LR: 1.6e-06\n",
      "üìà Step 20625 | Loss: 1.2613 | Avg: 1.8520 | LR: 1.6e-06\n",
      "üìà Step 20650 | Loss: 1.3573 | Avg: 1.8341 | LR: 1.6e-06\n",
      "üìà Step 20675 | Loss: 1.6806 | Avg: 1.7689 | LR: 1.5e-06\n",
      "üìà Step 20700 | Loss: 1.9678 | Avg: 1.7391 | LR: 1.5e-06\n",
      "üìà Step 20725 | Loss: 1.2594 | Avg: 1.6716 | LR: 1.5e-06\n",
      "üìà Step 20750 | Loss: 2.4484 | Avg: 1.8537 | LR: 1.5e-06\n",
      "üìà Step 20775 | Loss: 2.4588 | Avg: 1.8128 | LR: 1.5e-06\n",
      "üìà Step 20800 | Loss: 1.3992 | Avg: 1.6635 | LR: 1.5e-06\n",
      "üìà Step 20825 | Loss: 1.3462 | Avg: 1.8308 | LR: 1.5e-06\n",
      "üìà Step 20850 | Loss: 1.3834 | Avg: 1.6946 | LR: 1.5e-06\n",
      "üìà Step 20875 | Loss: 1.7974 | Avg: 1.6341 | LR: 1.5e-06\n",
      "üìà Step 20900 | Loss: 2.9641 | Avg: 1.6780 | LR: 1.4e-06\n",
      "üìà Step 20925 | Loss: 2.6142 | Avg: 1.8680 | LR: 1.4e-06\n",
      "üìà Step 20950 | Loss: 1.4981 | Avg: 1.7338 | LR: 1.4e-06\n",
      "üìà Step 20975 | Loss: 1.5401 | Avg: 1.7948 | LR: 1.4e-06\n",
      "üìà Step 21000 | Loss: 1.4838 | Avg: 1.7388 | LR: 1.4e-06\n",
      "üìà Step 21025 | Loss: 1.7419 | Avg: 1.8128 | LR: 1.4e-06\n",
      "üìà Step 21050 | Loss: 1.4495 | Avg: 1.7134 | LR: 1.4e-06\n",
      "üìà Step 21075 | Loss: 1.1796 | Avg: 1.8044 | LR: 1.4e-06\n",
      "üìà Step 21100 | Loss: 2.1282 | Avg: 1.8296 | LR: 1.4e-06\n",
      "üìà Step 21125 | Loss: 1.1188 | Avg: 1.6704 | LR: 1.3e-06\n",
      "üìà Step 21150 | Loss: 1.8503 | Avg: 1.9609 | LR: 1.3e-06\n",
      "üìà Step 21175 | Loss: 1.5841 | Avg: 1.7470 | LR: 1.3e-06\n",
      "üìà Step 21200 | Loss: 0.8621 | Avg: 1.6632 | LR: 1.3e-06\n",
      "üìà Step 21225 | Loss: 2.6127 | Avg: 1.6950 | LR: 1.3e-06\n",
      "üìà Step 21250 | Loss: 1.6134 | Avg: 1.7915 | LR: 1.3e-06\n",
      "üìà Step 21275 | Loss: 1.2789 | Avg: 1.7680 | LR: 1.3e-06\n",
      "üìà Step 21300 | Loss: 2.7407 | Avg: 1.7017 | LR: 1.3e-06\n",
      "üìà Step 21325 | Loss: 1.2793 | Avg: 1.8048 | LR: 1.3e-06\n",
      "üìà Step 21350 | Loss: 2.4155 | Avg: 1.8523 | LR: 1.2e-06\n",
      "üìà Step 21375 | Loss: 0.7441 | Avg: 1.7341 | LR: 1.2e-06\n",
      "üìà Step 21400 | Loss: 1.4624 | Avg: 1.6184 | LR: 1.2e-06\n",
      "üìà Step 21425 | Loss: 1.4791 | Avg: 1.7430 | LR: 1.2e-06\n",
      "üìà Step 21450 | Loss: 0.6135 | Avg: 1.7221 | LR: 1.2e-06\n",
      "üìà Step 21475 | Loss: 1.8992 | Avg: 1.8928 | LR: 1.2e-06\n",
      "üìà Step 21500 | Loss: 0.7698 | Avg: 1.8476 | LR: 1.2e-06\n",
      "üìà Step 21525 | Loss: 2.9498 | Avg: 1.9196 | LR: 1.2e-06\n",
      "üìà Step 21550 | Loss: 1.0829 | Avg: 1.8050 | LR: 1.2e-06\n",
      "üìà Step 21575 | Loss: 1.3136 | Avg: 1.7218 | LR: 1.1e-06\n",
      "üìà Step 21600 | Loss: 2.4050 | Avg: 1.6917 | LR: 1.1e-06\n",
      "üìà Step 21625 | Loss: 2.2534 | Avg: 1.8719 | LR: 1.1e-06\n",
      "üìà Step 21650 | Loss: 1.8614 | Avg: 1.6602 | LR: 1.1e-06\n",
      "üìà Step 21675 | Loss: 1.6850 | Avg: 1.9104 | LR: 1.1e-06\n",
      "üìà Step 21700 | Loss: 1.8917 | Avg: 1.8273 | LR: 1.1e-06\n",
      "üìà Step 21725 | Loss: 2.2088 | Avg: 1.8451 | LR: 1.1e-06\n",
      "üìà Step 21750 | Loss: 1.9082 | Avg: 1.7187 | LR: 1.1e-06\n",
      "üìà Step 21775 | Loss: 2.0687 | Avg: 1.7813 | LR: 1.0e-06\n",
      "üìà Step 21800 | Loss: 3.8094 | Avg: 1.8751 | LR: 1.0e-06\n",
      "üìà Step 21825 | Loss: 1.7692 | Avg: 1.5940 | LR: 1.0e-06\n",
      "üìà Step 21850 | Loss: 1.4417 | Avg: 1.5599 | LR: 1.0e-06\n",
      "üìà Step 21875 | Loss: 1.1785 | Avg: 1.9070 | LR: 1.0e-06\n",
      "üìà Step 21900 | Loss: 2.0166 | Avg: 1.8609 | LR: 9.9e-07\n",
      "üìà Step 21925 | Loss: 1.6202 | Avg: 1.5990 | LR: 9.7e-07\n",
      "üìà Step 21950 | Loss: 1.3324 | Avg: 1.5789 | LR: 9.6e-07\n",
      "üìà Step 21975 | Loss: 2.0236 | Avg: 1.7267 | LR: 9.5e-07\n",
      "üìà Step 22000 | Loss: 2.1783 | Avg: 1.9151 | LR: 9.4e-07\n",
      "üìà Step 22025 | Loss: 2.2211 | Avg: 1.9040 | LR: 9.3e-07\n",
      "üìà Step 22050 | Loss: 1.8753 | Avg: 1.7716 | LR: 9.1e-07\n",
      "üìà Step 22075 | Loss: 1.2704 | Avg: 1.6458 | LR: 9.0e-07\n",
      "üìà Step 22100 | Loss: 1.7843 | Avg: 1.7423 | LR: 8.9e-07\n",
      "üìà Step 22125 | Loss: 2.2162 | Avg: 1.6121 | LR: 8.8e-07\n",
      "üìà Step 22150 | Loss: 2.2082 | Avg: 1.6484 | LR: 8.7e-07\n",
      "üìà Step 22175 | Loss: 2.0781 | Avg: 1.5560 | LR: 8.5e-07\n",
      "üìà Step 22200 | Loss: 2.6285 | Avg: 1.7389 | LR: 8.4e-07\n",
      "üìà Step 22225 | Loss: 2.5022 | Avg: 1.7891 | LR: 8.3e-07\n",
      "üìà Step 22250 | Loss: 2.2386 | Avg: 1.8824 | LR: 8.2e-07\n",
      "üìà Step 22275 | Loss: 2.2282 | Avg: 1.8271 | LR: 8.1e-07\n",
      "üìà Step 22300 | Loss: 1.1757 | Avg: 1.6311 | LR: 8.0e-07\n",
      "üìà Step 22325 | Loss: 1.7206 | Avg: 1.7131 | LR: 7.8e-07\n",
      "üìà Step 22350 | Loss: 1.0393 | Avg: 1.9061 | LR: 7.7e-07\n",
      "üìà Step 22375 | Loss: 1.2742 | Avg: 1.7985 | LR: 7.6e-07\n",
      "üìà Step 22400 | Loss: 1.6018 | Avg: 1.6563 | LR: 7.5e-07\n",
      "üìà Step 22425 | Loss: 1.3717 | Avg: 1.7641 | LR: 7.4e-07\n",
      "üìà Step 22450 | Loss: 2.7700 | Avg: 1.9517 | LR: 7.3e-07\n",
      "üìà Step 22475 | Loss: 2.4444 | Avg: 2.1051 | LR: 7.1e-07\n",
      "üìà Step 22500 | Loss: 1.4370 | Avg: 1.8264 | LR: 7.0e-07\n",
      "üìà Step 22525 | Loss: 1.3202 | Avg: 1.7428 | LR: 6.9e-07\n",
      "üìà Step 22550 | Loss: 0.6472 | Avg: 1.8769 | LR: 6.8e-07\n",
      "üìà Step 22575 | Loss: 1.3712 | Avg: 1.7986 | LR: 6.7e-07\n",
      "üìà Step 22600 | Loss: 1.9050 | Avg: 1.9675 | LR: 6.6e-07\n",
      "üìà Step 22625 | Loss: 1.3812 | Avg: 1.7839 | LR: 6.5e-07\n",
      "üìà Step 22650 | Loss: 1.5709 | Avg: 1.7618 | LR: 6.4e-07\n",
      "üìà Step 22675 | Loss: 1.3356 | Avg: 1.8013 | LR: 6.2e-07\n",
      "üìà Step 22700 | Loss: 2.2068 | Avg: 1.6356 | LR: 6.1e-07\n",
      "üìà Step 22725 | Loss: 2.4751 | Avg: 1.9803 | LR: 6.0e-07\n",
      "üìà Step 22750 | Loss: 1.7567 | Avg: 1.7354 | LR: 5.9e-07\n",
      "üìà Step 22775 | Loss: 1.6500 | Avg: 1.8371 | LR: 5.8e-07\n",
      "üìà Step 22800 | Loss: 1.2232 | Avg: 1.6424 | LR: 5.7e-07\n",
      "üìà Step 22825 | Loss: 1.9247 | Avg: 1.7897 | LR: 5.6e-07\n",
      "üìà Step 22850 | Loss: 0.9141 | Avg: 1.7099 | LR: 5.5e-07\n",
      "üìä Epoch 2 - Average train loss: 2.0622 (Stable!)\n",
      "üíæ End-of-epoch checkpoint saved: epoch2_final\n",
      "üîç Running validation for epoch 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96683d2a368849509734439d5bad8cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 2: train_loss=2.0622 val_loss=1.4827\n",
      "‚úÖ Best checkpoint updated!\n",
      "==== Epoch 3/3 (Ultra-Stable Mode) ====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827f46d2795049eaa32303a80ee0a800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 - Stable:   0%|          | 0/22861 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Step    25 | Loss: 1.8999 | Avg: 1.7587 | LR: 5.3e-07\n",
      "üìà Step    50 | Loss: 2.5936 | Avg: 1.8653 | LR: 5.2e-07\n",
      "üìà Step    75 | Loss: 1.6664 | Avg: 1.7487 | LR: 5.1e-07\n",
      "üìà Step   100 | Loss: 1.5564 | Avg: 1.9354 | LR: 5.0e-07\n",
      "üìà Step   125 | Loss: 1.8583 | Avg: 1.6046 | LR: 4.9e-07\n",
      "üìà Step   150 | Loss: 1.3506 | Avg: 1.5569 | LR: 4.8e-07\n",
      "üìà Step   175 | Loss: 1.5452 | Avg: 1.6287 | LR: 4.7e-07\n",
      "üìà Step   200 | Loss: 3.2693 | Avg: 1.8176 | LR: 4.6e-07\n",
      "üìà Step   225 | Loss: 1.7411 | Avg: 1.7012 | LR: 4.5e-07\n",
      "üìà Step   250 | Loss: 0.7183 | Avg: 1.5720 | LR: 4.4e-07\n",
      "üìà Step   275 | Loss: 1.9585 | Avg: 1.6376 | LR: 4.3e-07\n",
      "üìà Step   300 | Loss: 1.9801 | Avg: 1.9550 | LR: 4.2e-07\n",
      "üìà Step   325 | Loss: 2.1208 | Avg: 1.6199 | LR: 4.1e-07\n",
      "üìà Step   350 | Loss: 1.0944 | Avg: 1.6983 | LR: 4.0e-07\n",
      "üìà Step   375 | Loss: 1.5609 | Avg: 1.5659 | LR: 3.9e-07\n",
      "üìà Step   400 | Loss: 1.2918 | Avg: 1.9483 | LR: 3.8e-07\n",
      "üìà Step   425 | Loss: 0.7959 | Avg: 1.6613 | LR: 3.7e-07\n",
      "üìà Step   450 | Loss: 2.0632 | Avg: 1.6604 | LR: 3.6e-07\n",
      "üìà Step   475 | Loss: 2.3054 | Avg: 1.7692 | LR: 3.6e-07\n",
      "üìà Step   500 | Loss: 1.3675 | Avg: 1.5519 | LR: 3.5e-07\n",
      "üìà Step   525 | Loss: 1.5613 | Avg: 1.7063 | LR: 3.4e-07\n",
      "üìà Step   550 | Loss: 1.4116 | Avg: 1.8531 | LR: 3.3e-07\n",
      "üìà Step   575 | Loss: 1.3999 | Avg: 1.8227 | LR: 3.2e-07\n",
      "üìà Step   600 | Loss: 2.1896 | Avg: 1.7405 | LR: 3.1e-07\n",
      "üìà Step   625 | Loss: 2.7142 | Avg: 1.7244 | LR: 3.0e-07\n",
      "üìà Step   650 | Loss: 2.0412 | Avg: 1.9330 | LR: 2.9e-07\n",
      "üìà Step   675 | Loss: 2.1172 | Avg: 1.7939 | LR: 2.9e-07\n",
      "üìà Step   700 | Loss: 1.0823 | Avg: 1.8705 | LR: 2.8e-07\n",
      "üìà Step   725 | Loss: 2.0077 | Avg: 1.6307 | LR: 2.7e-07\n",
      "üìà Step   750 | Loss: 1.7059 | Avg: 1.8601 | LR: 2.6e-07\n",
      "üìà Step   775 | Loss: 1.2121 | Avg: 1.5358 | LR: 2.5e-07\n",
      "üìà Step   800 | Loss: 2.3611 | Avg: 1.9762 | LR: 2.4e-07\n",
      "üìà Step   825 | Loss: 2.5425 | Avg: 1.5345 | LR: 2.4e-07\n",
      "üìà Step   850 | Loss: 2.7886 | Avg: 1.7574 | LR: 2.3e-07\n",
      "üìà Step   875 | Loss: 1.4329 | Avg: 1.6967 | LR: 2.2e-07\n",
      "üìà Step   900 | Loss: 1.3111 | Avg: 1.8703 | LR: 2.1e-07\n",
      "üìà Step   925 | Loss: 1.9878 | Avg: 1.6341 | LR: 2.1e-07\n",
      "üìà Step   950 | Loss: 1.3324 | Avg: 1.6544 | LR: 2.0e-07\n",
      "üìà Step   975 | Loss: 1.7951 | Avg: 1.7934 | LR: 1.9e-07\n",
      "üìà Step  1000 | Loss: 1.6227 | Avg: 1.5821 | LR: 1.9e-07\n",
      "üìà Step  1025 | Loss: 1.9742 | Avg: 1.6723 | LR: 1.8e-07\n",
      "üìà Step  1050 | Loss: 1.1475 | Avg: 1.6679 | LR: 1.7e-07\n",
      "üìà Step  1075 | Loss: 1.2327 | Avg: 1.8302 | LR: 1.7e-07\n",
      "üìà Step  1100 | Loss: 2.2773 | Avg: 1.7495 | LR: 1.6e-07\n",
      "üìà Step  1125 | Loss: 3.4544 | Avg: 1.6673 | LR: 1.5e-07\n",
      "üìà Step  1150 | Loss: 2.0456 | Avg: 1.5368 | LR: 1.5e-07\n",
      "üìà Step  1175 | Loss: 1.4634 | Avg: 1.7361 | LR: 1.4e-07\n",
      "üìà Step  1200 | Loss: 1.1302 | Avg: 1.8762 | LR: 1.3e-07\n",
      "üìà Step  1225 | Loss: 2.4410 | Avg: 1.7846 | LR: 1.3e-07\n",
      "üìà Step  1250 | Loss: 2.6308 | Avg: 1.6283 | LR: 1.2e-07\n",
      "üìà Step  1275 | Loss: 2.5516 | Avg: 1.7136 | LR: 1.2e-07\n",
      "üìà Step  1300 | Loss: 2.4171 | Avg: 1.7388 | LR: 1.1e-07\n",
      "üìà Step  1325 | Loss: 2.3155 | Avg: 1.7720 | LR: 1.1e-07\n",
      "üìà Step  1350 | Loss: 1.9745 | Avg: 1.8633 | LR: 1.0e-07\n",
      "üìà Step  1375 | Loss: 1.6838 | Avg: 1.6245 | LR: 9.5e-08\n",
      "üìà Step  1400 | Loss: 1.5984 | Avg: 1.5000 | LR: 9.0e-08\n",
      "üìà Step  1425 | Loss: 1.6611 | Avg: 1.7251 | LR: 8.5e-08\n",
      "üìà Step  1450 | Loss: 1.4151 | Avg: 1.6824 | LR: 8.1e-08\n",
      "üìà Step  1475 | Loss: 0.7749 | Avg: 1.7645 | LR: 7.6e-08\n",
      "üìà Step  1500 | Loss: 2.5707 | Avg: 1.8414 | LR: 7.1e-08\n",
      "üìà Step  1525 | Loss: 1.6379 | Avg: 1.7174 | LR: 6.7e-08\n",
      "üìà Step  1550 | Loss: 3.4824 | Avg: 1.9623 | LR: 6.3e-08\n",
      "üìà Step  1575 | Loss: 2.6169 | Avg: 1.7334 | LR: 5.9e-08\n",
      "üìà Step  1600 | Loss: 1.4844 | Avg: 1.7018 | LR: 5.5e-08\n",
      "üìà Step  1625 | Loss: 1.2858 | Avg: 1.7571 | LR: 5.1e-08\n",
      "üìà Step  1650 | Loss: 1.6044 | Avg: 1.5567 | LR: 4.7e-08\n",
      "üìà Step  1675 | Loss: 2.1749 | Avg: 1.7469 | LR: 4.4e-08\n",
      "üìà Step  1700 | Loss: 1.1407 | Avg: 1.8421 | LR: 4.0e-08\n",
      "üìà Step  1725 | Loss: 2.7409 | Avg: 1.8155 | LR: 3.7e-08\n",
      "üìà Step  1750 | Loss: 1.1810 | Avg: 1.8709 | LR: 3.4e-08\n",
      "üìà Step  1775 | Loss: 0.9967 | Avg: 1.7173 | LR: 3.1e-08\n",
      "üìà Step  1800 | Loss: 1.3106 | Avg: 1.6567 | LR: 2.8e-08\n",
      "üìà Step  1825 | Loss: 2.6063 | Avg: 1.9594 | LR: 2.5e-08\n",
      "üìà Step  1850 | Loss: 1.0439 | Avg: 1.8131 | LR: 2.3e-08\n",
      "üìà Step  1875 | Loss: 1.6657 | Avg: 1.7555 | LR: 2.0e-08\n",
      "üìà Step  1900 | Loss: 2.4814 | Avg: 1.5954 | LR: 1.8e-08\n",
      "üìà Step  1925 | Loss: 1.2940 | Avg: 1.6966 | LR: 1.6e-08\n",
      "üìà Step  1950 | Loss: 0.7781 | Avg: 1.7271 | LR: 1.4e-08\n",
      "üìà Step  1975 | Loss: 1.3331 | Avg: 1.7800 | LR: 1.2e-08\n",
      "üìà Step  2000 | Loss: 1.6346 | Avg: 1.6978 | LR: 1.0e-08\n",
      "üìà Step  2025 | Loss: 1.0163 | Avg: 1.5661 | LR: 8.4e-09\n",
      "üìà Step  2050 | Loss: 2.4551 | Avg: 1.6303 | LR: 6.9e-09\n",
      "üìà Step  2075 | Loss: 1.6230 | Avg: 1.8202 | LR: 5.6e-09\n",
      "üìà Step  2100 | Loss: 2.0238 | Avg: 1.8772 | LR: 4.4e-09\n",
      "üìà Step  2125 | Loss: 1.5902 | Avg: 1.7469 | LR: 3.3e-09\n",
      "üìà Step  2150 | Loss: 0.8972 | Avg: 1.8270 | LR: 2.4e-09\n",
      "üìà Step  2175 | Loss: 1.1913 | Avg: 1.8696 | LR: 1.7e-09\n",
      "üìà Step  2200 | Loss: 1.4995 | Avg: 1.6721 | LR: 1.1e-09\n",
      "üìà Step  2225 | Loss: 1.0687 | Avg: 1.6749 | LR: 5.8e-10\n",
      "üìà Step  2250 | Loss: 2.1218 | Avg: 1.9026 | LR: 2.4e-10\n",
      "üìà Step  2275 | Loss: 1.2275 | Avg: 1.7431 | LR: 5.0e-11\n",
      "üìà Step  2300 | Loss: 1.9221 | Avg: 1.8138 | LR: 1.8e-12\n",
      "üìà Step  2325 | Loss: 1.1092 | Avg: 1.7530 | LR: 9.6e-11\n",
      "üìà Step  2350 | Loss: 1.5133 | Avg: 1.8517 | LR: 3.3e-10\n",
      "üìà Step  2375 | Loss: 1.4944 | Avg: 1.5874 | LR: 7.1e-10\n",
      "üìà Step  2400 | Loss: 2.1644 | Avg: 1.5673 | LR: 1.2e-09\n",
      "üìà Step  2425 | Loss: 1.8744 | Avg: 1.7106 | LR: 1.9e-09\n",
      "üìà Step  2450 | Loss: 1.9879 | Avg: 1.7939 | LR: 2.7e-09\n",
      "üìà Step  2475 | Loss: 1.6538 | Avg: 1.7313 | LR: 3.7e-09\n",
      "üìà Step  2500 | Loss: 1.2604 | Avg: 1.5442 | LR: 4.7e-09\n",
      "üìà Step  2525 | Loss: 1.7103 | Avg: 1.6333 | LR: 6.0e-09\n",
      "üìà Step  2550 | Loss: 2.2776 | Avg: 1.6632 | LR: 7.4e-09\n",
      "üìà Step  2575 | Loss: 2.3308 | Avg: 1.8535 | LR: 8.9e-09\n",
      "üìà Step  2600 | Loss: 2.1840 | Avg: 1.8068 | LR: 1.1e-08\n",
      "üìà Step  2625 | Loss: 2.1711 | Avg: 1.7047 | LR: 1.2e-08\n",
      "üìà Step  2650 | Loss: 1.7666 | Avg: 1.7237 | LR: 1.4e-08\n",
      "üìà Step  2675 | Loss: 2.0048 | Avg: 1.6361 | LR: 1.6e-08\n",
      "üìà Step  2700 | Loss: 0.9327 | Avg: 1.7071 | LR: 1.9e-08\n",
      "üìà Step  2725 | Loss: 1.8735 | Avg: 1.5835 | LR: 2.1e-08\n",
      "üìà Step  2750 | Loss: 2.8691 | Avg: 1.8590 | LR: 2.3e-08\n",
      "üìà Step  2775 | Loss: 3.1790 | Avg: 1.9285 | LR: 2.6e-08\n",
      "üìà Step  2800 | Loss: 1.0986 | Avg: 1.6620 | LR: 2.9e-08\n",
      "üìà Step  2825 | Loss: 1.3982 | Avg: 1.6462 | LR: 3.2e-08\n",
      "üìà Step  2850 | Loss: 3.0103 | Avg: 1.9416 | LR: 3.5e-08\n",
      "üìà Step  2875 | Loss: 2.1596 | Avg: 1.6951 | LR: 3.8e-08\n",
      "üìà Step  2900 | Loss: 1.8467 | Avg: 1.8039 | LR: 4.1e-08\n",
      "üìà Step  2925 | Loss: 1.5401 | Avg: 1.6089 | LR: 4.5e-08\n",
      "üìà Step  2950 | Loss: 1.8171 | Avg: 1.8300 | LR: 4.8e-08\n",
      "üìà Step  2975 | Loss: 1.3752 | Avg: 1.8436 | LR: 5.2e-08\n",
      "üìà Step  3000 | Loss: 2.2409 | Avg: 1.5654 | LR: 5.6e-08\n",
      "üìà Step  3025 | Loss: 2.1148 | Avg: 1.8760 | LR: 6.0e-08\n",
      "üìà Step  3050 | Loss: 1.4232 | Avg: 1.6586 | LR: 6.4e-08\n",
      "üìà Step  3075 | Loss: 2.4182 | Avg: 1.6462 | LR: 6.8e-08\n",
      "üìà Step  3100 | Loss: 1.2916 | Avg: 1.7211 | LR: 7.3e-08\n",
      "üìà Step  3125 | Loss: 1.3558 | Avg: 1.6267 | LR: 7.7e-08\n",
      "üìà Step  3150 | Loss: 0.8491 | Avg: 1.5167 | LR: 8.2e-08\n",
      "üìà Step  3175 | Loss: 1.0011 | Avg: 1.7161 | LR: 8.7e-08\n",
      "üìà Step  3200 | Loss: 1.8727 | Avg: 1.6971 | LR: 9.2e-08\n",
      "üìà Step  3225 | Loss: 1.6311 | Avg: 1.6415 | LR: 9.7e-08\n",
      "üìà Step  3250 | Loss: 1.9343 | Avg: 1.8055 | LR: 1.0e-07\n",
      "üìà Step  3275 | Loss: 1.0311 | Avg: 1.6907 | LR: 1.1e-07\n",
      "üìà Step  3300 | Loss: 2.3633 | Avg: 2.0882 | LR: 1.1e-07\n",
      "üìà Step  3325 | Loss: 1.6356 | Avg: 1.8450 | LR: 1.2e-07\n",
      "üìà Step  3350 | Loss: 2.0139 | Avg: 1.7186 | LR: 1.2e-07\n",
      "üìà Step  3375 | Loss: 1.3276 | Avg: 1.6481 | LR: 1.3e-07\n",
      "üìà Step  3400 | Loss: 1.7507 | Avg: 1.5967 | LR: 1.4e-07\n",
      "üìà Step  3425 | Loss: 1.4383 | Avg: 1.4799 | LR: 1.4e-07\n",
      "üíæ Stable checkpoint saved at 15% of epoch 3\n",
      "üìà Step  3450 | Loss: 2.5271 | Avg: 1.7300 | LR: 1.5e-07\n",
      "üìà Step  3475 | Loss: 1.6442 | Avg: 1.6359 | LR: 1.5e-07\n",
      "üìà Step  3500 | Loss: 2.3552 | Avg: 1.7457 | LR: 1.6e-07\n",
      "üìà Step  3525 | Loss: 1.2649 | Avg: 1.6903 | LR: 1.7e-07\n",
      "üìà Step  3550 | Loss: 1.2638 | Avg: 1.6498 | LR: 1.7e-07\n",
      "üìà Step  3575 | Loss: 1.6553 | Avg: 1.8103 | LR: 1.8e-07\n",
      "üìà Step  3600 | Loss: 2.1095 | Avg: 1.6929 | LR: 1.9e-07\n",
      "üìà Step  3625 | Loss: 2.2900 | Avg: 1.7744 | LR: 1.9e-07\n",
      "üìà Step  3650 | Loss: 3.6116 | Avg: 1.6585 | LR: 2.0e-07\n",
      "üìà Step  3675 | Loss: 1.9064 | Avg: 1.8686 | LR: 2.1e-07\n",
      "üìà Step  3700 | Loss: 2.7879 | Avg: 1.8256 | LR: 2.2e-07\n",
      "üìà Step  3725 | Loss: 1.8394 | Avg: 1.7536 | LR: 2.2e-07\n",
      "üìà Step  3750 | Loss: 2.8998 | Avg: 1.7038 | LR: 2.3e-07\n",
      "üìà Step  3775 | Loss: 2.0162 | Avg: 1.8487 | LR: 2.4e-07\n",
      "üìà Step  3800 | Loss: 1.7357 | Avg: 1.6209 | LR: 2.5e-07\n",
      "üìà Step  3825 | Loss: 1.4077 | Avg: 1.6612 | LR: 2.6e-07\n",
      "üìà Step  3850 | Loss: 0.8099 | Avg: 1.6291 | LR: 2.6e-07\n",
      "üìà Step  3875 | Loss: 2.4181 | Avg: 1.6034 | LR: 2.7e-07\n",
      "üìà Step  3900 | Loss: 1.6169 | Avg: 1.6684 | LR: 2.8e-07\n",
      "üìà Step  3925 | Loss: 2.7991 | Avg: 1.5220 | LR: 2.9e-07\n",
      "üìà Step  3950 | Loss: 1.9119 | Avg: 1.6454 | LR: 3.0e-07\n",
      "üìà Step  3975 | Loss: 2.3220 | Avg: 1.8048 | LR: 3.0e-07\n",
      "üìà Step  4000 | Loss: 1.9576 | Avg: 1.8153 | LR: 3.1e-07\n",
      "üìà Step  4025 | Loss: 1.8217 | Avg: 1.6947 | LR: 3.2e-07\n",
      "üìà Step  4050 | Loss: 1.0418 | Avg: 1.6119 | LR: 3.3e-07\n",
      "üìà Step  4075 | Loss: 0.7889 | Avg: 1.7530 | LR: 3.4e-07\n",
      "üìà Step  4100 | Loss: 1.7961 | Avg: 1.7835 | LR: 3.5e-07\n",
      "üìà Step  4125 | Loss: 1.7975 | Avg: 1.7174 | LR: 3.6e-07\n",
      "üìà Step  4150 | Loss: 2.2816 | Avg: 1.7570 | LR: 3.7e-07\n",
      "üìà Step  4175 | Loss: 2.1748 | Avg: 1.8270 | LR: 3.8e-07\n",
      "üìà Step  4200 | Loss: 1.3468 | Avg: 1.8090 | LR: 3.9e-07\n",
      "üìà Step  4225 | Loss: 1.0914 | Avg: 1.5545 | LR: 4.0e-07\n",
      "üìà Step  4250 | Loss: 1.7368 | Avg: 1.7732 | LR: 4.1e-07\n",
      "üìà Step  4275 | Loss: 1.1810 | Avg: 2.0351 | LR: 4.1e-07\n",
      "üìà Step  4300 | Loss: 2.7142 | Avg: 1.8834 | LR: 4.2e-07\n",
      "üìà Step  4325 | Loss: 1.3287 | Avg: 1.6113 | LR: 4.3e-07\n",
      "üìà Step  4350 | Loss: 1.2933 | Avg: 1.8960 | LR: 4.4e-07\n",
      "üìà Step  4375 | Loss: 1.8447 | Avg: 1.7465 | LR: 4.5e-07\n",
      "üìà Step  4400 | Loss: 1.4954 | Avg: 1.7654 | LR: 4.6e-07\n",
      "üìà Step  4425 | Loss: 2.4925 | Avg: 1.7608 | LR: 4.7e-07\n",
      "üìà Step  4450 | Loss: 1.4807 | Avg: 1.5964 | LR: 4.8e-07\n",
      "üìà Step  4475 | Loss: 2.7559 | Avg: 1.7827 | LR: 4.9e-07\n",
      "üìà Step  4500 | Loss: 1.0570 | Avg: 1.6208 | LR: 5.1e-07\n",
      "üìà Step  4525 | Loss: 1.5883 | Avg: 1.7138 | LR: 5.2e-07\n",
      "üìà Step  4550 | Loss: 3.5384 | Avg: 1.7611 | LR: 5.3e-07\n",
      "üìà Step  4575 | Loss: 2.3501 | Avg: 1.7183 | LR: 5.4e-07\n",
      "üìà Step  4600 | Loss: 3.1679 | Avg: 1.8658 | LR: 5.5e-07\n",
      "üìà Step  4625 | Loss: 2.2584 | Avg: 1.8043 | LR: 5.6e-07\n",
      "üìà Step  4650 | Loss: 2.5795 | Avg: 1.7154 | LR: 5.7e-07\n",
      "üìà Step  4675 | Loss: 1.5812 | Avg: 1.9938 | LR: 5.8e-07\n",
      "üìà Step  4700 | Loss: 1.1966 | Avg: 1.7140 | LR: 5.9e-07\n",
      "üìà Step  4725 | Loss: 1.6106 | Avg: 1.8248 | LR: 6.0e-07\n",
      "üìà Step  4750 | Loss: 1.5851 | Avg: 1.7877 | LR: 6.1e-07\n",
      "üìà Step  4775 | Loss: 2.0625 | Avg: 1.6805 | LR: 6.2e-07\n",
      "üìà Step  4800 | Loss: 0.8103 | Avg: 1.6920 | LR: 6.3e-07\n",
      "üìà Step  4825 | Loss: 2.4962 | Avg: 1.7626 | LR: 6.5e-07\n",
      "üìà Step  4850 | Loss: 1.3266 | Avg: 1.7193 | LR: 6.6e-07\n",
      "üìà Step  4875 | Loss: 1.4198 | Avg: 1.6715 | LR: 6.7e-07\n",
      "üìà Step  4900 | Loss: 2.4589 | Avg: 1.7763 | LR: 6.8e-07\n",
      "üìà Step  4925 | Loss: 1.1378 | Avg: 1.8294 | LR: 6.9e-07\n",
      "üìà Step  4950 | Loss: 0.8352 | Avg: 1.7066 | LR: 7.0e-07\n",
      "üìà Step  4975 | Loss: 1.8801 | Avg: 1.6457 | LR: 7.1e-07\n",
      "üìà Step  5000 | Loss: 2.4432 | Avg: 1.6362 | LR: 7.2e-07\n",
      "üìà Step  5025 | Loss: 1.3744 | Avg: 1.7719 | LR: 7.4e-07\n",
      "üìà Step  5050 | Loss: 2.3010 | Avg: 1.4930 | LR: 7.5e-07\n",
      "üìà Step  5075 | Loss: 1.9838 | Avg: 1.7586 | LR: 7.6e-07\n",
      "üìà Step  5100 | Loss: 1.1620 | Avg: 1.6797 | LR: 7.7e-07\n",
      "üìà Step  5125 | Loss: 2.4185 | Avg: 1.7811 | LR: 7.8e-07\n",
      "üìà Step  5150 | Loss: 1.7458 | Avg: 1.8002 | LR: 7.9e-07\n",
      "üìà Step  5175 | Loss: 1.8989 | Avg: 1.7578 | LR: 8.1e-07\n",
      "üìà Step  5200 | Loss: 1.6098 | Avg: 1.6807 | LR: 8.2e-07\n",
      "üìà Step  5225 | Loss: 1.4588 | Avg: 1.7428 | LR: 8.3e-07\n",
      "üìà Step  5250 | Loss: 1.7784 | Avg: 1.5206 | LR: 8.4e-07\n",
      "üìà Step  5275 | Loss: 1.5780 | Avg: 1.6340 | LR: 8.5e-07\n",
      "üìà Step  5300 | Loss: 1.7144 | Avg: 1.6360 | LR: 8.7e-07\n",
      "üìà Step  5325 | Loss: 1.4883 | Avg: 1.8013 | LR: 8.8e-07\n",
      "üìà Step  5350 | Loss: 1.7331 | Avg: 1.6119 | LR: 8.9e-07\n",
      "üìà Step  5375 | Loss: 1.6948 | Avg: 1.8372 | LR: 9.0e-07\n",
      "üìà Step  5400 | Loss: 2.3067 | Avg: 1.7682 | LR: 9.1e-07\n",
      "üìà Step  5425 | Loss: 1.5139 | Avg: 1.7633 | LR: 9.2e-07\n",
      "üìà Step  5450 | Loss: 2.7595 | Avg: 1.5510 | LR: 9.4e-07\n",
      "üìà Step  5475 | Loss: 1.4796 | Avg: 1.7922 | LR: 9.5e-07\n",
      "üìà Step  5500 | Loss: 1.3722 | Avg: 1.6254 | LR: 9.6e-07\n",
      "üìà Step  5525 | Loss: 2.8813 | Avg: 1.4187 | LR: 9.7e-07\n",
      "üìà Step  5550 | Loss: 1.5364 | Avg: 1.5385 | LR: 9.8e-07\n",
      "üìà Step  5575 | Loss: 1.7845 | Avg: 1.7123 | LR: 1.0e-06\n",
      "üìà Step  5600 | Loss: 1.7443 | Avg: 1.6694 | LR: 1.0e-06\n",
      "üìà Step  5625 | Loss: 1.3865 | Avg: 1.7070 | LR: 1.0e-06\n",
      "üìà Step  5650 | Loss: 1.1134 | Avg: 1.6838 | LR: 1.0e-06\n",
      "üìà Step  5675 | Loss: 1.5046 | Avg: 1.6639 | LR: 1.0e-06\n",
      "üìà Step  5700 | Loss: 1.7066 | Avg: 1.6689 | LR: 1.1e-06\n",
      "üìà Step  5725 | Loss: 1.1139 | Avg: 1.7899 | LR: 1.1e-06\n",
      "üìà Step  5750 | Loss: 2.3503 | Avg: 1.6216 | LR: 1.1e-06\n",
      "üìà Step  5775 | Loss: 1.7105 | Avg: 1.6314 | LR: 1.1e-06\n",
      "üìà Step  5800 | Loss: 1.4413 | Avg: 1.5563 | LR: 1.1e-06\n",
      "üìà Step  5825 | Loss: 1.6018 | Avg: 1.8683 | LR: 1.1e-06\n",
      "üìà Step  5850 | Loss: 1.6455 | Avg: 1.6908 | LR: 1.1e-06\n",
      "üìà Step  5875 | Loss: 1.0581 | Avg: 1.5153 | LR: 1.1e-06\n",
      "üìà Step  5900 | Loss: 1.2797 | Avg: 1.7965 | LR: 1.2e-06\n",
      "üìà Step  5925 | Loss: 1.9787 | Avg: 1.5265 | LR: 1.2e-06\n",
      "üìà Step  5950 | Loss: 1.0302 | Avg: 1.7450 | LR: 1.2e-06\n",
      "üìà Step  5975 | Loss: 0.9508 | Avg: 1.5387 | LR: 1.2e-06\n",
      "üìà Step  6000 | Loss: 1.2324 | Avg: 1.7749 | LR: 1.2e-06\n",
      "üìà Step  6025 | Loss: 1.7412 | Avg: 1.6123 | LR: 1.2e-06\n",
      "üìà Step  6050 | Loss: 2.4689 | Avg: 1.7148 | LR: 1.2e-06\n",
      "üìà Step  6075 | Loss: 2.1526 | Avg: 1.8743 | LR: 1.2e-06\n",
      "üìà Step  6100 | Loss: 2.7598 | Avg: 1.6709 | LR: 1.2e-06\n",
      "üìà Step  6125 | Loss: 1.2996 | Avg: 1.8015 | LR: 1.3e-06\n",
      "üìà Step  6150 | Loss: 2.1674 | Avg: 1.6183 | LR: 1.3e-06\n",
      "üìà Step  6175 | Loss: 1.5418 | Avg: 1.6792 | LR: 1.3e-06\n",
      "üìà Step  6200 | Loss: 2.0246 | Avg: 1.6172 | LR: 1.3e-06\n",
      "üìà Step  6225 | Loss: 1.7397 | Avg: 1.8264 | LR: 1.3e-06\n",
      "üìà Step  6250 | Loss: 1.5175 | Avg: 1.5949 | LR: 1.3e-06\n",
      "üìà Step  6275 | Loss: 1.5366 | Avg: 1.6965 | LR: 1.3e-06\n",
      "üìà Step  6300 | Loss: 2.3624 | Avg: 1.5911 | LR: 1.3e-06\n",
      "üìà Step  6325 | Loss: 1.2136 | Avg: 1.7336 | LR: 1.3e-06\n",
      "üìà Step  6350 | Loss: 2.3474 | Avg: 1.7889 | LR: 1.4e-06\n",
      "üìà Step  6375 | Loss: 2.0781 | Avg: 1.7888 | LR: 1.4e-06\n",
      "üìà Step  6400 | Loss: 1.8100 | Avg: 1.6787 | LR: 1.4e-06\n",
      "üìà Step  6425 | Loss: 1.0129 | Avg: 1.5957 | LR: 1.4e-06\n",
      "üìà Step  6450 | Loss: 1.9407 | Avg: 1.6332 | LR: 1.4e-06\n",
      "üìà Step  6475 | Loss: 1.7041 | Avg: 1.6601 | LR: 1.4e-06\n",
      "üìà Step  6500 | Loss: 2.1796 | Avg: 1.6921 | LR: 1.4e-06\n",
      "üìà Step  6525 | Loss: 1.8340 | Avg: 1.7113 | LR: 1.4e-06\n",
      "üìà Step  6550 | Loss: 2.3873 | Avg: 1.7888 | LR: 1.4e-06\n",
      "üìà Step  6575 | Loss: 1.2946 | Avg: 1.7676 | LR: 1.5e-06\n",
      "üìà Step  6600 | Loss: 1.4783 | Avg: 1.4674 | LR: 1.5e-06\n",
      "üìà Step  6625 | Loss: 1.1239 | Avg: 1.6863 | LR: 1.5e-06\n",
      "üìà Step  6650 | Loss: 1.7823 | Avg: 1.5988 | LR: 1.5e-06\n",
      "üìà Step  6675 | Loss: 2.0184 | Avg: 1.7203 | LR: 1.5e-06\n",
      "üìà Step  6700 | Loss: 1.7917 | Avg: 1.7350 | LR: 1.5e-06\n",
      "üìà Step  6725 | Loss: 2.0323 | Avg: 1.8084 | LR: 1.5e-06\n",
      "üìà Step  6750 | Loss: 2.6797 | Avg: 1.7511 | LR: 1.5e-06\n",
      "üìà Step  6775 | Loss: 1.6079 | Avg: 1.6424 | LR: 1.5e-06\n",
      "üìà Step  6800 | Loss: 1.8322 | Avg: 1.5435 | LR: 1.5e-06\n",
      "üìà Step  6825 | Loss: 1.6300 | Avg: 1.5822 | LR: 1.6e-06\n",
      "üìà Step  6850 | Loss: 1.5365 | Avg: 1.8662 | LR: 1.6e-06\n",
      "üíæ Stable checkpoint saved at 30% of epoch 3\n",
      "üìà Step  6875 | Loss: 1.6403 | Avg: 1.6532 | LR: 1.6e-06\n",
      "üìà Step  6900 | Loss: 0.8051 | Avg: 1.5853 | LR: 1.6e-06\n",
      "üìà Step  6925 | Loss: 1.3568 | Avg: 1.6403 | LR: 1.6e-06\n",
      "üìà Step  6950 | Loss: 0.7281 | Avg: 1.7815 | LR: 1.6e-06\n",
      "üìà Step  6975 | Loss: 1.1664 | Avg: 1.6653 | LR: 1.6e-06\n",
      "üìà Step  7000 | Loss: 0.9879 | Avg: 1.4771 | LR: 1.6e-06\n",
      "üìà Step  7025 | Loss: 1.5312 | Avg: 1.5082 | LR: 1.6e-06\n",
      "üìà Step  7050 | Loss: 2.0208 | Avg: 1.6780 | LR: 1.6e-06\n",
      "üìà Step  7075 | Loss: 1.3827 | Avg: 1.7457 | LR: 1.7e-06\n",
      "üìà Step  7100 | Loss: 2.2015 | Avg: 1.5145 | LR: 1.7e-06\n",
      "üìà Step  7125 | Loss: 1.6267 | Avg: 1.6445 | LR: 1.7e-06\n",
      "üìà Step  7150 | Loss: 1.9092 | Avg: 1.8699 | LR: 1.7e-06\n",
      "üìà Step  7175 | Loss: 1.1400 | Avg: 1.6382 | LR: 1.7e-06\n",
      "üìà Step  7200 | Loss: 1.1764 | Avg: 1.6353 | LR: 1.7e-06\n",
      "üìà Step  7225 | Loss: 2.0226 | Avg: 1.6065 | LR: 1.7e-06\n",
      "üìà Step  7250 | Loss: 1.2401 | Avg: 1.7633 | LR: 1.7e-06\n",
      "üìà Step  7275 | Loss: 1.9394 | Avg: 1.7568 | LR: 1.7e-06\n",
      "üìà Step  7300 | Loss: 2.2064 | Avg: 1.4766 | LR: 1.7e-06\n",
      "üìà Step  7325 | Loss: 2.2611 | Avg: 1.4537 | LR: 1.7e-06\n",
      "üìà Step  7350 | Loss: 1.8819 | Avg: 1.7140 | LR: 1.7e-06\n",
      "üìà Step  7375 | Loss: 1.7466 | Avg: 1.5954 | LR: 1.8e-06\n",
      "üìà Step  7400 | Loss: 1.6816 | Avg: 1.5959 | LR: 1.8e-06\n",
      "üìà Step  7425 | Loss: 1.7463 | Avg: 1.6826 | LR: 1.8e-06\n",
      "üìà Step  7450 | Loss: 1.1860 | Avg: 1.5181 | LR: 1.8e-06\n",
      "üìà Step  7475 | Loss: 2.2943 | Avg: 1.5860 | LR: 1.8e-06\n",
      "üìà Step  7500 | Loss: 1.5010 | Avg: 1.6501 | LR: 1.8e-06\n",
      "üìà Step  7525 | Loss: 0.6890 | Avg: 1.5769 | LR: 1.8e-06\n",
      "üìà Step  7550 | Loss: 2.0967 | Avg: 1.6693 | LR: 1.8e-06\n",
      "üìà Step  7575 | Loss: 1.9327 | Avg: 1.5286 | LR: 1.8e-06\n",
      "üìà Step  7600 | Loss: 1.3915 | Avg: 1.6123 | LR: 1.8e-06\n",
      "üìà Step  7625 | Loss: 0.9148 | Avg: 1.6653 | LR: 1.8e-06\n",
      "üìà Step  7650 | Loss: 1.8229 | Avg: 1.5787 | LR: 1.8e-06\n",
      "üìà Step  7675 | Loss: 1.6058 | Avg: 1.5070 | LR: 1.8e-06\n",
      "üìà Step  7700 | Loss: 2.3319 | Avg: 1.4436 | LR: 1.8e-06\n",
      "üìà Step  7725 | Loss: 0.9036 | Avg: 1.6422 | LR: 1.9e-06\n",
      "üìà Step  7750 | Loss: 1.3733 | Avg: 1.6231 | LR: 1.9e-06\n",
      "üìà Step  7775 | Loss: 0.7789 | Avg: 1.4325 | LR: 1.9e-06\n",
      "üìà Step  7800 | Loss: 2.2231 | Avg: 1.7062 | LR: 1.9e-06\n",
      "üìà Step  7825 | Loss: 1.6086 | Avg: 1.7742 | LR: 1.9e-06\n",
      "üìà Step  7850 | Loss: 1.5290 | Avg: 1.5635 | LR: 1.9e-06\n",
      "üìà Step  7875 | Loss: 2.3363 | Avg: 1.4509 | LR: 1.9e-06\n",
      "üìà Step  7900 | Loss: 1.6255 | Avg: 1.7704 | LR: 1.9e-06\n",
      "üìà Step  7925 | Loss: 1.9707 | Avg: 1.6091 | LR: 1.9e-06\n",
      "üìà Step  7950 | Loss: 1.7731 | Avg: 1.3632 | LR: 1.9e-06\n",
      "üìà Step  7975 | Loss: 1.0342 | Avg: 1.7004 | LR: 1.9e-06\n",
      "üìà Step  8000 | Loss: 1.3806 | Avg: 1.5204 | LR: 1.9e-06\n",
      "üìà Step  8025 | Loss: 1.4574 | Avg: 1.6700 | LR: 1.9e-06\n",
      "üìà Step  8050 | Loss: 1.4634 | Avg: 1.4875 | LR: 1.9e-06\n",
      "üìà Step  8075 | Loss: 0.8172 | Avg: 1.6460 | LR: 1.9e-06\n",
      "üìà Step  8100 | Loss: 1.3785 | Avg: 1.5343 | LR: 1.9e-06\n",
      "üìà Step  8125 | Loss: 1.9164 | Avg: 1.6128 | LR: 1.9e-06\n",
      "üìà Step  8150 | Loss: 1.0652 | Avg: 1.5629 | LR: 1.9e-06\n",
      "üìà Step  8175 | Loss: 1.2079 | Avg: 1.6339 | LR: 1.9e-06\n",
      "üìà Step  8200 | Loss: 1.4075 | Avg: 1.6872 | LR: 1.9e-06\n",
      "üìà Step  8225 | Loss: 1.6811 | Avg: 1.5096 | LR: 2.0e-06\n",
      "üìà Step  8250 | Loss: 1.0222 | Avg: 1.3403 | LR: 2.0e-06\n",
      "üìà Step  8275 | Loss: 1.3643 | Avg: 1.5013 | LR: 2.0e-06\n",
      "üìà Step  8300 | Loss: 1.4878 | Avg: 1.5227 | LR: 2.0e-06\n",
      "üìà Step  8325 | Loss: 1.3649 | Avg: 1.5869 | LR: 2.0e-06\n",
      "üìà Step  8350 | Loss: 1.9410 | Avg: 1.6286 | LR: 2.0e-06\n",
      "üìà Step  8375 | Loss: 2.3481 | Avg: 1.8207 | LR: 2.0e-06\n",
      "üìà Step  8400 | Loss: 1.5102 | Avg: 1.5142 | LR: 2.0e-06\n",
      "üìà Step  8425 | Loss: 1.3061 | Avg: 1.3909 | LR: 2.0e-06\n",
      "üìà Step  8450 | Loss: 1.6237 | Avg: 1.5489 | LR: 2.0e-06\n",
      "üìà Step  8475 | Loss: 0.9644 | Avg: 1.5131 | LR: 2.0e-06\n",
      "üìà Step  8500 | Loss: 2.1299 | Avg: 1.7937 | LR: 2.0e-06\n",
      "üìà Step  8525 | Loss: 1.2699 | Avg: 1.4959 | LR: 2.0e-06\n",
      "üìà Step  8550 | Loss: 2.0566 | Avg: 1.4718 | LR: 2.0e-06\n",
      "üìà Step  8575 | Loss: 1.0722 | Avg: 1.7426 | LR: 2.0e-06\n",
      "üìà Step  8600 | Loss: 1.8642 | Avg: 1.3723 | LR: 2.0e-06\n",
      "üìà Step  8625 | Loss: 2.6189 | Avg: 1.4994 | LR: 2.0e-06\n",
      "üìà Step  8650 | Loss: 1.1185 | Avg: 1.6785 | LR: 2.0e-06\n",
      "üìà Step  8675 | Loss: 0.7296 | Avg: 1.6997 | LR: 2.0e-06\n",
      "üìà Step  8700 | Loss: 1.4366 | Avg: 1.6249 | LR: 2.0e-06\n",
      "üìà Step  8725 | Loss: 0.7329 | Avg: 1.4123 | LR: 2.0e-06\n",
      "üìà Step  8750 | Loss: 1.4205 | Avg: 1.7234 | LR: 2.0e-06\n",
      "üìà Step  8775 | Loss: 0.7263 | Avg: 1.5162 | LR: 2.0e-06\n",
      "üìà Step  8800 | Loss: 1.2808 | Avg: 1.4720 | LR: 2.0e-06\n",
      "üìà Step  8825 | Loss: 1.3212 | Avg: 1.4184 | LR: 2.0e-06\n",
      "üìà Step  8850 | Loss: 2.8808 | Avg: 1.6560 | LR: 2.0e-06\n",
      "üìà Step  8875 | Loss: 1.7218 | Avg: 1.5870 | LR: 2.0e-06\n",
      "üìà Step  8900 | Loss: 1.5338 | Avg: 1.4116 | LR: 2.0e-06\n",
      "üìà Step  8925 | Loss: 0.9484 | Avg: 1.6198 | LR: 2.0e-06\n",
      "üìà Step  8950 | Loss: 1.6479 | Avg: 1.5492 | LR: 2.0e-06\n",
      "üìà Step  8975 | Loss: 2.1206 | Avg: 1.5574 | LR: 2.0e-06\n",
      "üìà Step  9000 | Loss: 1.5098 | Avg: 1.5843 | LR: 2.0e-06\n",
      "üìà Step  9025 | Loss: 1.2198 | Avg: 1.6688 | LR: 2.0e-06\n",
      "üìà Step  9050 | Loss: 1.3581 | Avg: 1.4061 | LR: 2.0e-06\n",
      "üìà Step  9075 | Loss: 1.2244 | Avg: 1.5063 | LR: 2.0e-06\n",
      "üìà Step  9100 | Loss: 2.7455 | Avg: 1.5789 | LR: 2.0e-06\n",
      "üìà Step  9125 | Loss: 1.6637 | Avg: 1.4334 | LR: 2.0e-06\n",
      "üìà Step  9150 | Loss: 2.1904 | Avg: 1.6153 | LR: 2.0e-06\n",
      "üìà Step  9175 | Loss: 0.9293 | Avg: 1.6185 | LR: 2.0e-06\n",
      "üìà Step  9200 | Loss: 1.2036 | Avg: 1.5458 | LR: 2.0e-06\n",
      "üìà Step  9225 | Loss: 2.0448 | Avg: 1.5097 | LR: 2.0e-06\n",
      "üìà Step  9250 | Loss: 2.0444 | Avg: 1.6069 | LR: 2.0e-06\n",
      "üìà Step  9275 | Loss: 1.4322 | Avg: 1.4066 | LR: 2.0e-06\n",
      "üìà Step  9300 | Loss: 3.5062 | Avg: 1.4624 | LR: 2.0e-06\n",
      "üìà Step  9325 | Loss: 1.7289 | Avg: 1.4624 | LR: 2.0e-06\n",
      "üìà Step  9350 | Loss: 1.4512 | Avg: 1.3758 | LR: 2.0e-06\n",
      "üìà Step  9375 | Loss: 2.8189 | Avg: 1.5585 | LR: 2.0e-06\n",
      "üìà Step  9400 | Loss: 1.2845 | Avg: 1.5753 | LR: 2.0e-06\n",
      "üìà Step  9425 | Loss: 1.2393 | Avg: 1.6320 | LR: 2.0e-06\n",
      "üìà Step  9450 | Loss: 1.5653 | Avg: 1.6900 | LR: 2.0e-06\n",
      "üìà Step  9475 | Loss: 1.2959 | Avg: 1.4024 | LR: 2.0e-06\n",
      "üìà Step  9500 | Loss: 1.4449 | Avg: 1.5608 | LR: 2.0e-06\n",
      "üìà Step  9525 | Loss: 2.5894 | Avg: 1.6097 | LR: 2.0e-06\n",
      "üìà Step  9550 | Loss: 1.9993 | Avg: 1.5783 | LR: 1.9e-06\n",
      "üìà Step  9575 | Loss: 1.5556 | Avg: 1.4137 | LR: 1.9e-06\n",
      "üìà Step  9600 | Loss: 1.2114 | Avg: 1.4089 | LR: 1.9e-06\n",
      "üìà Step  9625 | Loss: 2.2555 | Avg: 1.4345 | LR: 1.9e-06\n",
      "üìà Step  9650 | Loss: 2.2438 | Avg: 1.5812 | LR: 1.9e-06\n",
      "üìà Step  9675 | Loss: 2.4002 | Avg: 1.5233 | LR: 1.9e-06\n",
      "üìà Step  9700 | Loss: 1.4251 | Avg: 1.4730 | LR: 1.9e-06\n",
      "üìà Step  9725 | Loss: 2.1008 | Avg: 1.7066 | LR: 1.9e-06\n",
      "üìà Step  9750 | Loss: 1.1308 | Avg: 1.3970 | LR: 1.9e-06\n",
      "üìà Step  9775 | Loss: 0.8810 | Avg: 1.5427 | LR: 1.9e-06\n",
      "üìà Step  9800 | Loss: 1.1703 | Avg: 1.6218 | LR: 1.9e-06\n",
      "üìà Step  9825 | Loss: 1.3558 | Avg: 1.5758 | LR: 1.9e-06\n",
      "üìà Step  9850 | Loss: 0.9212 | Avg: 1.4683 | LR: 1.9e-06\n",
      "üìà Step  9875 | Loss: 0.9552 | Avg: 1.3950 | LR: 1.9e-06\n",
      "üìà Step  9900 | Loss: 1.5445 | Avg: 1.3957 | LR: 1.9e-06\n",
      "üìà Step  9925 | Loss: 2.3190 | Avg: 1.6248 | LR: 1.9e-06\n",
      "üìà Step  9950 | Loss: 1.7432 | Avg: 1.3322 | LR: 1.9e-06\n",
      "üìà Step  9975 | Loss: 1.3908 | Avg: 1.4201 | LR: 1.9e-06\n",
      "üìà Step 10000 | Loss: 0.6607 | Avg: 1.2463 | LR: 1.9e-06\n",
      "üìà Step 10025 | Loss: 1.4394 | Avg: 1.5442 | LR: 1.9e-06\n",
      "üìà Step 10050 | Loss: 2.9089 | Avg: 1.7010 | LR: 1.8e-06\n",
      "üìà Step 10075 | Loss: 2.1322 | Avg: 1.5087 | LR: 1.8e-06\n",
      "üìà Step 10100 | Loss: 1.3053 | Avg: 1.4613 | LR: 1.8e-06\n",
      "üìà Step 10125 | Loss: 1.7507 | Avg: 1.5439 | LR: 1.8e-06\n",
      "üìà Step 10150 | Loss: 1.0783 | Avg: 1.5392 | LR: 1.8e-06\n",
      "üìà Step 10175 | Loss: 0.5827 | Avg: 1.5013 | LR: 1.8e-06\n",
      "üìà Step 10200 | Loss: 1.6427 | Avg: 1.7060 | LR: 1.8e-06\n",
      "üìà Step 10225 | Loss: 1.9512 | Avg: 1.7292 | LR: 1.8e-06\n",
      "üìà Step 10250 | Loss: 0.7853 | Avg: 1.5648 | LR: 1.8e-06\n",
      "üìà Step 10275 | Loss: 1.1987 | Avg: 1.4870 | LR: 1.8e-06\n",
      "üíæ Stable checkpoint saved at 45% of epoch 3\n",
      "üìà Step 10300 | Loss: 1.7214 | Avg: 1.6540 | LR: 1.8e-06\n",
      "üìà Step 10325 | Loss: 1.3543 | Avg: 1.8463 | LR: 1.8e-06\n",
      "üìà Step 10350 | Loss: 0.7158 | Avg: 1.4082 | LR: 1.8e-06\n",
      "üìà Step 10375 | Loss: 1.5407 | Avg: 1.4257 | LR: 1.8e-06\n",
      "üìà Step 10400 | Loss: 1.2000 | Avg: 1.5661 | LR: 1.7e-06\n",
      "üìà Step 10425 | Loss: 1.6603 | Avg: 1.5666 | LR: 1.7e-06\n",
      "üìà Step 10450 | Loss: 1.0475 | Avg: 1.4504 | LR: 1.7e-06\n",
      "üìà Step 10475 | Loss: 1.0990 | Avg: 1.4489 | LR: 1.7e-06\n",
      "üìà Step 10500 | Loss: 2.3822 | Avg: 1.5028 | LR: 1.7e-06\n",
      "üìà Step 10525 | Loss: 1.9567 | Avg: 1.5984 | LR: 1.7e-06\n",
      "üìà Step 10550 | Loss: 1.8984 | Avg: 1.6197 | LR: 1.7e-06\n",
      "üìà Step 10575 | Loss: 1.7635 | Avg: 1.4805 | LR: 1.7e-06\n",
      "üìà Step 10600 | Loss: 2.3431 | Avg: 1.5943 | LR: 1.7e-06\n",
      "üìà Step 10625 | Loss: 1.7703 | Avg: 1.6064 | LR: 1.7e-06\n",
      "üìà Step 10650 | Loss: 1.0723 | Avg: 1.4465 | LR: 1.7e-06\n",
      "üìà Step 10675 | Loss: 1.2399 | Avg: 1.2902 | LR: 1.7e-06\n",
      "üìà Step 10700 | Loss: 1.1496 | Avg: 1.4161 | LR: 1.6e-06\n",
      "üìà Step 10725 | Loss: 1.1882 | Avg: 1.5164 | LR: 1.6e-06\n",
      "üìà Step 10750 | Loss: 0.7048 | Avg: 1.5246 | LR: 1.6e-06\n",
      "üìà Step 10775 | Loss: 0.7247 | Avg: 1.4734 | LR: 1.6e-06\n",
      "üìà Step 10800 | Loss: 1.3241 | Avg: 1.3787 | LR: 1.6e-06\n",
      "üìà Step 10825 | Loss: 1.6308 | Avg: 1.2528 | LR: 1.6e-06\n",
      "üìà Step 10850 | Loss: 1.3902 | Avg: 1.5316 | LR: 1.6e-06\n",
      "üìà Step 10875 | Loss: 1.5813 | Avg: 1.6523 | LR: 1.6e-06\n",
      "üìà Step 10900 | Loss: 1.9854 | Avg: 1.4987 | LR: 1.6e-06\n",
      "üìà Step 10925 | Loss: 1.4528 | Avg: 1.5314 | LR: 1.6e-06\n",
      "üìà Step 10950 | Loss: 1.3956 | Avg: 1.5133 | LR: 1.5e-06\n",
      "üìà Step 10975 | Loss: 1.0598 | Avg: 1.5206 | LR: 1.5e-06\n",
      "üìà Step 11000 | Loss: 1.5162 | Avg: 1.5585 | LR: 1.5e-06\n",
      "üìà Step 11025 | Loss: 0.6706 | Avg: 1.5288 | LR: 1.5e-06\n",
      "üìà Step 11050 | Loss: 1.4026 | Avg: 1.4157 | LR: 1.5e-06\n",
      "üìà Step 11075 | Loss: 1.5833 | Avg: 1.5234 | LR: 1.5e-06\n",
      "üìà Step 11100 | Loss: 1.7821 | Avg: 1.4706 | LR: 1.5e-06\n",
      "üìà Step 11125 | Loss: 1.3444 | Avg: 1.2128 | LR: 1.5e-06\n",
      "üìà Step 11150 | Loss: 2.3061 | Avg: 1.5391 | LR: 1.5e-06\n",
      "üìà Step 11175 | Loss: 1.3244 | Avg: 1.3732 | LR: 1.5e-06\n",
      "üìà Step 11200 | Loss: 1.0213 | Avg: 1.4529 | LR: 1.4e-06\n",
      "üìà Step 11225 | Loss: 0.8803 | Avg: 1.4622 | LR: 1.4e-06\n",
      "üìà Step 11250 | Loss: 1.8670 | Avg: 1.3580 | LR: 1.4e-06\n",
      "üìà Step 11275 | Loss: 1.0801 | Avg: 1.4274 | LR: 1.4e-06\n",
      "üìà Step 11300 | Loss: 1.8854 | Avg: 1.5044 | LR: 1.4e-06\n",
      "üìà Step 11325 | Loss: 1.7295 | Avg: 1.5800 | LR: 1.4e-06\n",
      "üìà Step 11350 | Loss: 1.3174 | Avg: 1.4698 | LR: 1.4e-06\n",
      "üìà Step 11375 | Loss: 1.1121 | Avg: 1.4169 | LR: 1.4e-06\n",
      "üìà Step 11400 | Loss: 1.3079 | Avg: 1.3975 | LR: 1.4e-06\n",
      "üìà Step 11425 | Loss: 1.1114 | Avg: 1.4014 | LR: 1.3e-06\n",
      "üìà Step 11450 | Loss: 1.9166 | Avg: 1.3900 | LR: 1.3e-06\n",
      "üìà Step 11475 | Loss: 3.3179 | Avg: 1.4547 | LR: 1.3e-06\n",
      "üìà Step 11500 | Loss: 1.3552 | Avg: 1.5497 | LR: 1.3e-06\n",
      "üìà Step 11525 | Loss: 2.7817 | Avg: 1.5201 | LR: 1.3e-06\n",
      "üìà Step 11550 | Loss: 1.3284 | Avg: 1.2962 | LR: 1.3e-06\n",
      "üìà Step 11575 | Loss: 1.5491 | Avg: 1.4126 | LR: 1.3e-06\n",
      "üìà Step 11600 | Loss: 2.6051 | Avg: 1.5444 | LR: 1.3e-06\n",
      "üìà Step 11625 | Loss: 1.3402 | Avg: 1.4188 | LR: 1.3e-06\n",
      "üìà Step 11650 | Loss: 0.9124 | Avg: 1.4734 | LR: 1.2e-06\n",
      "üìà Step 11675 | Loss: 1.5340 | Avg: 1.4238 | LR: 1.2e-06\n",
      "üìà Step 11700 | Loss: 2.3319 | Avg: 1.5989 | LR: 1.2e-06\n",
      "üìà Step 11725 | Loss: 1.5819 | Avg: 1.5781 | LR: 1.2e-06\n",
      "üìà Step 11750 | Loss: 0.4451 | Avg: 1.2956 | LR: 1.2e-06\n",
      "üìà Step 11775 | Loss: 0.8798 | Avg: 1.5412 | LR: 1.2e-06\n",
      "üìà Step 11800 | Loss: 1.0697 | Avg: 1.4716 | LR: 1.2e-06\n",
      "üìà Step 11825 | Loss: 2.4383 | Avg: 1.6582 | LR: 1.2e-06\n",
      "üìà Step 11850 | Loss: 0.9964 | Avg: 1.3182 | LR: 1.1e-06\n",
      "üìà Step 11875 | Loss: 1.4867 | Avg: 1.3447 | LR: 1.1e-06\n",
      "üìà Step 11900 | Loss: 1.9877 | Avg: 1.4805 | LR: 1.1e-06\n",
      "üìà Step 11925 | Loss: 1.3418 | Avg: 1.3833 | LR: 1.1e-06\n",
      "üìà Step 11950 | Loss: 0.9947 | Avg: 1.7250 | LR: 1.1e-06\n",
      "üìà Step 11975 | Loss: 1.0324 | Avg: 1.3913 | LR: 1.1e-06\n",
      "üìà Step 12000 | Loss: 2.3377 | Avg: 1.5987 | LR: 1.1e-06\n",
      "üìà Step 12025 | Loss: 2.2030 | Avg: 1.5296 | LR: 1.1e-06\n",
      "üìà Step 12050 | Loss: 2.0368 | Avg: 1.4778 | LR: 1.1e-06\n",
      "üìà Step 12075 | Loss: 0.6948 | Avg: 1.3897 | LR: 1.0e-06\n",
      "üìà Step 12100 | Loss: 1.7448 | Avg: 1.5213 | LR: 1.0e-06\n",
      "üìà Step 12125 | Loss: 1.5603 | Avg: 1.4355 | LR: 1.0e-06\n",
      "üìà Step 12150 | Loss: 2.2345 | Avg: 1.4681 | LR: 1.0e-06\n",
      "üìà Step 12175 | Loss: 1.7294 | Avg: 1.5294 | LR: 9.9e-07\n",
      "üìà Step 12200 | Loss: 2.4429 | Avg: 1.6321 | LR: 9.8e-07\n",
      "üìà Step 12225 | Loss: 1.1194 | Avg: 1.5351 | LR: 9.7e-07\n",
      "üìà Step 12250 | Loss: 1.9090 | Avg: 1.3620 | LR: 9.6e-07\n",
      "üìà Step 12275 | Loss: 0.5735 | Avg: 1.4169 | LR: 9.4e-07\n",
      "üìà Step 12300 | Loss: 1.2043 | Avg: 1.4559 | LR: 9.3e-07\n",
      "üìà Step 12325 | Loss: 1.9385 | Avg: 1.3286 | LR: 9.2e-07\n",
      "üìà Step 12350 | Loss: 2.2688 | Avg: 1.4090 | LR: 9.1e-07\n",
      "üìà Step 12375 | Loss: 1.6311 | Avg: 1.3904 | LR: 9.0e-07\n",
      "üìà Step 12400 | Loss: 1.3393 | Avg: 1.5768 | LR: 8.8e-07\n",
      "üìà Step 12425 | Loss: 1.1526 | Avg: 1.5717 | LR: 8.7e-07\n",
      "üìà Step 12450 | Loss: 1.4835 | Avg: 1.4728 | LR: 8.6e-07\n",
      "üìà Step 12475 | Loss: 0.8463 | Avg: 1.5034 | LR: 8.5e-07\n",
      "üìà Step 12500 | Loss: 1.3503 | Avg: 1.1696 | LR: 8.4e-07\n",
      "üìà Step 12525 | Loss: 1.5088 | Avg: 1.6421 | LR: 8.3e-07\n",
      "üìà Step 12550 | Loss: 0.5707 | Avg: 1.2388 | LR: 8.1e-07\n",
      "üìà Step 12575 | Loss: 0.5841 | Avg: 1.4907 | LR: 8.0e-07\n",
      "üìà Step 12600 | Loss: 0.8290 | Avg: 1.2558 | LR: 7.9e-07\n",
      "üìà Step 12625 | Loss: 2.9593 | Avg: 1.4900 | LR: 7.8e-07\n",
      "üìà Step 12650 | Loss: 1.6075 | Avg: 1.4268 | LR: 7.7e-07\n",
      "üìà Step 12675 | Loss: 1.4575 | Avg: 1.4998 | LR: 7.5e-07\n",
      "üìà Step 12700 | Loss: 1.4593 | Avg: 1.5096 | LR: 7.4e-07\n",
      "üìà Step 12725 | Loss: 1.3661 | Avg: 1.4753 | LR: 7.3e-07\n",
      "üìà Step 12750 | Loss: 0.7597 | Avg: 1.3742 | LR: 7.2e-07\n",
      "üìà Step 12775 | Loss: 0.6914 | Avg: 1.5547 | LR: 7.1e-07\n",
      "üìà Step 12800 | Loss: 1.2895 | Avg: 1.6842 | LR: 7.0e-07\n",
      "üìà Step 12825 | Loss: 1.0284 | Avg: 1.4127 | LR: 6.9e-07\n",
      "üìà Step 12850 | Loss: 0.7196 | Avg: 1.5288 | LR: 6.7e-07\n",
      "üìà Step 12875 | Loss: 1.5923 | Avg: 1.4381 | LR: 6.6e-07\n",
      "üìà Step 12900 | Loss: 1.0406 | Avg: 1.5771 | LR: 6.5e-07\n",
      "üìà Step 12925 | Loss: 2.0338 | Avg: 1.3970 | LR: 6.4e-07\n",
      "üìà Step 12950 | Loss: 1.5469 | Avg: 1.4124 | LR: 6.3e-07\n",
      "üìà Step 12975 | Loss: 2.0067 | Avg: 1.4736 | LR: 6.2e-07\n",
      "üìà Step 13000 | Loss: 0.6357 | Avg: 1.3243 | LR: 6.1e-07\n",
      "üìà Step 13025 | Loss: 1.4676 | Avg: 1.5281 | LR: 6.0e-07\n",
      "üìà Step 13050 | Loss: 1.0035 | Avg: 1.2676 | LR: 5.9e-07\n",
      "üìà Step 13075 | Loss: 0.9066 | Avg: 1.3883 | LR: 5.8e-07\n",
      "üìà Step 13100 | Loss: 0.8010 | Avg: 1.5090 | LR: 5.6e-07\n",
      "üìà Step 13125 | Loss: 1.4077 | Avg: 1.4056 | LR: 5.5e-07\n",
      "üìà Step 13150 | Loss: 1.5326 | Avg: 1.4099 | LR: 5.4e-07\n",
      "üìà Step 13175 | Loss: 2.8495 | Avg: 1.3601 | LR: 5.3e-07\n",
      "üìà Step 13200 | Loss: 2.0450 | Avg: 1.3947 | LR: 5.2e-07\n",
      "üìà Step 13225 | Loss: 1.2031 | Avg: 1.5575 | LR: 5.1e-07\n",
      "üìà Step 13250 | Loss: 2.9749 | Avg: 1.4405 | LR: 5.0e-07\n",
      "üìà Step 13275 | Loss: 1.5393 | Avg: 1.3615 | LR: 4.9e-07\n",
      "üìà Step 13300 | Loss: 1.1408 | Avg: 1.4403 | LR: 4.8e-07\n",
      "üìà Step 13325 | Loss: 2.4017 | Avg: 1.5442 | LR: 4.7e-07\n",
      "üìà Step 13350 | Loss: 1.9155 | Avg: 1.4966 | LR: 4.6e-07\n",
      "üìà Step 13375 | Loss: 1.9634 | Avg: 1.5606 | LR: 4.5e-07\n",
      "üìà Step 13400 | Loss: 0.9799 | Avg: 1.3367 | LR: 4.4e-07\n",
      "üìà Step 13425 | Loss: 1.8723 | Avg: 1.3774 | LR: 4.3e-07\n",
      "üìà Step 13450 | Loss: 1.0286 | Avg: 1.2756 | LR: 4.2e-07\n",
      "üìà Step 13475 | Loss: 1.2533 | Avg: 1.2554 | LR: 4.1e-07\n",
      "üìà Step 13500 | Loss: 0.5255 | Avg: 1.5341 | LR: 4.0e-07\n",
      "üìà Step 13525 | Loss: 1.9396 | Avg: 1.4078 | LR: 3.9e-07\n",
      "üìà Step 13550 | Loss: 0.5473 | Avg: 1.3569 | LR: 3.8e-07\n",
      "üìà Step 13575 | Loss: 1.2368 | Avg: 1.4124 | LR: 3.7e-07\n",
      "üìà Step 13600 | Loss: 1.1069 | Avg: 1.4749 | LR: 3.6e-07\n",
      "üìà Step 13625 | Loss: 0.7579 | Avg: 1.3525 | LR: 3.5e-07\n",
      "üìà Step 13650 | Loss: 1.6408 | Avg: 1.3902 | LR: 3.5e-07\n",
      "üìà Step 13675 | Loss: 1.4122 | Avg: 1.6638 | LR: 3.4e-07\n",
      "üìà Step 13700 | Loss: 1.2021 | Avg: 1.4382 | LR: 3.3e-07\n",
      "üíæ Stable checkpoint saved at 60% of epoch 3\n",
      "üìà Step 13725 | Loss: 1.8289 | Avg: 1.4043 | LR: 3.2e-07\n",
      "üìà Step 13750 | Loss: 1.5938 | Avg: 1.3791 | LR: 3.1e-07\n",
      "üìà Step 13775 | Loss: 1.7036 | Avg: 1.4570 | LR: 3.0e-07\n",
      "üìà Step 13800 | Loss: 0.8323 | Avg: 1.3069 | LR: 2.9e-07\n",
      "üìà Step 13825 | Loss: 0.7564 | Avg: 1.3737 | LR: 2.8e-07\n",
      "üìà Step 13850 | Loss: 1.4596 | Avg: 1.2134 | LR: 2.8e-07\n",
      "üìà Step 13875 | Loss: 0.5947 | Avg: 1.3177 | LR: 2.7e-07\n",
      "üìà Step 13900 | Loss: 1.9339 | Avg: 1.3453 | LR: 2.6e-07\n",
      "üìà Step 13925 | Loss: 1.0632 | Avg: 1.5041 | LR: 2.5e-07\n",
      "üìà Step 13950 | Loss: 1.2433 | Avg: 1.6219 | LR: 2.4e-07\n",
      "üìà Step 13975 | Loss: 1.3669 | Avg: 1.4396 | LR: 2.4e-07\n",
      "üìà Step 14000 | Loss: 1.2459 | Avg: 1.2574 | LR: 2.3e-07\n",
      "üìà Step 14025 | Loss: 1.9960 | Avg: 1.3606 | LR: 2.2e-07\n",
      "üìà Step 14050 | Loss: 2.0346 | Avg: 1.3156 | LR: 2.1e-07\n",
      "üìà Step 14075 | Loss: 2.0526 | Avg: 1.4499 | LR: 2.1e-07\n",
      "üìà Step 14100 | Loss: 1.6298 | Avg: 1.5069 | LR: 2.0e-07\n",
      "üìà Step 14125 | Loss: 1.2284 | Avg: 1.3834 | LR: 1.9e-07\n",
      "üìà Step 14150 | Loss: 0.9419 | Avg: 1.4297 | LR: 1.9e-07\n",
      "üìà Step 14175 | Loss: 1.2126 | Avg: 1.6268 | LR: 1.8e-07\n",
      "üìà Step 14200 | Loss: 2.8309 | Avg: 1.4958 | LR: 1.7e-07\n",
      "üìà Step 14225 | Loss: 1.0537 | Avg: 1.5190 | LR: 1.6e-07\n",
      "üìà Step 14250 | Loss: 1.7411 | Avg: 1.1111 | LR: 1.6e-07\n",
      "üìà Step 14275 | Loss: 0.9936 | Avg: 1.3990 | LR: 1.5e-07\n",
      "üìà Step 14300 | Loss: 0.6383 | Avg: 1.4899 | LR: 1.5e-07\n",
      "üìà Step 14325 | Loss: 2.4179 | Avg: 1.4006 | LR: 1.4e-07\n",
      "üìà Step 14350 | Loss: 1.4603 | Avg: 1.5033 | LR: 1.3e-07\n",
      "üìà Step 14375 | Loss: 1.6392 | Avg: 1.4128 | LR: 1.3e-07\n",
      "üìà Step 14400 | Loss: 2.5204 | Avg: 1.3648 | LR: 1.2e-07\n",
      "üìà Step 14425 | Loss: 1.6167 | Avg: 1.5280 | LR: 1.2e-07\n",
      "üìà Step 14450 | Loss: 1.0302 | Avg: 1.3788 | LR: 1.1e-07\n",
      "üìà Step 14475 | Loss: 1.1040 | Avg: 1.4439 | LR: 1.1e-07\n",
      "üìà Step 14500 | Loss: 1.0584 | Avg: 1.4538 | LR: 1.0e-07\n",
      "üìà Step 14525 | Loss: 1.0750 | Avg: 1.3078 | LR: 9.5e-08\n",
      "üìà Step 14550 | Loss: 1.3372 | Avg: 1.5354 | LR: 9.0e-08\n",
      "üìà Step 14575 | Loss: 0.8068 | Avg: 1.4963 | LR: 8.5e-08\n",
      "üìà Step 14600 | Loss: 2.0444 | Avg: 1.4358 | LR: 8.0e-08\n",
      "üìà Step 14625 | Loss: 1.8538 | Avg: 1.5257 | LR: 7.6e-08\n",
      "üìà Step 14650 | Loss: 0.9241 | Avg: 1.3010 | LR: 7.1e-08\n",
      "üìà Step 14675 | Loss: 1.1954 | Avg: 1.4450 | LR: 6.7e-08\n",
      "üìà Step 14700 | Loss: 1.2512 | Avg: 1.3996 | LR: 6.3e-08\n",
      "üìà Step 14725 | Loss: 1.2641 | Avg: 1.5894 | LR: 5.8e-08\n",
      "üìà Step 14750 | Loss: 0.6616 | Avg: 1.4096 | LR: 5.4e-08\n",
      "üìà Step 14775 | Loss: 1.1517 | Avg: 1.3638 | LR: 5.1e-08\n",
      "üìà Step 14800 | Loss: 1.5842 | Avg: 1.1712 | LR: 4.7e-08\n",
      "üìà Step 14825 | Loss: 0.9145 | Avg: 1.5349 | LR: 4.3e-08\n",
      "üìà Step 14850 | Loss: 1.3825 | Avg: 1.4018 | LR: 4.0e-08\n",
      "üìà Step 14875 | Loss: 1.9474 | Avg: 1.4378 | LR: 3.7e-08\n",
      "üìà Step 14900 | Loss: 2.2812 | Avg: 1.6142 | LR: 3.4e-08\n",
      "üìà Step 14925 | Loss: 1.0916 | Avg: 1.3534 | LR: 3.1e-08\n",
      "üìà Step 14950 | Loss: 1.4574 | Avg: 1.3163 | LR: 2.8e-08\n",
      "üìà Step 14975 | Loss: 1.5057 | Avg: 1.4411 | LR: 2.5e-08\n",
      "üìà Step 15000 | Loss: 1.6400 | Avg: 1.5287 | LR: 2.2e-08\n",
      "üìà Step 15025 | Loss: 1.1502 | Avg: 1.4372 | LR: 2.0e-08\n",
      "üìà Step 15050 | Loss: 0.6469 | Avg: 1.2886 | LR: 1.8e-08\n",
      "üìà Step 15075 | Loss: 2.0419 | Avg: 1.3836 | LR: 1.6e-08\n",
      "üìà Step 15100 | Loss: 1.5625 | Avg: 1.2572 | LR: 1.3e-08\n",
      "üìà Step 15125 | Loss: 1.4736 | Avg: 1.6445 | LR: 1.2e-08\n",
      "üìà Step 15150 | Loss: 1.1924 | Avg: 1.4042 | LR: 9.9e-09\n",
      "üìà Step 15175 | Loss: 1.9494 | Avg: 1.6096 | LR: 8.3e-09\n",
      "üìà Step 15200 | Loss: 0.6839 | Avg: 1.4395 | LR: 6.8e-09\n",
      "üìà Step 15225 | Loss: 1.4978 | Avg: 1.2930 | LR: 5.5e-09\n",
      "üìà Step 15250 | Loss: 1.4488 | Avg: 1.4639 | LR: 4.3e-09\n",
      "üìà Step 15275 | Loss: 1.6860 | Avg: 1.3970 | LR: 3.3e-09\n",
      "üìà Step 15300 | Loss: 1.7373 | Avg: 1.5760 | LR: 2.4e-09\n",
      "üìà Step 15325 | Loss: 1.2766 | Avg: 1.4224 | LR: 1.6e-09\n",
      "üìà Step 15350 | Loss: 1.0135 | Avg: 1.4074 | LR: 1.0e-09\n",
      "üìà Step 15375 | Loss: 1.6079 | Avg: 1.4154 | LR: 5.4e-10\n",
      "üìà Step 15400 | Loss: 1.8741 | Avg: 1.4376 | LR: 2.2e-10\n",
      "üìà Step 15425 | Loss: 1.5375 | Avg: 1.4981 | LR: 4.1e-11\n",
      "üìà Step 15450 | Loss: 1.3332 | Avg: 1.5697 | LR: 4.1e-12\n",
      "üìà Step 15475 | Loss: 2.4295 | Avg: 1.5273 | LR: 1.1e-10\n",
      "üìà Step 15500 | Loss: 1.2535 | Avg: 1.3160 | LR: 3.6e-10\n",
      "üìà Step 15525 | Loss: 2.0434 | Avg: 1.4660 | LR: 7.5e-10\n",
      "üìà Step 15550 | Loss: 1.1649 | Avg: 1.3451 | LR: 1.3e-09\n",
      "üìà Step 15575 | Loss: 2.3698 | Avg: 1.5223 | LR: 2.0e-09\n",
      "üìà Step 15600 | Loss: 1.2743 | Avg: 1.4680 | LR: 2.8e-09\n",
      "üìà Step 15625 | Loss: 0.5322 | Avg: 1.4257 | LR: 3.7e-09\n",
      "üìà Step 15650 | Loss: 0.9071 | Avg: 1.4801 | LR: 4.8e-09\n",
      "üìà Step 15675 | Loss: 1.3144 | Avg: 1.3584 | LR: 6.1e-09\n",
      "üìà Step 15700 | Loss: 1.2318 | Avg: 1.3703 | LR: 7.5e-09\n",
      "üìà Step 15725 | Loss: 1.3353 | Avg: 1.3369 | LR: 9.0e-09\n",
      "üìà Step 15750 | Loss: 1.0906 | Avg: 1.5194 | LR: 1.1e-08\n",
      "üìà Step 15775 | Loss: 1.0610 | Avg: 1.2114 | LR: 1.2e-08\n",
      "üìà Step 15800 | Loss: 1.1804 | Avg: 1.3412 | LR: 1.4e-08\n",
      "üìà Step 15825 | Loss: 1.4389 | Avg: 1.2284 | LR: 1.7e-08\n",
      "üìà Step 15850 | Loss: 1.2503 | Avg: 1.4641 | LR: 1.9e-08\n",
      "üìà Step 15875 | Loss: 2.0246 | Avg: 1.5504 | LR: 2.1e-08\n",
      "üìà Step 15900 | Loss: 1.3845 | Avg: 1.6895 | LR: 2.4e-08\n",
      "üìà Step 15925 | Loss: 1.2558 | Avg: 1.2901 | LR: 2.6e-08\n",
      "üìà Step 15950 | Loss: 0.8137 | Avg: 1.5623 | LR: 2.9e-08\n",
      "üìà Step 15975 | Loss: 1.4827 | Avg: 1.3550 | LR: 3.2e-08\n",
      "üìà Step 16000 | Loss: 1.3732 | Avg: 1.2054 | LR: 3.5e-08\n",
      "üìà Step 16025 | Loss: 0.8781 | Avg: 1.3411 | LR: 3.8e-08\n",
      "üìà Step 16050 | Loss: 0.9062 | Avg: 1.4320 | LR: 4.2e-08\n",
      "üìà Step 16075 | Loss: 1.5579 | Avg: 1.4469 | LR: 4.5e-08\n",
      "üìà Step 16100 | Loss: 1.0291 | Avg: 1.6365 | LR: 4.9e-08\n",
      "üìà Step 16125 | Loss: 1.7129 | Avg: 1.3144 | LR: 5.2e-08\n",
      "üìà Step 16150 | Loss: 1.5622 | Avg: 1.7845 | LR: 5.6e-08\n",
      "üìà Step 16175 | Loss: 1.6426 | Avg: 1.4788 | LR: 6.0e-08\n",
      "üìà Step 16200 | Loss: 0.5818 | Avg: 1.2789 | LR: 6.5e-08\n",
      "üìà Step 16225 | Loss: 1.1428 | Avg: 1.4725 | LR: 6.9e-08\n",
      "üìà Step 16250 | Loss: 1.0394 | Avg: 1.4880 | LR: 7.3e-08\n",
      "üìà Step 16275 | Loss: 1.4075 | Avg: 1.5961 | LR: 7.8e-08\n",
      "üìà Step 16300 | Loss: 2.1758 | Avg: 1.5651 | LR: 8.3e-08\n",
      "üìà Step 16325 | Loss: 2.7817 | Avg: 1.4502 | LR: 8.7e-08\n",
      "üìà Step 16350 | Loss: 2.0474 | Avg: 1.2681 | LR: 9.2e-08\n",
      "üìà Step 16375 | Loss: 1.0064 | Avg: 1.3240 | LR: 9.7e-08\n",
      "üìà Step 16400 | Loss: 1.4816 | Avg: 1.4977 | LR: 1.0e-07\n",
      "üìà Step 16425 | Loss: 0.8341 | Avg: 1.4586 | LR: 1.1e-07\n",
      "üìà Step 16450 | Loss: 1.4317 | Avg: 1.3241 | LR: 1.1e-07\n",
      "üìà Step 16475 | Loss: 1.3654 | Avg: 1.6654 | LR: 1.2e-07\n",
      "üìà Step 16500 | Loss: 2.1389 | Avg: 1.6011 | LR: 1.2e-07\n",
      "üìà Step 16525 | Loss: 0.5480 | Avg: 1.5076 | LR: 1.3e-07\n",
      "üìà Step 16550 | Loss: 1.2511 | Avg: 1.3138 | LR: 1.4e-07\n",
      "üìà Step 16575 | Loss: 1.4508 | Avg: 1.3636 | LR: 1.4e-07\n",
      "üìà Step 16600 | Loss: 1.7668 | Avg: 1.3081 | LR: 1.5e-07\n",
      "üìà Step 16625 | Loss: 1.6255 | Avg: 1.2985 | LR: 1.6e-07\n",
      "üìà Step 16650 | Loss: 0.6218 | Avg: 1.4696 | LR: 1.6e-07\n",
      "üìà Step 16675 | Loss: 1.7782 | Avg: 1.4606 | LR: 1.7e-07\n",
      "üìà Step 16700 | Loss: 1.0663 | Avg: 1.3561 | LR: 1.7e-07\n",
      "üìà Step 16725 | Loss: 1.6839 | Avg: 1.4746 | LR: 1.8e-07\n",
      "üìà Step 16750 | Loss: 3.1568 | Avg: 1.3521 | LR: 1.9e-07\n",
      "üìà Step 16775 | Loss: 0.8083 | Avg: 1.4868 | LR: 2.0e-07\n",
      "üìà Step 16800 | Loss: 0.9630 | Avg: 1.4153 | LR: 2.0e-07\n",
      "üìà Step 16825 | Loss: 1.3186 | Avg: 1.2829 | LR: 2.1e-07\n",
      "üìà Step 16850 | Loss: 0.8816 | Avg: 1.4654 | LR: 2.2e-07\n",
      "üìà Step 16875 | Loss: 2.4343 | Avg: 1.5981 | LR: 2.2e-07\n",
      "üìà Step 16900 | Loss: 1.0831 | Avg: 1.3538 | LR: 2.3e-07\n",
      "üìà Step 16925 | Loss: 1.2876 | Avg: 1.3006 | LR: 2.4e-07\n",
      "üìà Step 16950 | Loss: 1.6276 | Avg: 1.4652 | LR: 2.5e-07\n",
      "üìà Step 16975 | Loss: 0.8534 | Avg: 1.5723 | LR: 2.6e-07\n",
      "üìà Step 17000 | Loss: 1.9514 | Avg: 1.7578 | LR: 2.6e-07\n",
      "üìà Step 17025 | Loss: 0.9371 | Avg: 1.3185 | LR: 2.7e-07\n",
      "üìà Step 17050 | Loss: 1.4638 | Avg: 1.3112 | LR: 2.8e-07\n",
      "üìà Step 17075 | Loss: 0.9470 | Avg: 1.5646 | LR: 2.9e-07\n",
      "üìà Step 17100 | Loss: 0.7059 | Avg: 1.1875 | LR: 3.0e-07\n",
      "üìà Step 17125 | Loss: 1.1429 | Avg: 1.4280 | LR: 3.1e-07\n",
      "üíæ Stable checkpoint saved at 75% of epoch 3\n",
      "üìà Step 17150 | Loss: 1.0708 | Avg: 1.3922 | LR: 3.1e-07\n",
      "üìà Step 17175 | Loss: 1.1626 | Avg: 1.4446 | LR: 3.2e-07\n",
      "üìà Step 17200 | Loss: 1.7056 | Avg: 1.4928 | LR: 3.3e-07\n",
      "üìà Step 17225 | Loss: 1.2082 | Avg: 1.3255 | LR: 3.4e-07\n",
      "üìà Step 17250 | Loss: 0.7386 | Avg: 1.3135 | LR: 3.5e-07\n",
      "üìà Step 17275 | Loss: 1.0212 | Avg: 1.3190 | LR: 3.6e-07\n",
      "üìà Step 17300 | Loss: 2.1722 | Avg: 1.5532 | LR: 3.7e-07\n",
      "üìà Step 17325 | Loss: 1.0652 | Avg: 1.4185 | LR: 3.8e-07\n",
      "üìà Step 17350 | Loss: 0.9466 | Avg: 1.3852 | LR: 3.9e-07\n",
      "üìà Step 17375 | Loss: 1.7407 | Avg: 1.4953 | LR: 4.0e-07\n",
      "üìà Step 17400 | Loss: 1.5241 | Avg: 1.4937 | LR: 4.1e-07\n",
      "üìà Step 17425 | Loss: 1.3149 | Avg: 1.5662 | LR: 4.2e-07\n",
      "üìà Step 17450 | Loss: 1.4618 | Avg: 1.4230 | LR: 4.3e-07\n",
      "üìà Step 17475 | Loss: 1.0923 | Avg: 1.3063 | LR: 4.4e-07\n",
      "üìà Step 17500 | Loss: 0.9857 | Avg: 1.4215 | LR: 4.5e-07\n",
      "üìà Step 17525 | Loss: 0.9198 | Avg: 1.6653 | LR: 4.6e-07\n",
      "üìà Step 17550 | Loss: 1.6367 | Avg: 1.3945 | LR: 4.7e-07\n",
      "üìà Step 17575 | Loss: 1.4293 | Avg: 1.3850 | LR: 4.8e-07\n",
      "üìà Step 17600 | Loss: 1.5941 | Avg: 1.4199 | LR: 4.9e-07\n",
      "üìà Step 17625 | Loss: 0.9742 | Avg: 1.3500 | LR: 5.0e-07\n",
      "üìà Step 17650 | Loss: 0.8247 | Avg: 1.5025 | LR: 5.1e-07\n",
      "üìà Step 17675 | Loss: 1.1501 | Avg: 1.4004 | LR: 5.2e-07\n",
      "üìà Step 17700 | Loss: 2.2222 | Avg: 1.4347 | LR: 5.3e-07\n",
      "üìà Step 17725 | Loss: 1.9428 | Avg: 1.5254 | LR: 5.4e-07\n",
      "üìà Step 17750 | Loss: 1.6138 | Avg: 1.3726 | LR: 5.5e-07\n",
      "üìà Step 17775 | Loss: 2.1223 | Avg: 1.3864 | LR: 5.6e-07\n",
      "üìà Step 17800 | Loss: 1.1829 | Avg: 1.3198 | LR: 5.7e-07\n",
      "üìà Step 17825 | Loss: 0.9781 | Avg: 1.4180 | LR: 5.8e-07\n",
      "üìà Step 17850 | Loss: 0.8293 | Avg: 1.5488 | LR: 5.9e-07\n",
      "üìà Step 17875 | Loss: 1.9742 | Avg: 1.4293 | LR: 6.0e-07\n",
      "üìà Step 17900 | Loss: 0.9098 | Avg: 1.4084 | LR: 6.1e-07\n",
      "üìà Step 17925 | Loss: 1.1774 | Avg: 1.5383 | LR: 6.2e-07\n",
      "üìà Step 17950 | Loss: 1.1007 | Avg: 1.3890 | LR: 6.4e-07\n",
      "üìà Step 17975 | Loss: 1.5060 | Avg: 1.5134 | LR: 6.5e-07\n",
      "üìà Step 18000 | Loss: 1.8221 | Avg: 1.4077 | LR: 6.6e-07\n",
      "üìà Step 18025 | Loss: 1.2232 | Avg: 1.4479 | LR: 6.7e-07\n",
      "üìà Step 18050 | Loss: 1.9052 | Avg: 1.4569 | LR: 6.8e-07\n",
      "üìà Step 18075 | Loss: 2.3900 | Avg: 1.4566 | LR: 6.9e-07\n",
      "üìà Step 18100 | Loss: 0.8839 | Avg: 1.4371 | LR: 7.0e-07\n",
      "üìà Step 18125 | Loss: 1.2371 | Avg: 1.3281 | LR: 7.1e-07\n",
      "üìà Step 18150 | Loss: 1.8725 | Avg: 1.1679 | LR: 7.3e-07\n",
      "üìà Step 18175 | Loss: 2.1742 | Avg: 1.5352 | LR: 7.4e-07\n",
      "üìà Step 18200 | Loss: 0.4669 | Avg: 1.4902 | LR: 7.5e-07\n",
      "üìà Step 18225 | Loss: 1.5232 | Avg: 1.4103 | LR: 7.6e-07\n",
      "üìà Step 18250 | Loss: 2.0017 | Avg: 1.4582 | LR: 7.7e-07\n",
      "üìà Step 18275 | Loss: 1.1634 | Avg: 1.2318 | LR: 7.8e-07\n",
      "üìà Step 18300 | Loss: 1.1957 | Avg: 1.4800 | LR: 8.0e-07\n",
      "üìà Step 18325 | Loss: 1.2976 | Avg: 1.6359 | LR: 8.1e-07\n",
      "üìà Step 18350 | Loss: 1.7491 | Avg: 1.3715 | LR: 8.2e-07\n",
      "üìà Step 18375 | Loss: 1.4627 | Avg: 1.4465 | LR: 8.3e-07\n",
      "üìà Step 18400 | Loss: 2.0701 | Avg: 1.3169 | LR: 8.4e-07\n",
      "üìà Step 18425 | Loss: 1.1633 | Avg: 1.4316 | LR: 8.5e-07\n",
      "üìà Step 18450 | Loss: 0.8790 | Avg: 1.6236 | LR: 8.7e-07\n",
      "üìà Step 18475 | Loss: 1.9144 | Avg: 1.5812 | LR: 8.8e-07\n",
      "üìà Step 18500 | Loss: 2.2171 | Avg: 1.6634 | LR: 8.9e-07\n",
      "üìà Step 18525 | Loss: 1.1255 | Avg: 1.2391 | LR: 9.0e-07\n",
      "üìà Step 18550 | Loss: 2.4243 | Avg: 1.5644 | LR: 9.1e-07\n",
      "üìà Step 18575 | Loss: 1.6980 | Avg: 1.3285 | LR: 9.3e-07\n",
      "üìà Step 18600 | Loss: 0.8918 | Avg: 1.4326 | LR: 9.4e-07\n",
      "üìà Step 18625 | Loss: 0.8504 | Avg: 1.6225 | LR: 9.5e-07\n",
      "üìà Step 18650 | Loss: 1.1751 | Avg: 1.5103 | LR: 9.6e-07\n",
      "üìà Step 18675 | Loss: 1.4861 | Avg: 1.2779 | LR: 9.7e-07\n",
      "üìà Step 18700 | Loss: 1.2469 | Avg: 1.3706 | LR: 9.9e-07\n",
      "üìà Step 18725 | Loss: 1.5186 | Avg: 1.5223 | LR: 1.0e-06\n",
      "üìà Step 18750 | Loss: 1.1910 | Avg: 1.3186 | LR: 1.0e-06\n",
      "üìà Step 18775 | Loss: 1.0433 | Avg: 1.2258 | LR: 1.0e-06\n",
      "üìà Step 18800 | Loss: 1.4975 | Avg: 1.5175 | LR: 1.0e-06\n",
      "üìà Step 18825 | Loss: 1.6203 | Avg: 1.2335 | LR: 1.0e-06\n",
      "üìà Step 18850 | Loss: 1.2776 | Avg: 1.4095 | LR: 1.1e-06\n",
      "üìà Step 18875 | Loss: 2.2134 | Avg: 1.5987 | LR: 1.1e-06\n",
      "üìà Step 18900 | Loss: 1.0454 | Avg: 1.3891 | LR: 1.1e-06\n",
      "üìà Step 18925 | Loss: 1.2113 | Avg: 1.3369 | LR: 1.1e-06\n",
      "üìà Step 18950 | Loss: 1.2833 | Avg: 1.3353 | LR: 1.1e-06\n",
      "üìà Step 18975 | Loss: 1.1183 | Avg: 1.3716 | LR: 1.1e-06\n",
      "üìà Step 19000 | Loss: 1.6124 | Avg: 1.3883 | LR: 1.1e-06\n",
      "üìà Step 19025 | Loss: 1.2431 | Avg: 1.3752 | LR: 1.1e-06\n",
      "üìà Step 19050 | Loss: 1.5537 | Avg: 1.6093 | LR: 1.2e-06\n",
      "üìà Step 19075 | Loss: 1.7673 | Avg: 1.3958 | LR: 1.2e-06\n",
      "üìà Step 19100 | Loss: 1.8293 | Avg: 1.5029 | LR: 1.2e-06\n",
      "üìà Step 19125 | Loss: 2.0746 | Avg: 1.2055 | LR: 1.2e-06\n",
      "üìà Step 19150 | Loss: 1.4515 | Avg: 1.5336 | LR: 1.2e-06\n",
      "üìà Step 19175 | Loss: 1.4461 | Avg: 1.5325 | LR: 1.2e-06\n",
      "üìà Step 19200 | Loss: 1.8310 | Avg: 1.4152 | LR: 1.2e-06\n",
      "üìà Step 19225 | Loss: 2.8088 | Avg: 1.4299 | LR: 1.2e-06\n",
      "üìà Step 19250 | Loss: 1.5150 | Avg: 1.6378 | LR: 1.2e-06\n",
      "üìà Step 19275 | Loss: 1.5809 | Avg: 1.4663 | LR: 1.3e-06\n",
      "üìà Step 19300 | Loss: 0.5671 | Avg: 1.2775 | LR: 1.3e-06\n",
      "üìà Step 19325 | Loss: 1.8201 | Avg: 1.4011 | LR: 1.3e-06\n",
      "üìà Step 19350 | Loss: 1.0855 | Avg: 1.3070 | LR: 1.3e-06\n",
      "üìà Step 19375 | Loss: 2.4306 | Avg: 1.2871 | LR: 1.3e-06\n",
      "üìà Step 19400 | Loss: 1.4583 | Avg: 1.5036 | LR: 1.3e-06\n",
      "üìà Step 19425 | Loss: 1.3491 | Avg: 1.3337 | LR: 1.3e-06\n",
      "üìà Step 19450 | Loss: 1.7111 | Avg: 1.5072 | LR: 1.3e-06\n",
      "üìà Step 19475 | Loss: 1.3942 | Avg: 1.3780 | LR: 1.3e-06\n",
      "üìà Step 19500 | Loss: 1.1105 | Avg: 1.4874 | LR: 1.4e-06\n",
      "üìà Step 19525 | Loss: 2.6073 | Avg: 1.6481 | LR: 1.4e-06\n",
      "üìà Step 19550 | Loss: 1.7532 | Avg: 1.2298 | LR: 1.4e-06\n",
      "üìà Step 19575 | Loss: 0.9645 | Avg: 1.2428 | LR: 1.4e-06\n",
      "üìà Step 19600 | Loss: 0.9859 | Avg: 1.2785 | LR: 1.4e-06\n",
      "üìà Step 19625 | Loss: 1.3892 | Avg: 1.3729 | LR: 1.4e-06\n",
      "üìà Step 19650 | Loss: 1.6420 | Avg: 1.3179 | LR: 1.4e-06\n",
      "üìà Step 19675 | Loss: 0.5994 | Avg: 1.3717 | LR: 1.4e-06\n",
      "üìà Step 19700 | Loss: 1.3462 | Avg: 1.3543 | LR: 1.4e-06\n",
      "üìà Step 19725 | Loss: 1.4769 | Avg: 1.2759 | LR: 1.5e-06\n",
      "üìà Step 19750 | Loss: 1.7619 | Avg: 1.5614 | LR: 1.5e-06\n",
      "üìà Step 19775 | Loss: 0.9717 | Avg: 1.4432 | LR: 1.5e-06\n",
      "üìà Step 19800 | Loss: 1.2621 | Avg: 1.3157 | LR: 1.5e-06\n",
      "üìà Step 19825 | Loss: 1.1752 | Avg: 1.5510 | LR: 1.5e-06\n",
      "üìà Step 19850 | Loss: 1.1553 | Avg: 1.4056 | LR: 1.5e-06\n",
      "üìà Step 19875 | Loss: 1.3157 | Avg: 1.6048 | LR: 1.5e-06\n",
      "üìà Step 19900 | Loss: 1.3860 | Avg: 1.4664 | LR: 1.5e-06\n",
      "üìà Step 19925 | Loss: 2.2396 | Avg: 1.4894 | LR: 1.5e-06\n",
      "üìà Step 19950 | Loss: 0.8980 | Avg: 1.5028 | LR: 1.6e-06\n",
      "üìà Step 19975 | Loss: 1.7336 | Avg: 1.4715 | LR: 1.6e-06\n",
      "üìà Step 20000 | Loss: 1.2123 | Avg: 1.2438 | LR: 1.6e-06\n",
      "üìà Step 20025 | Loss: 0.6291 | Avg: 1.4919 | LR: 1.6e-06\n",
      "üìà Step 20050 | Loss: 1.4483 | Avg: 1.3050 | LR: 1.6e-06\n",
      "üìà Step 20075 | Loss: 0.6768 | Avg: 1.3102 | LR: 1.6e-06\n",
      "üìà Step 20100 | Loss: 1.7062 | Avg: 1.3385 | LR: 1.6e-06\n",
      "üìà Step 20125 | Loss: 1.0602 | Avg: 1.2312 | LR: 1.6e-06\n",
      "üìà Step 20150 | Loss: 0.8761 | Avg: 1.2833 | LR: 1.6e-06\n",
      "üìà Step 20175 | Loss: 0.5317 | Avg: 1.2905 | LR: 1.6e-06\n",
      "üìà Step 20200 | Loss: 0.9435 | Avg: 1.4608 | LR: 1.6e-06\n",
      "üìà Step 20225 | Loss: 1.4962 | Avg: 1.5057 | LR: 1.7e-06\n",
      "üìà Step 20250 | Loss: 1.5936 | Avg: 1.4970 | LR: 1.7e-06\n",
      "üìà Step 20275 | Loss: 2.2061 | Avg: 1.4517 | LR: 1.7e-06\n",
      "üìà Step 20300 | Loss: 2.2905 | Avg: 1.4768 | LR: 1.7e-06\n",
      "üìà Step 20325 | Loss: 0.6905 | Avg: 1.5216 | LR: 1.7e-06\n",
      "üìà Step 20350 | Loss: 1.0262 | Avg: 1.2393 | LR: 1.7e-06\n",
      "üìà Step 20375 | Loss: 1.2425 | Avg: 1.4005 | LR: 1.7e-06\n",
      "üìà Step 20400 | Loss: 1.8562 | Avg: 1.4363 | LR: 1.7e-06\n",
      "üìà Step 20425 | Loss: 1.5597 | Avg: 1.2498 | LR: 1.7e-06\n",
      "üìà Step 20450 | Loss: 2.0621 | Avg: 1.4184 | LR: 1.7e-06\n",
      "üìà Step 20475 | Loss: 0.6657 | Avg: 1.2923 | LR: 1.7e-06\n",
      "üìà Step 20500 | Loss: 0.8119 | Avg: 1.4547 | LR: 1.7e-06\n",
      "üìà Step 20525 | Loss: 1.4205 | Avg: 1.3814 | LR: 1.8e-06\n",
      "üìà Step 20550 | Loss: 1.3061 | Avg: 1.3353 | LR: 1.8e-06\n",
      "üíæ Stable checkpoint saved at 90% of epoch 3\n",
      "üìà Step 20575 | Loss: 1.4008 | Avg: 1.3684 | LR: 1.8e-06\n",
      "üìà Step 20600 | Loss: 1.8967 | Avg: 1.2872 | LR: 1.8e-06\n",
      "üìà Step 20625 | Loss: 2.1063 | Avg: 1.4929 | LR: 1.8e-06\n",
      "üìà Step 20650 | Loss: 1.1520 | Avg: 1.3735 | LR: 1.8e-06\n",
      "üìà Step 20675 | Loss: 1.2024 | Avg: 1.2782 | LR: 1.8e-06\n",
      "üìà Step 20700 | Loss: 1.0176 | Avg: 1.2119 | LR: 1.8e-06\n",
      "üìà Step 20725 | Loss: 1.1624 | Avg: 1.3024 | LR: 1.8e-06\n",
      "üìà Step 20750 | Loss: 0.6598 | Avg: 1.4209 | LR: 1.8e-06\n",
      "üìà Step 20775 | Loss: 0.6560 | Avg: 1.2830 | LR: 1.8e-06\n",
      "üìà Step 20800 | Loss: 0.9159 | Avg: 1.2920 | LR: 1.8e-06\n",
      "üìà Step 20825 | Loss: 1.0268 | Avg: 1.1853 | LR: 1.8e-06\n",
      "üìà Step 20850 | Loss: 1.2616 | Avg: 1.2734 | LR: 1.8e-06\n",
      "üìà Step 20875 | Loss: 3.0613 | Avg: 1.3532 | LR: 1.9e-06\n",
      "üìà Step 20900 | Loss: 2.1218 | Avg: 1.3809 | LR: 1.9e-06\n",
      "üìà Step 20925 | Loss: 1.4466 | Avg: 1.4132 | LR: 1.9e-06\n",
      "üìà Step 20950 | Loss: 2.0926 | Avg: 1.4293 | LR: 1.9e-06\n",
      "üìà Step 20975 | Loss: 0.7917 | Avg: 1.2738 | LR: 1.9e-06\n",
      "üìà Step 21000 | Loss: 1.9799 | Avg: 1.3331 | LR: 1.9e-06\n",
      "üìà Step 21025 | Loss: 1.4657 | Avg: 1.3036 | LR: 1.9e-06\n",
      "üìà Step 21050 | Loss: 0.8506 | Avg: 1.2776 | LR: 1.9e-06\n",
      "üìà Step 21075 | Loss: 0.8076 | Avg: 1.2502 | LR: 1.9e-06\n",
      "üìà Step 21100 | Loss: 1.4058 | Avg: 1.3222 | LR: 1.9e-06\n",
      "üìà Step 21125 | Loss: 1.5580 | Avg: 1.2128 | LR: 1.9e-06\n",
      "üìà Step 21150 | Loss: 2.0856 | Avg: 1.3082 | LR: 1.9e-06\n",
      "üìà Step 21175 | Loss: 1.6802 | Avg: 1.3440 | LR: 1.9e-06\n",
      "üìà Step 21200 | Loss: 0.9725 | Avg: 1.3503 | LR: 1.9e-06\n",
      "üìà Step 21225 | Loss: 0.6834 | Avg: 1.3772 | LR: 1.9e-06\n",
      "üìà Step 21250 | Loss: 1.0808 | Avg: 1.3941 | LR: 1.9e-06\n",
      "üìà Step 21275 | Loss: 1.8343 | Avg: 1.3290 | LR: 1.9e-06\n",
      "üìà Step 21300 | Loss: 1.4378 | Avg: 1.6107 | LR: 1.9e-06\n",
      "üìà Step 21325 | Loss: 0.4219 | Avg: 1.1117 | LR: 1.9e-06\n",
      "üìà Step 21350 | Loss: 0.9123 | Avg: 1.3414 | LR: 1.9e-06\n",
      "üìà Step 21375 | Loss: 1.9909 | Avg: 1.3592 | LR: 2.0e-06\n",
      "üìà Step 21400 | Loss: 0.9889 | Avg: 1.4779 | LR: 2.0e-06\n",
      "üìà Step 21425 | Loss: 1.2904 | Avg: 1.4325 | LR: 2.0e-06\n",
      "üìà Step 21450 | Loss: 1.4870 | Avg: 1.3778 | LR: 2.0e-06\n",
      "üìà Step 21475 | Loss: 2.2713 | Avg: 1.4671 | LR: 2.0e-06\n",
      "üìà Step 21500 | Loss: 1.6735 | Avg: 1.1785 | LR: 2.0e-06\n",
      "üìà Step 21525 | Loss: 0.8459 | Avg: 1.3536 | LR: 2.0e-06\n",
      "üìà Step 21550 | Loss: 1.1406 | Avg: 1.4416 | LR: 2.0e-06\n",
      "üìà Step 21575 | Loss: 1.2125 | Avg: 1.1107 | LR: 2.0e-06\n",
      "üìà Step 21600 | Loss: 0.6507 | Avg: 1.4228 | LR: 2.0e-06\n",
      "üìà Step 21625 | Loss: 1.5722 | Avg: 1.2971 | LR: 2.0e-06\n",
      "üìà Step 21650 | Loss: 1.8264 | Avg: 1.3832 | LR: 2.0e-06\n",
      "üìà Step 21675 | Loss: 1.6147 | Avg: 1.2308 | LR: 2.0e-06\n",
      "üìà Step 21700 | Loss: 0.7176 | Avg: 1.3592 | LR: 2.0e-06\n",
      "üìà Step 21725 | Loss: 1.7151 | Avg: 1.3327 | LR: 2.0e-06\n",
      "üìà Step 21750 | Loss: 1.3794 | Avg: 1.2073 | LR: 2.0e-06\n",
      "üìà Step 21775 | Loss: 0.7856 | Avg: 1.1911 | LR: 2.0e-06\n",
      "üìà Step 21800 | Loss: 1.6330 | Avg: 1.4017 | LR: 2.0e-06\n",
      "üìà Step 21825 | Loss: 0.7744 | Avg: 1.3554 | LR: 2.0e-06\n",
      "üìà Step 21850 | Loss: 1.0926 | Avg: 1.2816 | LR: 2.0e-06\n",
      "üìà Step 21875 | Loss: 2.4250 | Avg: 1.1979 | LR: 2.0e-06\n",
      "üìà Step 21900 | Loss: 0.8215 | Avg: 1.3150 | LR: 2.0e-06\n",
      "üìà Step 21925 | Loss: 1.1819 | Avg: 1.4518 | LR: 2.0e-06\n",
      "üìà Step 21950 | Loss: 1.7181 | Avg: 1.4649 | LR: 2.0e-06\n",
      "üìà Step 21975 | Loss: 1.9501 | Avg: 1.4110 | LR: 2.0e-06\n",
      "üìà Step 22000 | Loss: 0.9255 | Avg: 1.0900 | LR: 2.0e-06\n",
      "üìà Step 22025 | Loss: 1.7580 | Avg: 1.4880 | LR: 2.0e-06\n",
      "üìà Step 22050 | Loss: 0.7164 | Avg: 1.3177 | LR: 2.0e-06\n",
      "üìà Step 22075 | Loss: 0.9996 | Avg: 1.2204 | LR: 2.0e-06\n",
      "üìà Step 22100 | Loss: 1.0938 | Avg: 1.0106 | LR: 2.0e-06\n",
      "üìà Step 22125 | Loss: 1.1888 | Avg: 1.4297 | LR: 2.0e-06\n",
      "üìà Step 22150 | Loss: 1.6487 | Avg: 1.3114 | LR: 2.0e-06\n",
      "üìà Step 22175 | Loss: 0.9108 | Avg: 1.4552 | LR: 2.0e-06\n",
      "üìà Step 22200 | Loss: 1.2138 | Avg: 1.4062 | LR: 2.0e-06\n",
      "üìà Step 22225 | Loss: 1.3523 | Avg: 1.2933 | LR: 2.0e-06\n",
      "üìà Step 22250 | Loss: 1.4293 | Avg: 1.2989 | LR: 2.0e-06\n",
      "üìà Step 22275 | Loss: 2.1161 | Avg: 1.1955 | LR: 2.0e-06\n",
      "üìà Step 22300 | Loss: 0.4217 | Avg: 1.1141 | LR: 2.0e-06\n",
      "üìà Step 22325 | Loss: 2.1902 | Avg: 1.4739 | LR: 2.0e-06\n",
      "üìà Step 22350 | Loss: 1.0753 | Avg: 1.2447 | LR: 2.0e-06\n",
      "üìà Step 22375 | Loss: 1.4252 | Avg: 1.2067 | LR: 2.0e-06\n",
      "üìà Step 22400 | Loss: 0.7910 | Avg: 1.3428 | LR: 2.0e-06\n",
      "üìà Step 22425 | Loss: 1.5593 | Avg: 1.1300 | LR: 2.0e-06\n",
      "üìà Step 22450 | Loss: 1.1080 | Avg: 1.4190 | LR: 2.0e-06\n",
      "üìà Step 22475 | Loss: 2.1855 | Avg: 1.2054 | LR: 2.0e-06\n",
      "üìà Step 22500 | Loss: 0.6253 | Avg: 1.2787 | LR: 2.0e-06\n",
      "üìà Step 22525 | Loss: 1.3299 | Avg: 1.4796 | LR: 2.0e-06\n",
      "üìà Step 22550 | Loss: 1.3556 | Avg: 1.4217 | LR: 2.0e-06\n",
      "üìà Step 22575 | Loss: 1.7088 | Avg: 1.2821 | LR: 2.0e-06\n",
      "üìà Step 22600 | Loss: 0.4928 | Avg: 1.2305 | LR: 2.0e-06\n",
      "üìà Step 22625 | Loss: 2.6658 | Avg: 1.3168 | LR: 2.0e-06\n",
      "üìà Step 22650 | Loss: 1.9694 | Avg: 1.3041 | LR: 2.0e-06\n",
      "üìà Step 22675 | Loss: 0.8859 | Avg: 1.3267 | LR: 2.0e-06\n",
      "üìà Step 22700 | Loss: 1.1279 | Avg: 1.2100 | LR: 1.9e-06\n",
      "üìà Step 22725 | Loss: 0.5372 | Avg: 1.3870 | LR: 1.9e-06\n",
      "üìà Step 22750 | Loss: 1.0064 | Avg: 1.2619 | LR: 1.9e-06\n",
      "üìà Step 22775 | Loss: 2.0760 | Avg: 1.3446 | LR: 1.9e-06\n",
      "üìà Step 22800 | Loss: 1.2248 | Avg: 1.2953 | LR: 1.9e-06\n",
      "üìà Step 22825 | Loss: 0.5990 | Avg: 1.2894 | LR: 1.9e-06\n",
      "üìà Step 22850 | Loss: 1.4549 | Avg: 1.1861 | LR: 1.9e-06\n",
      "üìä Epoch 3 - Average train loss: 1.5284 (Stable!)\n",
      "üíæ End-of-epoch checkpoint saved: epoch3_final\n",
      "üîç Running validation for epoch 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cea054d531e4a51a72a3554376b0381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/1299 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Epoch 3: train_loss=1.5284 val_loss=1.0978\n",
      "‚úÖ Best checkpoint updated!\n",
      "üîÑ Loading best checkpoint from artifacts/logbert-mlm-os/best\n",
      "üéâ Fine-tuning completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# üõ°Ô∏è ULTRA-STABLE TRAINING LOOP - No More Overshooting\n",
    "print(\"üõ°Ô∏è Starting ultra-stable training loop...\")\n",
    "accelerator.print(f\"üìä Ultra-conservative settings active\")\n",
    "accelerator.print(f\"üìä Training samples: {len(train_dataset):,}\")\n",
    "accelerator.print(f\"üìä Validation samples: {len(openstack_val):,}\")\n",
    "accelerator.print(f\"üìä Batches per epoch: {total_batches_per_epoch:,}\")\n",
    "\n",
    "# Create a validation loader without anomaly labels for MLM training\n",
    "val_loader_mlm = DataLoader(openstack_val, batch_size=train_cfg['training']['eval_batch_size_per_device'], \n",
    "                        shuffle=False, collate_fn=collate_train)  # Use collate_train to remove anomaly_label\n",
    "val_loader_mlm = accelerator.prepare(val_loader_mlm)\n",
    "accelerator.print(\"‚úÖ Created MLM-specific validation loader (without anomaly labels)\")\n",
    "\n",
    "for epoch in range(epochs_total):\n",
    "    model.train()\n",
    "    accelerator.print(f'==== Epoch {epoch+1}/{epochs_total} (Ultra-Stable Mode) ====')\n",
    "    \n",
    "    # Force enable progress bar\n",
    "    progress = tqdm(\n",
    "        total=len(train_loader), \n",
    "        desc=f\"Epoch {epoch+1} - Stable\", \n",
    "        disable=False,\n",
    "        leave=True,\n",
    "        position=0,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "    \n",
    "    step_losses = []\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            # Ultra-strict gradient clipping (0.1 instead of 0.5)\n",
    "            if accelerator.sync_gradients and max_grad_norm:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        step_losses.append(loss.detach().item())\n",
    "        global_step = epoch * len(train_loader) + step\n",
    "        \n",
    "        # Update progress bar with ultra-detailed info\n",
    "        current_lr = lr_scheduler.get_last_lr()[0]\n",
    "        progress.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'avg_loss': f'{np.mean(step_losses):.4f}',\n",
    "            'lr': f'{current_lr:.1e}',\n",
    "            'step': f'{step}/{len(train_loader)}'\n",
    "        })\n",
    "        progress.update(1)\n",
    "\n",
    "        # More frequent logging for stability monitoring\n",
    "        if step % 25 == 0:  # Every 25 steps instead of 50\n",
    "            avg_loss = np.mean(step_losses[-25:])  # Rolling average of last 25 steps\n",
    "            accelerator.print(f'üìà Step {step:5d} | Loss: {loss.item():.4f} | Avg: {avg_loss:.4f} | LR: {current_lr:.1e}')\n",
    "\n",
    "        # Save checkpoint every 15% of dataset processed\n",
    "        if step % checkpoint_every_batches == 0:\n",
    "            percent_complete = (step / total_batches_per_epoch) * 100\n",
    "            ckpt_dir = Path(checkpoint_cfg['output_dir']) / f'epoch{epoch+1}_step{step}_pct{percent_complete:.0f}'\n",
    "            if accelerator.is_main_process:\n",
    "                ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "                accelerator.unwrap_model(model).save_pretrained(ckpt_dir)\n",
    "                tokenizer.save_pretrained(ckpt_dir / 'tokenizer')\n",
    "                accelerator.print(f'üíæ Stable checkpoint saved at {percent_complete:.0f}% of epoch {epoch+1}')\n",
    "            accelerator.wait_for_everyone()\n",
    "            free_cuda()\n",
    "\n",
    "    progress.close()\n",
    "    train_loss = float(np.mean(step_losses))\n",
    "    accelerator.print(f'üìä Epoch {epoch+1} - Average train loss: {train_loss:.4f} (Stable!)')\n",
    "\n",
    "    # Save end-of-epoch checkpoint\n",
    "    epoch_ckpt_dir = Path(checkpoint_cfg['output_dir']) / f'epoch{epoch+1}_final'\n",
    "    if accelerator.is_main_process:\n",
    "        epoch_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        accelerator.unwrap_model(model).save_pretrained(epoch_ckpt_dir)\n",
    "        tokenizer.save_pretrained(epoch_ckpt_dir / 'tokenizer')\n",
    "        accelerator.print(f'üíæ End-of-epoch checkpoint saved: {epoch_ckpt_dir.name}')\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # Validation loop with MLM-only dataloader (without anomaly_label)\n",
    "    model.eval()\n",
    "    accelerator.print(f'üîç Running validation for epoch {epoch+1}...')\n",
    "    val_progress = tqdm(\n",
    "        total=len(val_loader_mlm), \n",
    "        desc=\"Validation\", \n",
    "        disable=False, \n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    val_losses = []\n",
    "    for batch in val_loader_mlm:  # Use the MLM-specific dataloader\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)  # Now the batch doesn't include anomaly_label\n",
    "            val_losses.append(accelerator.gather(outputs.loss.detach()).mean().item())\n",
    "        val_progress.update(1)\n",
    "    val_progress.close()\n",
    "    \n",
    "    val_loss = float(np.mean(val_losses))\n",
    "\n",
    "    history['epoch'].append(epoch+1)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    accelerator.print(f'‚úÖ Epoch {epoch+1}: train_loss={train_loss:.4f} val_loss={val_loss:.4f}')\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss + min_delta < best_val:\n",
    "        best_val = val_loss\n",
    "        wait = 0\n",
    "        best_checkpoint_path = Path(checkpoint_cfg['output_dir']) / 'best'\n",
    "        if accelerator.is_main_process:\n",
    "            best_checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "            accelerator.unwrap_model(model).save_pretrained(best_checkpoint_path)\n",
    "            tokenizer.save_pretrained(best_checkpoint_path / 'tokenizer')\n",
    "        accelerator.wait_for_everyone()\n",
    "        free_cuda()\n",
    "        accelerator.print('‚úÖ Best checkpoint updated!')\n",
    "    else:\n",
    "        wait += 1\n",
    "        accelerator.print(f'‚è≥ No improvement, patience {wait}/{patience}')\n",
    "        if wait >= patience:\n",
    "            accelerator.print('üõë Early stopping triggered.')\n",
    "            break\n",
    "\n",
    "free_cuda()\n",
    "\n",
    "if best_checkpoint_path and best_checkpoint_path.exists():\n",
    "    accelerator.print(f'üîÑ Loading best checkpoint from {best_checkpoint_path}')\n",
    "    best_model = AutoModelForMaskedLM.from_pretrained(best_checkpoint_path, config=config)\n",
    "    accelerator.unwrap_model(model).load_state_dict(best_model.state_dict())\n",
    "    del best_model\n",
    "    free_cuda()\n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "accelerator.print('üéâ Fine-tuning completed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ead67",
   "metadata": {},
   "source": [
    "## 8. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26fd3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DistilBertForMaskedLM.forward() got an unexpected keyword argument 'anomaly_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(scores), np.asarray(labels)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Use the evaluation loaders that preserve anomaly labels\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m val_scores, val_labels = \u001b[43mcollect_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m test_scores, test_labels = collect_scores(test_loader_eval)\n\u001b[32m     18\u001b[39m threshold_candidates = np.percentile(val_scores, np.linspace(\u001b[32m50\u001b[39m, \u001b[32m99\u001b[39m, \u001b[32m25\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcollect_scores\u001b[39m\u001b[34m(dataloader)\u001b[39m\n\u001b[32m      5\u001b[39m scores, labels = [], []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     loss = accelerator.gather(outputs.loss)\n\u001b[32m      9\u001b[39m     scores.extend(loss.cpu().numpy())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/accelerate/utils/operations.py:825\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/accelerate/utils/operations.py:813\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/log_anomaly/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: DistilBertForMaskedLM.forward() got an unexpected keyword argument 'anomaly_label'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_scores(dataloader) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    scores, labels = [], []\n",
    "    for batch in dataloader:\n",
    "        # Extract and remove anomaly_label from batch before passing to model\n",
    "        label_tensor = batch.pop('anomaly_label')\n",
    "        \n",
    "        # Pass the batch to model (now without anomaly_label)\n",
    "        outputs = model(**batch)\n",
    "        loss = accelerator.gather(outputs.loss)\n",
    "        scores.extend(loss.cpu().numpy())\n",
    "        \n",
    "        # Process the labels we extracted earlier\n",
    "        labels.extend(accelerator.gather(label_tensor).cpu().numpy())\n",
    "    return np.asarray(scores), np.asarray(labels)\n",
    "\n",
    "# Use the evaluation loaders that preserve anomaly labels\n",
    "val_scores, val_labels = collect_scores(val_loader_eval)\n",
    "test_scores, test_labels = collect_scores(test_loader_eval)\n",
    "\n",
    "threshold_candidates = np.percentile(val_scores, np.linspace(50, 99, 25))\n",
    "best_threshold, best_f1 = threshold_candidates[-1], 0.0\n",
    "for candidate in threshold_candidates:\n",
    "    preds = (val_scores >= candidate).astype(int)\n",
    "    f1 = f1_score(val_labels, preds, zero_division=0)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = candidate\n",
    "\n",
    "val_probs = np.exp(-val_scores)\n",
    "test_probs = np.exp(-test_scores)\n",
    "\n",
    "pred_test = (test_scores >= best_threshold).astype(int)\n",
    "f1 = f1_score(test_labels, pred_test, zero_division=0)\n",
    "roc_auc = roc_auc_score(test_labels, test_probs)\n",
    "pr_auc = average_precision_score(test_labels, test_probs)\n",
    "\n",
    "metrics = {\n",
    "    'val_threshold_loss': float(best_threshold),\n",
    "    'val_best_f1': float(best_f1),\n",
    "    'test_f1': float(f1),\n",
    "    'test_roc_auc': float(roc_auc),\n",
    "    'test_pr_auc': float(pr_auc)\n",
    "}\n",
    "metrics_path = metrics_dir / 'openstack_metrics.json'\n",
    "metrics_path.write_text(json.dumps(metrics, indent=2))\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "cm = confusion_matrix(test_labels, pred_test)\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "cm_path = eval_dir / 'confusion_matrix.png'\n",
    "fig.savefig(cm_path)\n",
    "plt.close(fig)\n",
    "print(f'Confusion matrix saved to {cm_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc1d9e9",
   "metadata": {},
   "source": [
    "## 9. Export TorchScript and ONNX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = Path(train_cfg['export']['output_dir'])\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "accelerator.wait_for_everyone()\n",
    "model_to_export = accelerator.unwrap_model(model).cpu()\n",
    "model_to_export.eval()\n",
    "\n",
    "seq_len = train_cfg['sequence']['max_length']\n",
    "dummy_ids = torch.ones((1, seq_len), dtype=torch.long)\n",
    "dummy_mask = torch.ones((1, seq_len), dtype=torch.long)\n",
    "\n",
    "with torch.no_grad():\n",
    "    traced = torch.jit.trace(model_to_export, (dummy_ids, dummy_mask))\n",
    "    ts_path = export_dir / train_cfg['export']['torchscript_filename']\n",
    "    traced.save(str(ts_path))\n",
    "    print(f'TorchScript saved to {ts_path}')\n",
    "\n",
    "onnx_path = export_dir / train_cfg['export']['onnx_filename']\n",
    "torch.onnx.export(\n",
    "    model_to_export,\n",
    "    (dummy_ids, dummy_mask),\n",
    "    str(onnx_path),\n",
    "    input_names=['input_ids', 'attention_mask'],\n",
    "    output_names=['logits'],\n",
    "    dynamic_axes={'input_ids': {0: 'batch'}, 'attention_mask': {0: 'batch'}, 'logits': {0: 'batch'}},\n",
    "    opset_version=train_cfg['export']['opset']\n",
    ")\n",
    "print(f'ONNX saved to {onnx_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99b485",
   "metadata": {},
   "source": [
    "## 10. Model Card Snippet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card_path = Path(train_cfg['checkpointing']['output_dir']) / 'MODEL_CARD.md'\n",
    "model_card = (\n",
    "    f\"# LogBERT OpenStack Fine-tune\n",
    "\n",
    "\"\n",
    "    f\"- Base checkpoint: {train_cfg['base_checkpoint_dir']}\n",
    "\"\n",
    "    f\"- Sequence length: {train_cfg['sequence']['max_length']}\n",
    "\"\n",
    "    f\"- Replay enabled: {train_cfg['replay']['enabled']}\n",
    "\"\n",
    "    f\"- LoRA enabled: {train_cfg['peft']['lora_enabled']}\n",
    "\n",
    "\"\n",
    "    f\"## Eval Metrics\n",
    "\"\n",
    "    f\"- F1: {metrics['test_f1']:.4f}\n",
    "\"\n",
    "    f\"- ROC-AUC: {metrics['test_roc_auc']:.4f}\n",
    "\"\n",
    "    f\"- PR-AUC: {metrics['test_pr_auc']:.4f}\n",
    "\"\n",
    "    f\"- Threshold (loss): {metrics['val_threshold_loss']:.6f}\n",
    "\n",
    "\"\n",
    "    f\"## Artifacts\n",
    "\"\n",
    "    f\"- TorchScript: {train_cfg['export']['torchscript_filename']}\n",
    "\"\n",
    "    f\"- ONNX: {train_cfg['export']['onnx_filename']}\n",
    "\"\n",
    ")\n",
    "model_card_path.write_text(model_card)\n",
    "print(f'Model card snippet written to {model_card_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d454fd",
   "metadata": {},
   "source": [
    "## 11. Persist Run Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f21b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_summary = {\n",
    "    'num_processes': accelerator.state.num_processes,\n",
    "    'device': str(accelerator.device),\n",
    "    'mixed_precision': accelerator.state.mixed_precision\n",
    "}\n",
    "run_payload = {\n",
    "    'train_openstack': train_cfg,\n",
    "    'data_config': data_cfg,\n",
    "    'accelerator_state': state_summary,\n",
    "    'metrics': metrics,\n",
    "    'is_mps': IS_MPS\n",
    "}\n",
    "run_config_path.write_text(json.dumps(run_payload, indent=2))\n",
    "print(f'Run config stored at {run_config_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33ea54",
   "metadata": {},
   "source": [
    "## Artifacts Produced\n",
    "\n",
    "- Fine-tuned checkpoints -> `artifacts/logbert-mlm-os/`\n",
    "- Metrics JSON and confusion matrix -> `artifacts/metrics/openstack/openstack_metrics.json`, `artifacts/eval/confusion_matrix.png`\n",
    "- Exported models -> `artifacts/exported_models/`\n",
    "- Model card snippet -> `artifacts/logbert-mlm-os/MODEL_CARD.md`\n",
    "\n",
    "Pipeline complete.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log_anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
