{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5065815a",
   "metadata": {},
   "source": [
    "# Template Model Inference & Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecc6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59882674",
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = Path.cwd().resolve()\n",
    "REPO_ROOT = CWD.parent if CWD.name == 'notebooks' else CWD\n",
    "cfg = yaml.safe_load((REPO_ROOT / 'configs/train_openstack.yaml').read_text())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sequences(parquet_path: Path):\n",
    "    table = pq.read_table(parquet_path, columns=['templates'])\n",
    "    sequences = []\n",
    "    for seq in table.column(0).to_pylist():\n",
    "        if seq and len(seq) > 1:\n",
    "            sequences.append([int(x) for x in seq])\n",
    "    return sequences\n",
    "\n",
    "class TemplateSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "class TemplateBatchCollator:\n",
    "    def __init__(self, pad_id: int, max_length: int):\n",
    "        self.pad_id = pad_id\n",
    "        self.max_length = max_length\n",
    "    def __call__(self, batch):\n",
    "        filtered = [seq[: self.max_length] for seq in batch if len(seq) > 1]\n",
    "        if not filtered:\n",
    "            filtered = [batch[0][: self.max_length]]\n",
    "        max_len = max(len(seq) for seq in filtered)\n",
    "        input_len = max_len - 1\n",
    "        bs = len(filtered)\n",
    "        input_ids = torch.full((bs, input_len), self.pad_id, dtype=torch.long)\n",
    "        target_ids = torch.full((bs, input_len), self.pad_id, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((bs, input_len), dtype=torch.long)\n",
    "        for i, seq in enumerate(filtered):\n",
    "            src = seq[:-1]\n",
    "            tgt = seq[1:]\n",
    "            input_ids[i, : len(src)] = torch.tensor(src, dtype=torch.long)\n",
    "            target_ids[i, : len(tgt)] = torch.tensor(tgt, dtype=torch.long)\n",
    "            attention_mask[i, : len(src)] = 1\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'target_ids': target_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16402311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, pad_id: int, d_model: int, n_layers: int, n_heads: int,\n",
    "                 ffn_dim: int, dropout: float, max_length: int):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.positional = nn.Parameter(torch.zeros(1, max_length, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=ffn_dim,\n",
    "                                                   dropout=dropout, batch_first=True, activation='gelu')\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.register_buffer('causal_mask', torch.triu(torch.ones(max_length, max_length), diagonal=1).bool(), persistent=False)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        seq_len = input_ids.size(1)\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x + self.positional[:, :seq_len, :]\n",
    "        causal = self.causal_mask[:seq_len, :seq_len]\n",
    "        causal = causal.float().masked_fill(causal, float('-inf'))\n",
    "        key_padding = attention_mask == 0\n",
    "        x = self.encoder(x, mask=causal, src_key_padding_mask=key_padding)\n",
    "        x = self.dropout(self.norm(x))\n",
    "        logits = self.output(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99350ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab and datasets\n",
    "vocab = json.loads((REPO_ROOT / cfg['data']['vocab_path']).read_text())\n",
    "base_vocab_size = len(vocab.get('id_to_template', []))\n",
    "pad_strategy = cfg['data'].get('pad_token_strategy', 'vocab_size')\n",
    "if pad_strategy == 'vocab_size':\n",
    "    pad_id = base_vocab_size\n",
    "    vocab_size = base_vocab_size + 1\n",
    "else:\n",
    "    pad_id = int(pad_strategy)\n",
    "    vocab_size = max(base_vocab_size + 1, pad_id + 1)\n",
    "\n",
    "max_seq_len = cfg['data'].get('max_sequence_length', 100)\n",
    "collator = TemplateBatchCollator(pad_id=pad_id, max_length=max_seq_len)\n",
    "\n",
    "val_sequences = read_sequences((REPO_ROOT / cfg['data']['val_file']).resolve())\n",
    "val_loader = DataLoader(TemplateSequenceDataset(val_sequences), batch_size=cfg['training']['eval_batch_size'], shuffle=False, collate_fn=collator)\n",
    "\n",
    "test_sequences = read_sequences((REPO_ROOT / cfg['data']['test_file']).resolve())\n",
    "test_loader = DataLoader(TemplateSequenceDataset(test_sequences), batch_size=cfg['training']['eval_batch_size'], shuffle=False, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "model_cfg = cfg['model']\n",
    "model = TemplateTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    pad_id=pad_id,\n",
    "    d_model=model_cfg['d_model'],\n",
    "    n_layers=model_cfg['n_layers'],\n",
    "    n_heads=model_cfg['n_heads'],\n",
    "    ffn_dim=model_cfg['ffn_dim'],\n",
    "    dropout=model_cfg['dropout'],\n",
    "    max_length=max_seq_len,\n",
    ").to(device)\n",
    "\n",
    "ckpt_path = (REPO_ROOT / cfg['checkpointing']['output_dir'] / 'best.pt').resolve()\n",
    "if not ckpt_path.exists():\n",
    "    raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "model.eval()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7abc498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_losses(model, loader):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            target_ids = batch['target_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            token_losses = F.nll_loss(\n",
    "                log_probs.view(-1, log_probs.size(-1)),\n",
    "                target_ids.view(-1),\n",
    "                reduction='none',\n",
    "                ignore_index=pad_id,\n",
    "            )\n",
    "            token_losses = token_losses.view(target_ids.size())\n",
    "            mask = (target_ids != pad_id)\n",
    "            seq_loss = (token_losses * mask).sum(dim=1) / mask.sum(dim=1).clamp_min(1)\n",
    "            losses.extend(seq_loss.cpu().tolist())\n",
    "    return losses\n",
    "\n",
    "val_losses = sequence_losses(model, val_loader)\n",
    "test_losses = sequence_losses(model, test_loader)\n",
    "print(f\"Validation loss mean {np.mean(val_losses):.4f} | std {np.std(val_losses):.4f}\")\n",
    "print(f\"Test loss mean {np.mean(test_losses):.4f} | std {np.std(test_losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be1ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = float(np.percentile(val_losses, 95))\n",
    "print(f\"Suggested anomaly threshold (95th percentile): {threshold:.4f}\")\n",
    "\n",
    "metrics_dir = (REPO_ROOT / cfg['logging'].get('metrics_dir', 'artifacts/metrics/openstack')).resolve()\n",
    "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "metrics_path = metrics_dir / 'openstack_threshold.json'\n",
    "metrics_path.write_text(json.dumps({\n",
    "    'val_loss_mean': float(np.mean(val_losses)),\n",
    "    'val_loss_std': float(np.std(val_losses)),\n",
    "    'test_loss_mean': float(np.mean(test_losses)),\n",
    "    'test_loss_std': float(np.std(test_losses)),\n",
    "    'threshold': threshold\n",
    "}, indent=2))\n",
    "print(f\"Saved threshold summary to {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sequences(parquet_path: Path) -> pd.DataFrame:\n",
    "    sequences = read_sequences(parquet_path)\n",
    "    loader = DataLoader(TemplateSequenceDataset(sequences), batch_size=cfg['training']['eval_batch_size'], shuffle=False, collate_fn=collator)\n",
    "    scores = sequence_losses(model, loader)\n",
    "    return pd.DataFrame({'sequence_index': range(len(scores)), 'avg_loss': scores, 'is_anomaly': [score >= threshold for score in scores]})\n",
    "\n",
    "# Example usage (disabled by default)\n",
    "# scored = score_sequences((REPO_ROOT / 'artifacts/openstack_finetune/val.parquet').resolve())\n",
    "# scored.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log_anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
