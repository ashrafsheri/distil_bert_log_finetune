# Masked-language-model pretraining on HDFS logs
model:
  d_model: 256
  n_layers: 6
  n_heads: 8
  ffn_dim: 1024
  dropout: 0.1

seed: 42

sequence:
  max_length: 100

training:
  epochs: 5
  batch_size: 64
  eval_batch_size: 64
  grad_accumulation_steps: 1
  max_grad_norm: 1.0

optimizer:
  type: adamw
  lr: 3.0e-4
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01
  warmup_steps: 200
  scheduler: cosine

precision:
  mixed_precision: fp16

checkpointing:
  output_dir: artifacts/hdfs_transformer
  keep_last_n: 3

logging:
  throughput_ema_beta: 0.9
  log_every_n_steps: 100

data:
  train_file: artifacts/hdfs_pretrain/train.parquet
  val_file: artifacts/hdfs_pretrain/val.parquet
  test_file: artifacts/hdfs_pretrain/test.parquet
  vocab_path: artifacts/hdfs_pretrain/template_vocab.json
  pad_token_strategy: vocab_size
