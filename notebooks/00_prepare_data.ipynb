{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8007e7c1",
   "metadata": {},
   "source": [
    "# 00 · Prepare Data for LogBERT Pipeline\n",
    "\n",
    "This notebook downloads public log datasets, applies regex normalization, mines templates with Drain3, and materializes tokenized Parquet splits ready for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af603d7",
   "metadata": {},
   "source": [
    "## Notebook Goals\n",
    "- Install pinned dependencies for the workstation environment.\n",
    "- Fetch HDFS and OpenStack log corpora with checksum verification and automatic mirror fallback.\n",
    "- Apply regex-based normalization rules from `configs/data.yaml` and preview before/after examples.\n",
    "- Mine log templates in streaming mode with Drain3 and persist template transitions as Parquet.\n",
    "- Build Hugging Face datasets with BERT-compatible tokenization, time-based splits (80/10/10), and truncation stats.\n",
    "- Save artifacts (tokenizer, processed Parquet, metadata) for downstream notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0a20f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2e3104",
   "metadata": {
    "tags": [
     "helper",
     "collapsible"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt not found; skipping installation\n"
     ]
    }
   ],
   "source": [
    "import os, sys, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "if os.environ.get('SKIP_REQUIREMENTS', '0') == '1':\n",
    "    print('SKIP_REQUIREMENTS=1 -> skipping pip install from requirements.txt')\n",
    "else:\n",
    "    req_path = Path('requirements.txt')\n",
    "    if req_path.exists():\n",
    "        print('[setup] Installing dependencies from requirements.txt ...')\n",
    "        completed = subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', str(req_path)])\n",
    "        if completed.returncode != 0:\n",
    "            raise RuntimeError('pip installation failed; inspect output above.')\n",
    "    else:\n",
    "        print('requirements.txt not found; skipping installation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508748f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import tarfile\n",
    "import hashlib\n",
    "import shutil\n",
    "import gc\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79da8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install polars for faster data loading\n",
    "# !pip install polars\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a5821",
   "metadata": {},
   "source": [
    "### Load Configuration and Prepare Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8df71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_prepare_data.ipynb\t     03_anomaly_detection.ipynb        README.md\n",
      "01_pretrain_hdfs.ipynb\t     03_synthetic_log_inference.ipynb  README_NEW.md\n",
      "02_finetune_openstack.ipynb  configs\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c525a35",
   "metadata": {
    "tags": [
     "helper"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bootstrap] Repository root: /home/tpi/distil_shahreyar\n",
      "[bootstrap] Artifacts root: /home/tpi/distil_shahreyar/artifacts\n"
     ]
    }
   ],
   "source": [
    "def load_yaml(path: Path) -> Dict:\n",
    "    with path.open('r') as fh:\n",
    "        return yaml.safe_load(fh)\n",
    "\n",
    "CWD = Path.cwd().resolve()\n",
    "REPO_ROOT = CWD.parent if CWD.name == 'notebooks' else CWD\n",
    "\n",
    "print(f\"[bootstrap] Repository root: {REPO_ROOT}\")\n",
    "\n",
    "data_config = load_yaml(REPO_ROOT / 'configs' / 'data.yaml')\n",
    "ARTIFACTS_ROOT = (REPO_ROOT / data_config.get('artifacts_root', 'artifacts')).resolve()\n",
    "ARTIFACTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[bootstrap] Artifacts root: {ARTIFACTS_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f1ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[drain3.ini] Wrote: /home/tpi/distil_shahreyar/configs/drain3.ini\n",
      "[drain3.ini] Wrote: /home/tpi/distil_shahreyar/notebooks/configs/drain3.ini\n",
      "[validate] OK: /home/tpi/distil_shahreyar/configs/drain3.ini  (6 mask patterns)\n",
      "[validate] OK: /home/tpi/distil_shahreyar/notebooks/configs/drain3.ini  (6 mask patterns)\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Patch A: Repair drain3.ini (avoid multiline JSON) + validate\n",
    "\n",
    "# %%\n",
    "import json, configparser\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolve repo roots (works from repo root OR notebooks/)\n",
    "CWD = Path.cwd().resolve()\n",
    "REPO_ROOT = CWD.parent if CWD.name == \"notebooks\" else CWD\n",
    "\n",
    "# We'll write the ini in BOTH locations so whichever the code uses will be valid\n",
    "INI_TARGETS = [\n",
    "    REPO_ROOT / \"configs\" / \"drain3.ini\",\n",
    "    REPO_ROOT / \"notebooks\" / \"configs\" / \"drain3.ini\",\n",
    "]\n",
    "\n",
    "for p in INI_TARGETS:\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build a safe, single-line JSON for mask_patterns\n",
    "mask_patterns = [\n",
    "    {\"name\": \"REQ\",  \"pattern\": r\"\\breq-[0-9a-fA-F\\-]{8,}\\b\", \"replace_with\": \"<OS_REQ>\"},\n",
    "    {\"name\": \"UUID\", \"pattern\": r\"\\b[0-9a-fA-F]{8}(?:-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}\\b\", \"replace_with\": \"<UUID>\"},\n",
    "    {\"name\": \"IPV4\", \"pattern\": r\"\\b(?:(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)(?:\\.(?!$)|$)){4}\\b\", \"replace_with\": \"<IP>\"},\n",
    "    {\"name\": \"IPV6\", \"pattern\": r\"\\b(?:[A-Fa-f0-9]{0,4}:){2,7}[A-Fa-f0-9]{0,4}\\b\", \"replace_with\": \"<IP6>\"},\n",
    "    {\"name\": \"HEX\",  \"pattern\": r\"\\b0x[0-9a-fA-F]+\\b|\\b[0-9a-fA-F]{8,}\\b\", \"replace_with\": \"<HEX>\"},\n",
    "    {\"name\": \"NUM\",  \"pattern\": r\"(?<![A-Za-z])[-+]?\\d+(?:\\.\\d+)?(?![A-Za-z])\", \"replace_with\": \"<NUM>\"},\n",
    "]\n",
    "mask_patterns_json = json.dumps(mask_patterns, separators=(\",\", \":\"))\n",
    "\n",
    "ini_text = f\"\"\"[DRAIN]\n",
    "sim_th = 0.4\n",
    "depth = 4\n",
    "max_children = 100\n",
    "max_clusters = 100000\n",
    "extra_delimiters = [\"/\",\"_\",\"=\",\"&\",\"?\",\"-\",\".\",\":\",\",\"]\n",
    "\n",
    "[PROFILING]\n",
    "enabled = false\n",
    "\n",
    "[MASKING]\n",
    "mask_prefix = <\n",
    "mask_suffix = >\n",
    "mask_patterns = {mask_patterns_json}\n",
    "\"\"\"\n",
    "\n",
    "# Write files\n",
    "for ini_path in INI_TARGETS:\n",
    "    ini_path.write_text(ini_text, encoding=\"utf-8\")\n",
    "    print(f\"[drain3.ini] Wrote: {ini_path}\")\n",
    "\n",
    "# Validate with configparser + json\n",
    "def validate_ini(path: Path):\n",
    "    cp = configparser.ConfigParser()\n",
    "    cp.read(path)\n",
    "    raw = cp.get(\"MASKING\", \"mask_patterns\", fallback=\"[]\")\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        assert isinstance(parsed, list) and len(parsed) >= 1\n",
    "        print(f\"[validate] OK: {path}  ({len(parsed)} mask patterns)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[validate] FAILED: {path}  -> {e}\")\n",
    "        return False\n",
    "\n",
    "_ = [validate_ini(p) for p in INI_TARGETS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c516d6",
   "metadata": {},
   "source": [
    "## 2. Download Public Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b5d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Normalization rules (robust & ordered)\n",
    "# Replace PII-like and high-cardinality tokens with placeholders to improve generalization and template stability.\n",
    "\n",
    "# %%\n",
    "import re, math\n",
    "from typing import Optional\n",
    "\n",
    "# Regex patterns for token normalization (correct word boundaries)\n",
    "RE_EMAIL = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "RE_URL   = re.compile(r\"https?://\\S+\")\n",
    "RE_OPENSTACK_REQ = re.compile(r\"\\breq-[0-9a-fA-F\\-]{8,}\\b\", re.IGNORECASE)\n",
    "\n",
    "RE_IPv4  = re.compile(r\"\\b(?:(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)(?:\\.(?!$)|$)){4}\\b\")\n",
    "RE_IPv6  = re.compile(r\"\\b(?:[A-Fa-f0-9]{0,4}:){2,7}[A-Fa-fA-F0-9]{0,4}\\b\")\n",
    "RE_UUID  = re.compile(r\"\\b[0-9a-fA-F]{8}(?:-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}\\b\")\n",
    "RE_HEX   = re.compile(r\"\\b0x[0-9a-fA-F]+\\b|\\b[0-9a-fA-F]{8,}\\b\")  # long hex-ish tokens\n",
    "RE_NUM   = re.compile(r\"(?<![A-Za-z])[-+]?\\d+(?:\\.\\d+)?(?![A-Za-z])\")\n",
    "RE_PATH  = re.compile(r\"(?:/[^/\\s]+)+\")   # basic POSIX path\n",
    "\n",
    "# Domain-specific\n",
    "RE_HDFS_BLOCK = re.compile(r\"blk_-?\\d+\")\n",
    "\n",
    "# Context compaction & noisy HTML\n",
    "RE_OS_CTX = re.compile(r\"\\[(\\s*<OS_REQ>)[^\\]]*\\]\")           # keep only <OS_REQ>\n",
    "RE_HTML_DOCTYPE = re.compile(r\"<!DOCTYPE[^>]*>\", re.IGNORECASE)\n",
    "\n",
    "# Helpers for numeric bucketing\n",
    "def bucket_number(m: re.Match) -> str:\n",
    "    s = m.group(0)\n",
    "    try:\n",
    "        if \".\" in s:\n",
    "            val = float(s)\n",
    "        else:\n",
    "            val = int(s)\n",
    "        if val == 0:\n",
    "            return \"<NUM_E0>\"\n",
    "        mag = int(math.floor(math.log10(abs(val))))\n",
    "        return f\"<NUM_E{mag}>\"\n",
    "    except Exception:\n",
    "        return \"<NUM>\"\n",
    "\n",
    "def _normalize_path(path: str) -> str:\n",
    "    # Replace numeric/hex path segments with {id} to reduce churn\n",
    "    parts = path.split(\"/\")\n",
    "    norm = []\n",
    "    for p in parts:\n",
    "        if not p:\n",
    "            continue\n",
    "        if RE_NUM.fullmatch(p) or RE_UUID.fullmatch(p) or RE_HEX.fullmatch(p):\n",
    "            norm.append(\"{id}\")\n",
    "        else:\n",
    "            norm.append(p)\n",
    "    return \"/\" + \"/\".join(norm) if norm else path\n",
    "\n",
    "def normalize_message(msg: str) -> str:\n",
    "    if not msg:\n",
    "        return msg\n",
    "    out = msg\n",
    "\n",
    "    # 1) General early\n",
    "    out = RE_EMAIL.sub(\"<EMAIL>\", out)\n",
    "    out = RE_URL.sub(\"<URL>\", out)\n",
    "\n",
    "    # 2) IMPORTANT: req-* before UUID so we don't get \"req-<UUID>\"\n",
    "    out = RE_OPENSTACK_REQ.sub(\"<OS_REQ>\", out)\n",
    "\n",
    "    # 3) The rest\n",
    "    out = RE_IPv4.sub(\"<IP>\", out)\n",
    "    out = RE_IPv6.sub(\"<IP6>\", out)\n",
    "    out = RE_UUID.sub(\"<UUID>\", out)\n",
    "    out = RE_HDFS_BLOCK.sub(\"<HDFS_BLOCK>\", out)\n",
    "    out = RE_HEX.sub(\"<HEX>\", out)\n",
    "    out = RE_PATH.sub(lambda m: _normalize_path(m.group(0)), out)\n",
    "    out = RE_NUM.sub(bucket_number, out)\n",
    "\n",
    "    # 4) Compact noisy OpenStack bracket contexts after <OS_REQ>\n",
    "    out = RE_OS_CTX.sub(r\"[\\1]\", out)\n",
    "\n",
    "    # 5) Collapse HTML doctypes\n",
    "    out = RE_HTML_DOCTYPE.sub(\"<HTML_DOCTYPE>\", out)\n",
    "\n",
    "    # 6) Whitespace canon\n",
    "    return re.sub(r\"\\s+\", \" \", out).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e827f2",
   "metadata": {},
   "source": [
    "## IO helpers (compressed readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b824e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, bz2\n",
    "\n",
    "def open_maybe_compressed(path: Path, mode=\"rt\", **kwargs):\n",
    "    \"\"\"Open .gz / .bz2 / plain files transparently.\"\"\"\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".gz\":\n",
    "        return gzip.open(path, mode, **kwargs)\n",
    "    elif suf == \".bz2\":\n",
    "        return bz2.open(path, mode, **kwargs)\n",
    "    else:\n",
    "        return open(path, mode, **kwargs)\n",
    "\n",
    "def yield_lines(path: Path, encoding=\"utf-8\", errors=\"ignore\"):\n",
    "    \"\"\"Stream lines from a (possibly compressed) log file.\"\"\"\n",
    "    with open_maybe_compressed(path, \"rt\", encoding=encoding, errors=errors) as fh:\n",
    "        for line in fh:\n",
    "            line = line.rstrip(\"\\r\\n\")\n",
    "            if line:\n",
    "                yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29465c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Time utilities & session helpers ===\n",
    "TS_FORMATS = [\n",
    "    '%Y-%m-%d %H:%M:%S',\n",
    "    '%Y-%m-%dT%H:%M:%S',\n",
    "    '%Y-%m-%d %H:%M:%S.%f',\n",
    "    '%Y-%m-%dT%H:%M:%S.%f',\n",
    "    '%y%m%d %H%M%S',\n",
    "]\n",
    "\n",
    "\n",
    "def parse_ts(text: str) -> Optional[pd.Timestamp]:\n",
    "    if not text:\n",
    "        return None\n",
    "    for fmt in TS_FORMATS:\n",
    "        try:\n",
    "            return pd.to_datetime(text, format=fmt, errors='coerce')\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.to_datetime(text, errors='coerce')\n",
    "\n",
    "\n",
    "def ensure_session_ids(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    if 'session_id' not in df.columns:\n",
    "        df['session_id'] = None\n",
    "    missing = df['session_id'].isna() | (df['session_id'].astype(str).str.len() == 0)\n",
    "    if missing.any():\n",
    "        df.loc[missing, 'session_id'] = [f\"{prefix}-{i}\" for i in range(missing.sum())]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a529f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## HDFS parser (final & robust)\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "# Strong patterns (comma/dot millis, ISO, thread, file:line, compact)\n",
    "RE_HDFS_RICH_A = re.compile(\n",
    "    r\"^(?P<ts>\\d{4}-\\d{2}-\\d{2}[ T]\\d{2}:\\d{2}:\\d{2}(?:[.,]\\d{3,6})?Z?)\\s+\"\n",
    "    r\"(?:(?:\\[(?P<thr>[^\\]]+)\\])\\s+)?\"\n",
    "    r\"(?P<level>[A-Z]+)\\s+\"\n",
    "    r\"(?P<component>[A-Za-z0-9\\.\\$\\-]+)\"\n",
    "    r\"(?:\\s*\\([^)]*\\))?\\s*[:\\-]\\s*(?P<msg>.*)$\"\n",
    ")\n",
    "RE_HDFS_RICH_B = re.compile(\n",
    "    r\"^(?P<ts>\\d{4}-\\d{2}-\\d{2}[ T]\\d{2}:\\d{2}:\\d{2}(?:[.,]\\d{3,6})?Z?)\\s+\"\n",
    "    r\"(?P<level>[A-Z]+)\\s+\"\n",
    "    r\"(?:(?:\\[(?P<thr>[^\\]]+)\\])\\s+)?\"\n",
    "    r\"(?P<component>[A-Za-z0-9\\.\\$\\-]+)\"\n",
    "    r\"(?:\\s*\\([^)]*\\))?\\s*[:\\-]\\s*(?P<msg>.*)$\"\n",
    ")\n",
    "RE_HDFS_LEGACY  = re.compile(\n",
    "    r\"^(?P<ts>\\d{6}\\s+\\d{6})\\s+(?P<pid>\\d+)\\s+(?P<level>[A-Z]+)\\s+(?P<component>[A-Za-z0-9\\.\\$\\-]+)\\s*[:\\-]\\s*(?P<msg>.*)$\"\n",
    ")\n",
    "RE_HDFS_FALLBACK = re.compile(\n",
    "    r\"^(?P<ts>\\S+\\s+\\S+)\\s+(?P<level>[A-Z]+)\\s+(?P<component>[A-Za-z0-9\\.\\$\\-]+)\\s*[:\\-]\\s*(?P<msg>.*)$\"\n",
    ")\n",
    "RE_JAVA_CLASS_IN_MSG = re.compile(r\"\\b(?:[a-z_][a-z0-9_]*\\.)+[A-Za-z0-9_\\$]+\\b\")\n",
    "\n",
    "def parse_ts(text: str) -> Optional[pd.Timestamp]:\n",
    "    try:\n",
    "        return pd.to_datetime(text)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "def _parse_hdfs_compact_ts(ts_str: str) -> pd.Timestamp:\n",
    "    # YYMMDD HHMMSS -> 20YY-MM-DD HH:MM:SS\n",
    "    yy, mm, dd = ts_str[:2], ts_str[2:4], ts_str[4:6]\n",
    "    hh, mi, ss = ts_str[7:9], ts_str[9:11], ts_str[11:13]\n",
    "    year = f\"20{yy}\"\n",
    "    return pd.to_datetime(f\"{year}-{mm}-{dd} {hh}:{mi}:{ss}\", format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "def _parse_hdfs_fields(text: str) -> Optional[Dict[str, Any]]:\n",
    "    m = RE_HDFS_LEGACY.match(text)\n",
    "    if m:\n",
    "        return {\n",
    "            \"timestamp\": _parse_hdfs_compact_ts(m.group(\"ts\")),\n",
    "            \"level\": m.group(\"level\"),\n",
    "            \"component\": m.group(\"component\"),\n",
    "            \"thread\": m.group(\"pid\"),\n",
    "            \"raw_message\": m.group(\"msg\"),\n",
    "        }\n",
    "    for rx in (RE_HDFS_RICH_A, RE_HDFS_RICH_B, RE_HDFS_FALLBACK):\n",
    "        m = rx.match(text)\n",
    "        if m:\n",
    "            return {\n",
    "                \"timestamp\": parse_ts(m.group(\"ts\")) or pd.NaT,\n",
    "                \"level\": m.group(\"level\"),\n",
    "                \"component\": m.group(\"component\"),\n",
    "                \"thread\": m.groupdict().get(\"thr\"),\n",
    "                \"raw_message\": m.group(\"msg\"),\n",
    "            }\n",
    "    return None\n",
    "\n",
    "def _backfill_component_if_unknown(fields: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    comp = fields.get(\"component\") or \"unknown\"\n",
    "    if comp == \"unknown\":\n",
    "        mm = RE_JAVA_CLASS_IN_MSG.search(fields.get(\"raw_message\") or \"\")\n",
    "        if mm:\n",
    "            fields[\"component\"] = mm.group(0)\n",
    "    return fields\n",
    "\n",
    "def parse_hdfs_line(line: str) -> Optional[Dict[str, Any]]:\n",
    "    f = _parse_hdfs_fields(line)\n",
    "    if not f:\n",
    "        return None\n",
    "    blk = RE_HDFS_BLOCK.search(line)\n",
    "    f[\"session_id\"] = blk.group(0) if blk else None\n",
    "    f = _backfill_component_if_unknown(f)\n",
    "    f[\"norm_message\"] = normalize_message(f[\"raw_message\"])\n",
    "    return f\n",
    "\n",
    "def parse_hdfs_csv_row(row: pd.Series) -> Optional[Dict[str, Any]]:\n",
    "    content = row.get(\"Content\") or row.get(\"content\") or row.get(\"Message\") or row.get(\"message\")\n",
    "    if not isinstance(content, str):\n",
    "        return None\n",
    "    f = _parse_hdfs_fields(content)\n",
    "    if not f:\n",
    "        f = {\n",
    "            \"timestamp\": parse_ts(str(row.get(\"Time\") or row.get(\"time\") or row.get(\"Timestamp\") or row.get(\"timestamp\"))) or pd.NaT,\n",
    "            \"level\": str(row.get(\"Level\") or row.get(\"level\") or \"INFO\").upper(),\n",
    "            \"component\": row.get(\"Component\") or row.get(\"component\") or \"unknown\",\n",
    "            \"thread\": None,\n",
    "            \"raw_message\": content,\n",
    "        }\n",
    "    f = _backfill_component_if_unknown(f)\n",
    "    blk = RE_HDFS_BLOCK.search(content)\n",
    "    f[\"session_id\"] = blk.group(0) if blk else None\n",
    "    f[\"norm_message\"] = normalize_message(f[\"raw_message\"])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac16756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## OpenStack parser (filename prefix tolerant, robust fields)\n",
    "\n",
    "# %%\n",
    "RE_OPENSTACK_RICH = re.compile(\n",
    "    r\"^(?P<ts>\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2}(?:\\.\\d{3,6})?)\\s+\"\n",
    "    r\"(?:(?P<pid>\\d+)\\s+)?(?:(?P<tid>[^\\s]+)\\s+)?\"\n",
    "    r\"(?P<level>[A-Z]+)\\s+(?P<component>[^\\s:]+)\\s*[:\\-]\\s*(?P<msg>.*)$\"\n",
    ")\n",
    "RE_OPENSTACK_GENERIC = re.compile(\n",
    "    r\"^(?P<ts>\\S+\\s+\\S+)\\s+(?P<level>[A-Z]+)\\s+(?P<component>[\\w\\.\\-\\[\\]/]+)\\s*[:\\-]\\s*(?P<msg>.*)$\"\n",
    ")\n",
    "\n",
    "def _strip_filename_prefix(line: str) -> str:\n",
    "    # OpenStack files sometimes prefix a filename before the timestamp\n",
    "    # e.g., \"nova-api.log.1.2017-05-16_13:53:08 2017-05-16 00:00:00.008 ...\"\n",
    "    p0 = line.split(\" \", 1)[0]\n",
    "    if p0.endswith(\".log\") or \".log.\" in p0:\n",
    "        parts = line.split(\" \", 1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "    return line\n",
    "\n",
    "def parse_openstack_line(line: str) -> Optional[Dict[str, Any]]:\n",
    "    line = _strip_filename_prefix(line.strip())\n",
    "    m = RE_OPENSTACK_RICH.match(line) or RE_OPENSTACK_GENERIC.match(line)\n",
    "    if not m:\n",
    "        parts = line.split(\" \", 5)\n",
    "        if len(parts) >= 6:\n",
    "            ts = parse_ts(f\"{parts[0]} {parts[1]}\") or pd.NaT\n",
    "            pid = parts[2] if parts[2].isdigit() else None\n",
    "            level = parts[3] if parts[3].isupper() else \"INFO\"\n",
    "            component = parts[4]; msg = parts[5]; tid = None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        ts = parse_ts(m.group(\"ts\")) or pd.NaT\n",
    "        level = m.group(\"level\"); component = m.group(\"component\"); msg = m.group(\"msg\")\n",
    "        pid = m.groupdict().get(\"pid\"); tid = m.groupdict().get(\"tid\")\n",
    "\n",
    "    mr = RE_OPENSTACK_REQ.search(line)\n",
    "    req = mr.group(0) if mr else None\n",
    "\n",
    "    return {\n",
    "        \"timestamp\": ts, \"level\": level, \"component\": component,\n",
    "        \"pid\": pid, \"thread\": tid, \"session_id\": req,\n",
    "        \"raw_message\": msg, \"norm_message\": normalize_message(msg),\n",
    "    }\n",
    "\n",
    "def parse_openstack_csv_row(row: pd.Series) -> Optional[Dict[str, Any]]:\n",
    "    content = row.get(\"Content\") or row.get(\"content\") or row.get(\"Message\") or row.get(\"message\")\n",
    "    if not isinstance(content, str):\n",
    "        return None\n",
    "    ts = row.get(\"Time\") or row.get(\"time\") or row.get(\"Timestamp\") or row.get(\"timestamp\")\n",
    "    ts = parse_ts(str(ts)) if ts is not None else pd.NaT\n",
    "    level = str(row.get(\"Level\") or row.get(\"level\") or \"INFO\").upper()\n",
    "    component = row.get(\"Component\") or row.get(\"component\") or \"unknown\"\n",
    "    mr = RE_OPENSTACK_REQ.search(content)\n",
    "    req = mr.group(0) if mr else None\n",
    "    return {\n",
    "        \"timestamp\": ts, \"level\": level, \"component\": component,\n",
    "        \"pid\": str(row.get(\"pid\") or row.get(\"PID\")) if pd.notna(row.get(\"pid\") or row.get(\"PID\")) else None,\n",
    "        \"thread\": None, \"session_id\": req,\n",
    "        \"raw_message\": content, \"norm_message\": normalize_message(content),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc2eee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Patch B: make_template_miner that prefers the *existent* fixed ini and explains failures\n",
    "\n",
    "# %%\n",
    "def _pick_drain3_ini():\n",
    "    # Prefer notebooks/configs if present, else root configs\n",
    "    for p in [\n",
    "        REPO_ROOT / \"notebooks\" / \"configs\" / \"drain3.ini\",\n",
    "        REPO_ROOT / \"configs\" / \"drain3.ini\",\n",
    "    ]:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def make_template_miner(persist_dir: Path):\n",
    "    persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ini_path = _pick_drain3_ini()\n",
    "    print(f\"[drain3] Using ini: {ini_path}\" if ini_path else \"[drain3] No ini found\")\n",
    "    try:\n",
    "        from drain3 import TemplateMiner\n",
    "        from drain3.file_persistence import FilePersistence\n",
    "        from drain3.template_miner_config import TemplateMinerConfig\n",
    "        cfg = TemplateMinerConfig()\n",
    "        if ini_path:\n",
    "            cfg.load(str(ini_path))\n",
    "        miner = TemplateMiner(FilePersistence(str(persist_dir / \"drain3_state.json\")), config=cfg)\n",
    "        print(\"[drain3] TemplateMiner initialised.\")\n",
    "        return miner, True\n",
    "    except Exception as e:\n",
    "        print(\"[drain3] FAILED, falling back to SimpleTemplateMiner:\", e)\n",
    "        class SimpleTemplateMiner:\n",
    "            # Minimal pass-through: the *normalized* message is the template\n",
    "            def add_log_message(self, msg: str):\n",
    "                return msg\n",
    "        return SimpleTemplateMiner(), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4494ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Node.js / generic JSON log parser ===\n",
    "RE_NODE_TEXT = re.compile(\n",
    "    r\"^(?P<ts>\\d{4}-\\d{2}-\\d{2}[ T]\\d{2}:\\d{2}:\\d{2}(?:\\.\\d{3,6})?Z?)\\s+\"\n",
    "    r\"(?P<level>[A-Z]+|info|warn|error|debug|trace)\\s+\"\n",
    "    r\"(?P<component>[^:]+?):\\s*(?P<msg>.*)$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "def parse_nodejs_line(line: str):\n",
    "    if not line:\n",
    "        return None\n",
    "    line = line.strip()\n",
    "    # JSON logs\n",
    "    if line.startswith('{') and line.endswith('}'):\n",
    "        try:\n",
    "            payload = json.loads(line)\n",
    "        except Exception:\n",
    "            payload = {}\n",
    "        ts = payload.get('time') or payload.get('@timestamp') or payload.get('timestamp')\n",
    "        level = str(payload.get('level') or payload.get('severity') or 'INFO').upper()\n",
    "        component = payload.get('service') or payload.get('component') or payload.get('logger') or 'node'\n",
    "        session = payload.get('reqId') or payload.get('requestId') or payload.get('session_id')\n",
    "        message = payload.get('msg') or payload.get('message') or line\n",
    "        if isinstance(ts, (int, float)):\n",
    "            ts = pd.to_datetime(ts, unit='ms', errors='coerce')\n",
    "        else:\n",
    "            ts = parse_ts(str(ts)) if ts else pd.NaT\n",
    "        return {\n",
    "            'timestamp': ts,\n",
    "            'level': level,\n",
    "            'component': component,\n",
    "            'thread': None,\n",
    "            'session_id': session,\n",
    "            'raw_message': message,\n",
    "            'norm_message': normalize_message(message),\n",
    "        }\n",
    "    # Text logs\n",
    "    m = RE_NODE_TEXT.match(line)\n",
    "    if m:\n",
    "        ts = parse_ts(m.group('ts')) or pd.NaT\n",
    "        level = m.group('level').upper()\n",
    "        component = m.group('component')\n",
    "        msg = m.group('msg')\n",
    "    else:\n",
    "        parts = line.split(' ', 2)\n",
    "        if len(parts) < 3:\n",
    "            return None\n",
    "        ts = parse_ts(parts[0]) or pd.NaT\n",
    "        level = parts[1].upper()\n",
    "        msg = parts[2]\n",
    "        component = 'node'\n",
    "    req = RE_OPENSTACK_REQ.search(line)\n",
    "    session_id = req.group(0) if req else None\n",
    "    return {\n",
    "        'timestamp': ts,\n",
    "        'level': level,\n",
    "        'component': component,\n",
    "        'thread': None,\n",
    "        'session_id': session_id,\n",
    "        'raw_message': msg,\n",
    "        'norm_message': normalize_message(msg),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f48846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## IO helpers: compressed input + safe line iterator\n",
    "\n",
    "# %%\n",
    "import gzip, bz2, lzma\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "def open_maybe_compressed(path: Path):\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".gz\":\n",
    "        return gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    if suf in (\".bz2\", \".bz\"):\n",
    "        return bz2.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    if suf in (\".xz\", \".lzma\"):\n",
    "        return lzma.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def yield_lines(path: Path) -> Iterable[str]:\n",
    "    with open_maybe_compressed(path) as f:\n",
    "        for line in f:\n",
    "            yield line.rstrip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e88bc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Template mining pipeline (Path A)\n",
    "\n",
    "# %%\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "WINDOW_LEN = int(data_config.get('preprocessing', {}).get('window_len', 100))\n",
    "\n",
    "@dataclass\n",
    "class ParseHooks:\n",
    "    row_parser: Optional[Any] = None\n",
    "    line_parser: Optional[Any] = None\n",
    "    name: str = 'dataset'\n",
    "\n",
    "def parse_log(path: Path, hooks: ParseHooks) -> pd.DataFrame:\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    if hooks.line_parser:\n",
    "        for line in yield_lines(path):\n",
    "            parsed = hooks.line_parser(line)\n",
    "            if parsed:\n",
    "                parsed['source_file'] = path.name\n",
    "                records.append(parsed)\n",
    "    elif hooks.row_parser:\n",
    "        df = pd.read_csv(path)\n",
    "        for _, row in df.iterrows():\n",
    "            parsed = hooks.row_parser(row)\n",
    "            if parsed:\n",
    "                parsed['source_file'] = path.name\n",
    "                records.append(parsed)\n",
    "    if not records:\n",
    "        return pd.DataFrame(columns=['timestamp','session_id','norm_message','raw_message','source_file'])\n",
    "    df = pd.DataFrame(records)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df = df.dropna(subset=['timestamp'])\n",
    "    return df\n",
    "\n",
    "def ensure_session_ids(df: pd.DataFrame, prefix: str) -> pd.DataFrame:\n",
    "    if 'session_id' not in df.columns:\n",
    "        df['session_id'] = None\n",
    "    missing = df['session_id'].isna() | (df['session_id'].astype(str).str.len() == 0)\n",
    "    if missing.any():\n",
    "        # generate stable pseudo sessions within file order\n",
    "        df.loc[missing, 'session_id'] = [f\"{prefix}-{i}\" for i in range(missing.sum())]\n",
    "    return df\n",
    "\n",
    "def assign_templates(df: pd.DataFrame, miner, is_drain3: bool):\n",
    "    template_to_id: Dict[str, int] = {}\n",
    "    id_to_template: List[str] = []\n",
    "    counts = defaultdict(int)\n",
    "    tids = []\n",
    "    for msg in df['norm_message']:\n",
    "        if is_drain3:\n",
    "            res = miner.add_log_message(msg)\n",
    "            # Drain3 returns dict with \"template_mined\"; fall back to msg if missing\n",
    "            template = (res.get('template_mined') if isinstance(res, dict) else None) or msg\n",
    "        else:\n",
    "            template = miner.add_log_message(msg)\n",
    "        if template not in template_to_id:\n",
    "            template_to_id[template] = len(id_to_template)\n",
    "            id_to_template.append(template)\n",
    "        tid = template_to_id[template]\n",
    "        counts[tid] += 1\n",
    "        tids.append(tid)\n",
    "    df['template_id'] = tids\n",
    "    vocab = {\n",
    "        'id_to_template': id_to_template,\n",
    "        'template_to_id': template_to_id,\n",
    "        'counts': {str(k): int(v) for k, v in counts.items()}\n",
    "    }\n",
    "    return df, vocab\n",
    "\n",
    "def build_sequences(df: pd.DataFrame, prefix: str) -> List[Dict[str, Any]]:\n",
    "    df = ensure_session_ids(df, prefix).sort_values(['session_id','timestamp'])\n",
    "    sequences: List[Dict[str, Any]] = []\n",
    "    for session_id, g in df.groupby('session_id', sort=False):\n",
    "        tids = g['template_id'].tolist()\n",
    "        tss  = g['timestamp'].tolist()\n",
    "        if not tids:\n",
    "            continue\n",
    "        # fixed windows of WINDOW_LEN in event order\n",
    "        for i in range(0, len(tids), WINDOW_LEN):\n",
    "            win = tids[i:i+WINDOW_LEN]\n",
    "            st  = tss[i]\n",
    "            en  = tss[min(i+len(win)-1, len(tss)-1)]\n",
    "            sequences.append({'session_id': session_id, 'templates': win, 'start_time': st, 'end_time': en})\n",
    "    return sequences\n",
    "\n",
    "def split_sequences(records: List[Dict[str, Any]], splits: Dict[str, float]):\n",
    "    cols = ['session_id', 'templates', 'start_time', 'end_time']\n",
    "    if not records:\n",
    "        empty = pd.DataFrame(columns=cols)\n",
    "        return {'train': empty.copy(), 'val': empty.copy(), 'test': empty.copy()}\n",
    "    records = sorted(records, key=lambda r: r['start_time'])\n",
    "    n = len(records)\n",
    "    train_end = max(1, int(n * splits.get('train', 0.8)))\n",
    "    val_end   = max(train_end + 1, int(n * (splits.get('train', 0.8) + splits.get('val', 0.1))))\n",
    "    train = pd.DataFrame(records[:train_end], columns=cols)\n",
    "    val   = pd.DataFrame(records[train_end:val_end], columns=cols)\n",
    "    test  = pd.DataFrame(records[val_end:], columns=cols)\n",
    "    return {'train': train, 'val': val, 'test': test}\n",
    "\n",
    "def save_artifacts(out_dir: Path, splits: Dict[str, pd.DataFrame], vocab: Dict[str, Any]):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for name, df in splits.items():\n",
    "        (out_dir / f'{name}.parquet').unlink(missing_ok=True)\n",
    "        df.to_parquet(out_dir / f'{name}.parquet', index=False)\n",
    "    vocab['created_at'] = pd.Timestamp.utcnow().isoformat() + 'Z'\n",
    "    (out_dir / 'template_vocab.json').write_text(json.dumps(vocab, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "448c8a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## OpenStack labels (clean word boundaries)\n",
    "\n",
    "# %%\n",
    "RE_OPENSTACK_REQ = re.compile(r\"\\breq-[0-9a-fA-F\\-]{8,}\\b\", re.IGNORECASE)\n",
    "\n",
    "def read_openstack_labels(label_path: Path) -> Dict[str, int]:\n",
    "    labels: Dict[str, int] = {}\n",
    "    if not label_path.exists():\n",
    "        print(f\"WARNING: label file not found: {label_path}\")\n",
    "        return labels\n",
    "\n",
    "    def _infer_label(text: str) -> Optional[int]:\n",
    "        t = text.strip().lower()\n",
    "        if \"abnormal\" in t or \"anomaly\" in t or re.search(r\"\\b1\\b\", t):\n",
    "            return 1\n",
    "        if \"normal\" in t or re.search(r\"\\b0\\b\", t):\n",
    "            return 0\n",
    "        return None\n",
    "\n",
    "    with open(label_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        for line in fh:\n",
    "            if not line.strip() or line.lstrip().startswith((\"#\",\"//\")):\n",
    "                continue\n",
    "            reqs = RE_OPENSTACK_REQ.findall(line)\n",
    "            if not reqs:\n",
    "                parts = re.split(r\"[\\s,]+\", line.strip())\n",
    "                reqs = [p for p in parts if RE_OPENSTACK_REQ.fullmatch(p)]\n",
    "            if not reqs:\n",
    "                continue\n",
    "            lab = _infer_label(line)\n",
    "            if lab is None:\n",
    "                lab = 1\n",
    "            for rid in reqs:\n",
    "                labels[rid] = lab\n",
    "    print(f\"Loaded labels for {len(labels)} request IDs from {label_path.name}\")\n",
    "    return labels\n",
    "\n",
    "def attach_true_labels(out_dir: Path, label_path: Path, suffix: str = \"_truth\"):\n",
    "    labmap = read_openstack_labels(label_path)\n",
    "    if not labmap:\n",
    "        print(\"No labels found; skipping truth attachment.\")\n",
    "        return\n",
    "    for split in [\"train\",\"val\",\"test\"]:\n",
    "        ip = out_dir / f\"{split}.parquet\"\n",
    "        if not ip.exists():\n",
    "            continue\n",
    "        df = pd.read_parquet(ip)\n",
    "        df[\"label\"] = df[\"session_id\"].map(labmap).fillna(0).astype(int)\n",
    "        op = out_dir / f\"{split}{suffix}.parquet\"\n",
    "        df.to_parquet(op, index=False)\n",
    "        print(f\"Annotated {op.name} with {int(df['label'].sum())} anomalies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c495c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Dataset runners (Path A)\n",
    "\n",
    "# %%\n",
    "SPLITS = data_config.get('splits', {'train': 0.8, 'val': 0.1, 'test': 0.1})\n",
    "FALLBACK_ROOTS = [REPO_ROOT / 'notebooks', REPO_ROOT / 'data']\n",
    "\n",
    "def _reset_out_dir(out_dir: Path):\n",
    "    if out_dir.exists():\n",
    "        for name in [\"sequences_raw.parquet\",\"train.parquet\",\"val.parquet\",\"test.parquet\",\n",
    "                     \"template_vocab.json\",\"train_truth.parquet\",\"val_truth.parquet\",\"test_truth.parquet\"]:\n",
    "            (out_dir / name).unlink(missing_ok=True)\n",
    "        state_dir = out_dir / \"templates_state\"\n",
    "        if state_dir.exists():\n",
    "            shutil.rmtree(state_dir, ignore_errors=True)\n",
    "\n",
    "def _resolve_path(path: Path) -> Optional[Path]:\n",
    "    if path.exists():\n",
    "        return path\n",
    "    cands: List[Path] = []\n",
    "    for root in FALLBACK_ROOTS:\n",
    "        cands.append(root / path.name)\n",
    "        if not path.is_absolute():\n",
    "            cands.append(root / path)\n",
    "        else:\n",
    "            try:\n",
    "                rel = path.relative_to(REPO_ROOT)\n",
    "                cands.append(root / rel)\n",
    "            except Exception:\n",
    "                pass\n",
    "    for cand in cands:\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "def _collect_logs(paths: List[Path], hooks: ParseHooks, prefix: str) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for path in paths:\n",
    "        resolved = _resolve_path(path)\n",
    "        if resolved is None:\n",
    "            print(f\"[skip] Missing log file: {path}\")\n",
    "            continue\n",
    "        if resolved != path:\n",
    "            print(f\"[info] Using fallback log at {resolved} for {path.name}\")\n",
    "        print(f\"Parsing {resolved.name} ...\")\n",
    "        df = parse_log(resolved, hooks)\n",
    "        if not df.empty:\n",
    "            frames.append(df)\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=['timestamp','session_id','norm_message','raw_message','source_file'])\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined = combined.sort_values('timestamp')\n",
    "    combined = combined.dropna(subset=['norm_message'])\n",
    "    return ensure_session_ids(combined, prefix)\n",
    "\n",
    "def process_dataset(paths: List[Path], hooks: ParseHooks, out_dir: Path, prefix: str, label_path: Optional[Path] = None):\n",
    "    _reset_out_dir(out_dir)\n",
    "    df = _collect_logs(paths, hooks, prefix)\n",
    "    if df.empty:\n",
    "        print('No records parsed; aborting.')\n",
    "        return\n",
    "    miner, is_drain3 = make_template_miner(out_dir / \"templates_state\")\n",
    "    df, vocab = assign_templates(df, miner, is_drain3)\n",
    "    df[['timestamp','session_id','template_id','source_file']].to_parquet(out_dir / \"sequences_raw.parquet\", index=False)\n",
    "\n",
    "    # Build sequences & splits\n",
    "    sequences = build_sequences(df, prefix)\n",
    "    splits = split_sequences(sequences, SPLITS)\n",
    "    save_artifacts(out_dir, splits, vocab)\n",
    "\n",
    "    # Optional label attachment (OpenStack)\n",
    "    if label_path is not None:\n",
    "        resolved = _resolve_path(label_path)\n",
    "        if resolved and resolved.exists():\n",
    "            attach_true_labels(out_dir, resolved)\n",
    "        else:\n",
    "            print(f\"Label file not found: {label_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2334b8",
   "metadata": {},
   "source": [
    "Patch C: Guard against duplicate dataset runs in this kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0829e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"___DATASETS_PROCESSED___\" not in globals():\n",
    "    ___DATASETS_PROCESSED___ = set()\n",
    "\n",
    "def _should_run_dataset(name: str) -> bool:\n",
    "    if name in ___DATASETS_PROCESSED___:\n",
    "        print(f\"[guard] Skipping duplicate run for dataset: {name}\")\n",
    "        return False\n",
    "    ___DATASETS_PROCESSED___.add(name)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd495ad6",
   "metadata": {},
   "source": [
    "Patch D: Diagnose anomaly_labels.txt parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "133037ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_openstack_labels(label_path: Path, max_lines=40):\n",
    "    if not label_path or not label_path.exists():\n",
    "        print(\"[labels] File missing:\", label_path); return\n",
    "    print(f\"[labels] Preview first {max_lines} lines of {label_path}:\")\n",
    "    with open(label_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "        lines = [next(fh, \"\") for _ in range(max_lines)]\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        m = re.findall(r\"\\breq-[0-9a-fA-F\\-]{8,}\\b\", line)\n",
    "        tag = f\"  -> REQS: {m}\" if m else \"\"\n",
    "        print(f\"{i:02d}: {line.rstrip()}{tag}\")\n",
    "\n",
    "# Example usage (uncomment if you have OPENSTACK_LABELS_PATH variable):\n",
    "# debug_openstack_labels(REPO_ROOT / 'data/openstack/raw/anomaly_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0e46b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hdfs] Starting preprocessing (hdfs) ...\n",
      "Parsing HDFS.log ...\n",
      "[drain3] Using ini: /home/tpi/distil_shahreyar/notebooks/configs/drain3.ini\n",
      "[drain3] TemplateMiner initialised.\n",
      "[drain3] Using ini: /home/tpi/distil_shahreyar/notebooks/configs/drain3.ini\n",
      "[drain3] TemplateMiner initialised.\n",
      "[openstack] Starting preprocessing (openstack) ...\n",
      "Parsing openstack_normal1.log ...\n",
      "[openstack] Starting preprocessing (openstack) ...\n",
      "Parsing openstack_normal1.log ...\n",
      "Parsing openstack_normal2.log ...\n",
      "Parsing openstack_normal2.log ...\n",
      "Parsing openstack_abnormal.log ...\n",
      "Parsing openstack_abnormal.log ...\n",
      "[drain3] Using ini: /home/tpi/distil_shahreyar/notebooks/configs/drain3.ini\n",
      "[drain3] TemplateMiner initialised.\n",
      "[drain3] Using ini: /home/tpi/distil_shahreyar/notebooks/configs/drain3.ini\n",
      "[drain3] TemplateMiner initialised.\n",
      "Loaded labels for 0 request IDs from anomaly_labels.txt\n",
      "No labels found; skipping truth attachment.\n",
      "[nodejs_example] Skipping (disabled in config).\n",
      "Loaded labels for 0 request IDs from anomaly_labels.txt\n",
      "No labels found; skipping truth attachment.\n",
      "[nodejs_example] Skipping (disabled in config).\n"
     ]
    }
   ],
   "source": [
    "# === Execute preprocessing for configured datasets ===\n",
    "dataset_specs = data_config.get('datasets', {})\n",
    "if not dataset_specs:\n",
    "    raise RuntimeError('No datasets defined in configs/data.yaml')\n",
    "\n",
    "for name, spec in dataset_specs.items():\n",
    "    if spec.get('enabled', True) is False:\n",
    "        print(f\"[{name}] Skipping (disabled in config).\")\n",
    "        continue\n",
    "    if not _should_run_dataset(name):\n",
    "        continue\n",
    "    dtype = spec.get('type')\n",
    "    inputs = spec.get('inputs', {}).get('logs', [])\n",
    "    if not inputs:\n",
    "        print(f\"[{name}] No input logs defined; skipping.\")\n",
    "        continue\n",
    "    log_paths = [Path(p) if Path(p).is_absolute() else (REPO_ROOT / p).resolve() for p in inputs]\n",
    "    output_dir = (REPO_ROOT / spec.get('output_dir', f'artifacts/{name}')).resolve()\n",
    "    prefix = spec.get('session_prefix', name)\n",
    "    label_path = None\n",
    "    labels_cfg = spec.get('labels') or {}\n",
    "    if 'request_ids' in labels_cfg:\n",
    "        raw = Path(labels_cfg['request_ids'])\n",
    "        label_path = raw if raw.is_absolute() else (REPO_ROOT / raw).resolve()\n",
    "\n",
    "    if dtype == 'hdfs':\n",
    "        hooks = ParseHooks(line_parser=parse_hdfs_line, name='HDFS')\n",
    "    elif dtype == 'openstack':\n",
    "        hooks = ParseHooks(line_parser=parse_openstack_line, name='OpenStack')\n",
    "    elif dtype == 'nodejs':\n",
    "        hooks = ParseHooks(line_parser=parse_nodejs_line, name='NodeJS')\n",
    "    else:\n",
    "        print(f\"[{name}] Unknown dataset type '{dtype}'; skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{name}] Starting preprocessing ({dtype}) ...\")\n",
    "    process_dataset(log_paths, hooks, output_dir, prefix=prefix, label_path=label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c5152",
   "metadata": {},
   "source": [
    "## Attach OpenStack Anomaly Labels (Instance UUID → Request ID Mapping)\n",
    "\n",
    "After preprocessing creates the parquet files, we need to map the instance UUIDs from `anomaly_labels.txt` to the request IDs used for sessionization. This cell:\n",
    "1. Scans OpenStack logs to build a mapping between instance UUIDs and request IDs\n",
    "2. Uses both direct co-occurrence (same line) and sliding window (±5 lines)\n",
    "3. Labels sequences where the session_id (req-...) is linked to an anomalous instance\n",
    "4. Outputs `*_truth.parquet` files with the `label` column added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d096b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[label_attach] Logs dir: /home/tpi/distil_shahreyar/data/openstack/raw\n",
      "[label_attach] Splits dir: /home/tpi/distil_shahreyar/artifacts/openstack_finetune\n",
      "[label_attach] Using logs:\n",
      "               - openstack_normal.log\n",
      "               - openstack_normal1.log\n",
      "               - openstack_normal2.log\n",
      "               - openstack_abnormal.log\n",
      "[label_attach] Found 4 anomalous instance UUIDs\n",
      "               Sample: ['1643649d-2f42-4303-bfcd-7798baec19f9', '544fd51c-4edc-4780-baae-ba1d80a0acfc', 'a445709b-6ad0-40ec-8860-bec60b6ca0c2']\n",
      "[label_attach] Pass 1 (same-line): 2067 instances → 6198 requests\n",
      "[label_attach] Pass 1 (same-line): 2067 instances → 6198 requests\n",
      "[label_attach] Pass 2 (±5 window): 2069 instances → 65651 requests\n",
      "[label_attach] [train] Labeled 561/85351 (0.7%) as anomalies → train_truth.parquet\n",
      "[label_attach] [val] Labeled 51/10669 (0.5%) as anomalies → val_truth.parquet\n",
      "[label_attach] [test] Labeled 184/10669 (1.7%) as anomalies → test_truth.parquet\n",
      "[label_attach] ✓ Complete!\n",
      "[label_attach] If all counts are 0, req-IDs and instances may not co-occur in logs.\n",
      "[label_attach] Alternative: sessionize by instance UUID instead of req-ID.\n",
      "[label_attach] Pass 2 (±5 window): 2069 instances → 65651 requests\n",
      "[label_attach] [train] Labeled 561/85351 (0.7%) as anomalies → train_truth.parquet\n",
      "[label_attach] [val] Labeled 51/10669 (0.5%) as anomalies → val_truth.parquet\n",
      "[label_attach] [test] Labeled 184/10669 (1.7%) as anomalies → test_truth.parquet\n",
      "[label_attach] ✓ Complete!\n",
      "[label_attach] If all counts are 0, req-IDs and instances may not co-occur in logs.\n",
      "[label_attach] Alternative: sessionize by instance UUID instead of req-ID.\n"
     ]
    }
   ],
   "source": [
    "# Attach OpenStack labels when anomaly_labels.txt lists *instance UUIDs* (not req-IDs).\n",
    "# Scans logs in data/openstack/raw, builds instance<->req map (co-occur + ±5 line window),\n",
    "# then labels sequences in artifacts/openstack_finetune by session_id (req-...).\n",
    "\n",
    "from collections import defaultdict, deque\n",
    "import re\n",
    "\n",
    "# --- Paths ---\n",
    "LOG_DIR = (REPO_ROOT / \"data\" / \"openstack\" / \"raw\").resolve()\n",
    "LABEL_FILE = LOG_DIR / \"anomaly_labels.txt\"\n",
    "\n",
    "# Split dir candidates (first existing wins)\n",
    "SPLIT_DIRS = [\n",
    "    REPO_ROOT / \"artifacts\" / \"openstack_finetune\",\n",
    "    REPO_ROOT / \"artifacts\" / \"openstack\",  # fallback\n",
    "]\n",
    "OPENSTACK_OUT = next((p for p in SPLIT_DIRS if (p / \"train.parquet\").exists()), None)\n",
    "\n",
    "if OPENSTACK_OUT is None:\n",
    "    print(\"[label_attach] OpenStack splits not found; skipping label attachment\")\n",
    "    print(\"               Expected artifacts/openstack_finetune/{train,val,test}.parquet\")\n",
    "else:\n",
    "    print(f\"[label_attach] Logs dir: {LOG_DIR}\")\n",
    "    print(f\"[label_attach] Splits dir: {OPENSTACK_OUT}\")\n",
    "    \n",
    "    # --- Regexes ---\n",
    "    RE_REQ   = re.compile(r\"\\breq-[0-9a-fA-F\\-]{8,}\\b\", re.IGNORECASE)\n",
    "    RE_INST1 = re.compile(r\"\\[instance:\\s*([0-9a-fA-F\\-]{36})\\]\", re.IGNORECASE)\n",
    "    RE_INST2 = re.compile(r\"\\binstance[=\\s:]+([0-9a-fA-F\\-]{36})\\b\", re.IGNORECASE)\n",
    "    RE_UUID  = re.compile(r\"\\b[0-9a-fA-F]{8}(?:-[0-9a-fA-F]{4}){3}-[0-9a-fA-F]{12}\\b\")\n",
    "    \n",
    "    # --- Collect logs present (normal, normal1/2, abnormal) ---\n",
    "    log_names = [\"openstack_normal.log\", \"openstack_normal1.log\", \"openstack_normal2.log\", \"openstack_abnormal.log\"]\n",
    "    log_files = [LOG_DIR / n for n in log_names if (LOG_DIR / n).exists()]\n",
    "    \n",
    "    if not log_files:\n",
    "        print(f\"[label_attach] No OpenStack logs found in {LOG_DIR}\")\n",
    "    elif not LABEL_FILE.exists():\n",
    "        print(f\"[label_attach] Label file not found: {LABEL_FILE}\")\n",
    "    else:\n",
    "        print(\"[label_attach] Using logs:\")\n",
    "        for lf in log_files:\n",
    "            print(f\"               - {lf.name}\")\n",
    "        \n",
    "        # --- Read anomaly instance UUIDs ---\n",
    "        anom_instances = set(u.lower() for u in RE_UUID.findall(LABEL_FILE.read_text(encoding=\"utf-8\", errors=\"ignore\")))\n",
    "        print(f\"[label_attach] Found {len(anom_instances)} anomalous instance UUIDs\")\n",
    "        if anom_instances:\n",
    "            print(f\"               Sample: {sorted(list(anom_instances))[:3]}\")\n",
    "        \n",
    "        # --- Pass 1: build direct co-occurrence map (instance + req on same line) ---\n",
    "        inst_to_reqs = defaultdict(set)\n",
    "        req_to_insts = defaultdict(set)\n",
    "        \n",
    "        def _update_maps(insts, reqs):\n",
    "            for i in insts:\n",
    "                for r in reqs:\n",
    "                    inst_to_reqs[i].add(r)\n",
    "                    req_to_insts[r].add(i)\n",
    "        \n",
    "        for lf in log_files:\n",
    "            with lf.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                for line in fh:\n",
    "                    reqs = [m.lower() for m in RE_REQ.findall(line)]\n",
    "                    insts = []\n",
    "                    m1 = RE_INST1.search(line)\n",
    "                    m2 = RE_INST2.search(line)\n",
    "                    if m1: insts.append(m1.group(1).lower())\n",
    "                    if m2: insts.append(m2.group(1).lower())\n",
    "                    if reqs and insts:\n",
    "                        _update_maps(insts, reqs)\n",
    "        \n",
    "        print(f\"[label_attach] Pass 1 (same-line): {len(inst_to_reqs)} instances → {len(req_to_insts)} requests\")\n",
    "        \n",
    "        # --- Pass 2: sliding window (±5 lines) to catch near-by mentions ---\n",
    "        WINDOW = 5\n",
    "        for lf in log_files:\n",
    "            buf = deque(maxlen=WINDOW)\n",
    "            with lf.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
    "                # Prime buffer\n",
    "                for _ in range(WINDOW):\n",
    "                    line = fh.readline()\n",
    "                    if line:\n",
    "                        buf.append(line)\n",
    "                # Sliding over file\n",
    "                for line in fh:\n",
    "                    window_lines = list(buf) + [line]\n",
    "                    reqs = set()\n",
    "                    insts = set()\n",
    "                    for L in window_lines:\n",
    "                        reqs.update(m.lower() for m in RE_REQ.findall(L))\n",
    "                        m1 = RE_INST1.search(L)\n",
    "                        m2 = RE_INST2.search(L)\n",
    "                        if m1: insts.add(m1.group(1).lower())\n",
    "                        if m2: insts.add(m2.group(1).lower())\n",
    "                    if reqs and insts:\n",
    "                        _update_maps(insts, reqs)\n",
    "                    buf.append(line)\n",
    "        \n",
    "        print(f\"[label_attach] Pass 2 (±5 window): {len(inst_to_reqs)} instances → {len(req_to_insts)} requests\")\n",
    "        \n",
    "        # --- Label splits: session_id is req-id; label=1 if any linked instance is anomalous ---\n",
    "        def label_split(split):\n",
    "            p = OPENSTACK_OUT / f\"{split}.parquet\"\n",
    "            if not p.exists():\n",
    "                print(f\"[label_attach] [{split}] File not found: {p}\")\n",
    "                return\n",
    "            \n",
    "            df = pd.read_parquet(p)\n",
    "            \n",
    "            def get_label(sid):\n",
    "                if pd.isna(sid):\n",
    "                    return 0\n",
    "                sid = str(sid).lower()\n",
    "                insts = req_to_insts.get(sid, set())\n",
    "                return int(any(i in anom_instances for i in insts))\n",
    "            \n",
    "            df[\"label\"] = df[\"session_id\"].map(get_label).fillna(0).astype(int)\n",
    "            \n",
    "            out = OPENSTACK_OUT / f\"{split}_truth.parquet\"\n",
    "            df.to_parquet(out, index=False)\n",
    "            \n",
    "            pos = int(df[\"label\"].sum())\n",
    "            total = len(df)\n",
    "            pct = (pos / total * 100) if total > 0 else 0\n",
    "            print(f\"[label_attach] [{split}] Labeled {pos}/{total} ({pct:.1f}%) as anomalies → {out.name}\")\n",
    "        \n",
    "        for sp in (\"train\", \"val\", \"test\"):\n",
    "            label_split(sp)\n",
    "        \n",
    "        print(\"[label_attach] ✓ Complete!\")\n",
    "        print(\"[label_attach] If all counts are 0, req-IDs and instances may not co-occur in logs.\")\n",
    "        print(\"[label_attach] Alternative: sessionize by instance UUID instead of req-ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55f6bd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: hdfs\n",
      "== hdfs_pretrain ==\n",
      "  train.parquet: True /home/tpi/distil_shahreyar/artifacts/hdfs_pretrain/train.parquet\n",
      "  val.parquet: True /home/tpi/distil_shahreyar/artifacts/hdfs_pretrain/val.parquet\n",
      "  test.parquet: True /home/tpi/distil_shahreyar/artifacts/hdfs_pretrain/test.parquet\n",
      "  template_vocab.json: True /home/tpi/distil_shahreyar/artifacts/hdfs_pretrain/template_vocab.json\n",
      "  train rows: 460119\n",
      "  vocab size: 118\n",
      "Top 5 templates:\n",
      "Dataset: openstack\n",
      "== openstack_finetune ==\n",
      "  train.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/train.parquet\n",
      "  val.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/val.parquet\n",
      "  test.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/test.parquet\n",
      "  template_vocab.json: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/template_vocab.json\n",
      "  train rows: 85351\n",
      "  vocab size: 158\n",
      "Top 5 templates:\n",
      "Top 5 templates:\n",
      "Dataset: openstack\n",
      "== openstack_finetune ==\n",
      "  train.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/train.parquet\n",
      "  val.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/val.parquet\n",
      "  test.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/test.parquet\n",
      "  template_vocab.json: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/template_vocab.json\n",
      "  train rows: 85351\n",
      "  vocab size: 158\n",
      "Top 5 templates:\n"
     ]
    }
   ],
   "source": [
    "# === Sanity checks ===\n",
    "def summarize(dirpath: Path):\n",
    "    print(f\"== {dirpath.name} ==\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        p = dirpath / f'{split}.parquet'\n",
    "        print(f\"  {split}.parquet:\", p.exists(), str(p) if p.exists() else '')\n",
    "    vocab_path = dirpath / 'template_vocab.json'\n",
    "    print('  template_vocab.json:', vocab_path.exists(), str(vocab_path) if vocab_path.exists() else '')\n",
    "    if (dirpath / 'train.parquet').exists():\n",
    "        pf = pq.ParquetFile(str(dirpath / 'train.parquet'))\n",
    "        print('  train rows:', pf.metadata.num_rows)\n",
    "    if vocab_path.exists():\n",
    "        vocab = json.loads(vocab_path.read_text())\n",
    "        print('  vocab size:', len(vocab.get('id_to_template', [])))\n",
    "\n",
    "\n",
    "def top_templates(dirpath: Path, k: int = 5):\n",
    "    vocab_path = dirpath / 'template_vocab.json'\n",
    "    if not vocab_path.exists():\n",
    "        return\n",
    "    vocab = json.loads(vocab_path.read_text())\n",
    "    id2t = vocab.get('id_to_template', [])\n",
    "    train_path = dirpath / 'train.parquet'\n",
    "    if not train_path.exists():\n",
    "        return\n",
    "    df = pd.read_parquet(train_path, columns=['templates'])\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "    for seq in df['templates']:\n",
    "        if isinstance(seq, list):\n",
    "            counter.update(seq)\n",
    "    print(f'Top {k} templates:')\n",
    "    for tid, count in counter.most_common(k):\n",
    "        desc = id2t[tid] if tid < len(id2t) else '<UNK>'\n",
    "        print(f'  [{tid}] x{count} :: {desc[:120]}')\n",
    "\n",
    "for name, spec in data_config.get('datasets', {}).items():\n",
    "    if spec.get('enabled', True) is False:\n",
    "        continue\n",
    "    out_dir = (REPO_ROOT / spec.get('output_dir', f'artifacts/{name}')).resolve()\n",
    "    if not out_dir.exists():\n",
    "        print(f\"== {name} == Output directory missing: {out_dir}\")\n",
    "        continue\n",
    "    print(f\"Dataset: {name}\")\n",
    "    summarize(out_dir)\n",
    "    if (out_dir / 'train.parquet').exists():\n",
    "        top_templates(out_dir, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237ef711",
   "metadata": {},
   "source": [
    "## Preprocessing Summary\n",
    "\n",
    "All patches have been successfully applied:\n",
    "- ✅ **Patch A**: Repaired drain3.ini with single-line JSON (no more parsing errors)\n",
    "- ✅ **Patch B**: Robust miner factory with explicit ini selection\n",
    "- ✅ **Patch C**: Duplicate-run guard to prevent accidental re-runs\n",
    "- ✅ **Patch D**: Label debugging utility\n",
    "- ✅ **IO Helpers**: Compressed file readers for .gz/.bz2 support\n",
    "- ✅ **Updated Parsers**: HDFS (compact format) and OpenStack (filename prefix handling)\n",
    "- ✅ **Updated Normalization**: Correct word boundaries and proper OS_REQ ordering\n",
    "\n",
    "The preprocessing pipeline now uses **Drain3** successfully for template mining!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5f9f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[labels] Preview first 20 lines of /home/tpi/distil_shahreyar/data/openstack/raw/anomaly_labels.txt:\n",
      "01: The following VM instances have injected anomalies as observed in openstack_abnormal.log.\n",
      "02: \n",
      "03: 544fd51c-4edc-4780-baae-ba1d80a0acfc\n",
      "04: ae651dff-c7ad-43d6-ac96-bbcd820ccca8\n",
      "05: a445709b-6ad0-40ec-8860-bec60b6ca0c2\n",
      "06: 1643649d-2f42-4303-bfcd-7798baec19f9\n",
      "07: \n",
      "08: \n",
      "09: \n",
      "10: \n",
      "11: \n",
      "12: \n",
      "13: \n",
      "14: \n",
      "15: \n",
      "16: \n",
      "17: \n",
      "18: \n",
      "19: \n",
      "20: \n"
     ]
    }
   ],
   "source": [
    "# Debug OpenStack labels to see why 0 request IDs were loaded\n",
    "label_file = REPO_ROOT / 'data/openstack/raw/anomaly_labels.txt'\n",
    "if label_file.exists():\n",
    "    debug_openstack_labels(label_file, max_lines=20)\n",
    "else:\n",
    "    print(f\"Label file not found: {label_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87863bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: hdfs\n",
      "== hdfs_pretrain ==\n",
      "  train.parquet: True /home/tpi/distil_shahreyar/artifacts/hdfs_pretrain/train.parquet\n",
      "  val.parquet: True /home/tpi/distil_shahreyar/artifacts/hdfs_pretrain/val.parquet\n",
      "  test.parquet: True /home/tpi/distil_shahreyar/artifacts/hdfs_pretrain/test.parquet\n",
      "  template_vocab.json: True /home/tpi/distil_shahreyar/artifacts/hdfs_pretrain/template_vocab.json\n",
      "  train rows: 460119\n",
      "  vocab size: 118\n",
      "Top 5 templates:\n",
      "Dataset: openstack\n",
      "== openstack_finetune ==\n",
      "  train.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/train.parquet\n",
      "  val.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/val.parquet\n",
      "  test.parquet: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/test.parquet\n",
      "  template_vocab.json: True /home/tpi/distil_shahreyar/artifacts/openstack_finetune/template_vocab.json\n",
      "  train rows: 85351\n",
      "  vocab size: 158\n",
      "Top 5 templates:\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Sanity checks\n",
    "\n",
    "# %%\n",
    "import pyarrow.parquet as pq, json\n",
    "\n",
    "def summarize(dirpath: Path):\n",
    "    print(f\"== {dirpath.name} ==\")\n",
    "    for split in ['train','val','test']:\n",
    "        p = dirpath / f'{split}.parquet'\n",
    "        print(f\"  {split}.parquet:\", p.exists(), str(p) if p.exists() else '')\n",
    "    vocab_path = dirpath / 'template_vocab.json'\n",
    "    print('  template_vocab.json:', vocab_path.exists(), str(vocab_path) if vocab_path.exists() else '')\n",
    "    if (dirpath / 'train.parquet').exists():\n",
    "        pf = pq.ParquetFile(str(dirpath / 'train.parquet'))\n",
    "        print('  train rows:', pf.metadata.num_rows)\n",
    "    if vocab_path.exists():\n",
    "        vocab = json.loads(vocab_path.read_text())\n",
    "        print('  vocab size:', len(vocab.get('id_to_template', [])))\n",
    "\n",
    "def top_templates(dirpath: Path, k: int = 5):\n",
    "    vocab_path = dirpath / 'template_vocab.json'\n",
    "    if not vocab_path.exists():\n",
    "        return\n",
    "    vocab = json.loads(vocab_path.read_text())\n",
    "    id2t = vocab.get('id_to_template', [])\n",
    "    train_path = dirpath / 'train.parquet'\n",
    "    if not train_path.exists():\n",
    "        return\n",
    "    df = pd.read_parquet(train_path, columns=['templates'])\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "    for seq in df['templates']:\n",
    "        if isinstance(seq, list):\n",
    "            counter.update(seq)\n",
    "    print(f'Top {k} templates:')\n",
    "    for tid, count in counter.most_common(k):\n",
    "        desc = id2t[tid] if tid < len(id2t) else '<UNK>'\n",
    "        print(f'  [{tid}] x{count} :: {desc[:120]}')\n",
    "\n",
    "for name, spec in data_config.get('datasets', {}).items():\n",
    "    if spec.get('enabled', True) is False:\n",
    "        continue\n",
    "    out_dir = (REPO_ROOT / spec.get('output_dir', f'artifacts/{name}')).resolve()\n",
    "    if not out_dir.exists():\n",
    "        print(f\"== {name} == Output directory missing: {out_dir}\")\n",
    "        continue\n",
    "    print(f\"Dataset: {name}\")\n",
    "    summarize(out_dir)\n",
    "    if (out_dir / 'train.parquet').exists():\n",
    "        top_templates(out_dir, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log_anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
