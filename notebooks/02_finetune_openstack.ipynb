{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 \u00b7 Fine-tune on OpenStack Logs\n",
        "\n",
        "This notebook adapts the HDFS-pretrained DistilBERT model to OpenStack anomaly detection with optional replay and LoRA support. Hugging Face Accelerate drives training on multi-GPU Linux or falls back to Apple MPS when available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Goals\n",
        "- Load OpenStack fine-tuning hyperparameters and reuse the Accelerate configuration (skipped for MPS).\n",
        "- Optionally replay a slice of HDFS data and/or enable LoRA adapters via config toggles.\n",
        "- Train with early stopping, checkpoint cadence, and GPU/MPS memory hygiene utilities.\n",
        "- Evaluate on validation/test splits with F1, ROC-AUC, PR-AUC, and confusion matrices.\n",
        "- Export TorchScript and ONNX artifacts and capture a MODEL_CARD snippet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_from_disk, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForMaskedLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    get_scheduler\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load YAML configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_yaml(path: Path) -> Dict:\n",
        "    with path.open('r') as fh:\n",
        "        return yaml.safe_load(fh)\n",
        "\n",
        "data_cfg = load_yaml(Path('configs/data.yaml'))\n",
        "train_cfg = load_yaml(Path('configs/train_openstack.yaml'))\n",
        "print('Configs loaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Device detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IS_MPS = torch.backends.mps.is_available()\n",
        "if IS_MPS:\n",
        "    os.environ.setdefault('ACCELERATE_USE_MPS_DEVICE', '1')\n",
        "    print('Apple Silicon (MPS) detected. Accelerate will use the MPS backend.')\n",
        "else:\n",
        "    print('MPS not available; using CUDA/CPU settings from training config.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parquet_dir = Path(data_cfg['preprocessing']['parquet_dir'])\n",
        "metadata_path = Path(data_cfg['preprocessing']['dataset_metadata'])\n",
        "metadata = json.loads(metadata_path.read_text()) if metadata_path.exists() else {}\n",
        "\n",
        "openstack_train = load_from_disk(str(parquet_dir / 'openstack_train_hf'))\n",
        "openstack_val = load_from_disk(str(parquet_dir / 'openstack_val_hf'))\n",
        "openstack_test = load_from_disk(str(parquet_dir / 'openstack_test_hf'))\n",
        "\n",
        "replay_cfg = train_cfg['replay']\n",
        "if replay_cfg['enabled']:\n",
        "    hdfs_dataset = load_from_disk(str(parquet_dir / 'hdfs_train_hf'))\n",
        "    replay_size = max(1, int(len(openstack_train) * replay_cfg['ratio']))\n",
        "    replay_subset = hdfs_dataset.shuffle(seed=train_cfg['seed']).select(range(replay_size))\n",
        "    combined = pd.concat([\n",
        "        openstack_train.to_pandas(),\n",
        "        replay_subset.to_pandas()\n",
        "    ], ignore_index=True)\n",
        "    combined = combined.sample(frac=1.0, random_state=train_cfg['seed']).reset_index(drop=True)\n",
        "    train_dataset = Dataset.from_pandas(combined, preserve_index=False)\n",
        "    print(f'Replay enabled: mixed {replay_size} HDFS rows with {len(openstack_train)} OpenStack rows.')\n",
        "else:\n",
        "    train_dataset = openstack_train\n",
        "    print('Replay disabled.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Tokenizer and Base Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_dir = Path(train_cfg['artifacts']['tokenizer_dir'])\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, use_fast=True)\n",
        "\n",
        "base_dir = Path(train_cfg['base_checkpoint_dir'])\n",
        "if not base_dir.exists():\n",
        "    raise FileNotFoundError(f'Base checkpoint directory not found: {base_dir}')\n",
        "\n",
        "candidate_configs = sorted(base_dir.glob('**/config.json'))\n",
        "model_path = candidate_configs[-1].parent if candidate_configs else base_dir\n",
        "print(f'Loading base checkpoint from {model_path}')\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_path)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_path, config=config)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "peft_cfg = train_cfg['peft']\n",
        "if peft_cfg['lora_enabled']:\n",
        "    lora_config = LoraConfig(\n",
        "        r=peft_cfg['r'],\n",
        "        lora_alpha=peft_cfg['alpha'],\n",
        "        target_modules=peft_cfg['target_modules'],\n",
        "        lora_dropout=peft_cfg['dropout'],\n",
        "        bias=peft_cfg['bias']\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    print('LoRA adapters enabled.')\n",
        "else:\n",
        "    print('LoRA disabled; full fine-tuning will run.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=train_cfg['sequence']['mlm_probability']\n",
        ")\n",
        "\n",
        "def collate_train(examples):\n",
        "    for example in examples:\n",
        "        example.pop('anomaly_label', None)\n",
        "    return collator(examples)\n",
        "\n",
        "def collate_eval(examples):\n",
        "    labels = [example.get('anomaly_label', 0) for example in examples]\n",
        "    features = [{k: v for k, v in example.items() if k != 'anomaly_label'} for example in examples]\n",
        "    batch = collator(features)\n",
        "    batch['anomaly_label'] = torch.tensor(labels, dtype=torch.long)\n",
        "    return batch\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=train_cfg['training']['train_batch_size_per_device'], shuffle=True, collate_fn=collate_train)\n",
        "val_loader = DataLoader(openstack_val, batch_size=train_cfg['training']['eval_batch_size_per_device'], shuffle=False, collate_fn=collate_eval)\n",
        "test_loader = DataLoader(openstack_test, batch_size=train_cfg['training']['eval_batch_size_per_device'], shuffle=False, collate_fn=collate_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Accelerator and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mixed_precision = 'no' if IS_MPS else train_cfg['precision']['mixed_precision']\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=train_cfg['training']['grad_accumulation_steps'],\n",
        "    mixed_precision=mixed_precision\n",
        ")\n",
        "print(accelerator.state)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=train_cfg['optimizer']['lr'],\n",
        "    betas=tuple(train_cfg['optimizer']['betas']),\n",
        "    eps=train_cfg['optimizer']['eps'],\n",
        "    weight_decay=train_cfg['optimizer']['weight_decay']\n",
        ")\n",
        "\n",
        "model, optimizer, train_loader, val_loader, test_loader = accelerator.prepare(\n",
        "    model, optimizer, train_loader, val_loader, test_loader\n",
        ")\n",
        "\n",
        "total_steps = math.ceil(len(train_loader) / train_cfg['training']['grad_accumulation_steps']) * train_cfg['training']['epochs']\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=train_cfg['optimizer']['scheduler'],\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=train_cfg['optimizer']['warmup_steps'],\n",
        "    num_training_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Memory Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def log_gpu_memory(tag: str):\n",
        "    if torch.cuda.is_available():\n",
        "        alloc = torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
        "        accelerator.print(f'[{tag}] gpu allocated={alloc:.2f} GB reserved={reserved:.2f} GB')\n",
        "    elif IS_MPS:\n",
        "        try:\n",
        "            import torch.mps\n",
        "            stats = torch.mps.current_allocated_memory() / (1024 ** 3)\n",
        "            accelerator.print(f'[{tag}] mps allocated={stats:.2f} GB')\n",
        "        except Exception:\n",
        "            accelerator.print(f'[{tag}] mps memory stats unavailable.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training with Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_cfg = train_cfg['checkpointing']\n",
        "metrics_dir = Path(train_cfg['artifacts']['metrics_dir'])\n",
        "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
        "eval_dir = Path(train_cfg['artifacts']['eval_dir'])\n",
        "eval_dir.mkdir(parents=True, exist_ok=True)\n",
        "run_config_path = Path(train_cfg['artifacts']['run_config_path'])\n",
        "run_config_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
        "\n",
        "best_val = float('inf')\n",
        "best_checkpoint_path = None\n",
        "patience = train_cfg['training']['patience']\n",
        "min_delta = train_cfg['training']['min_delta']\n",
        "wait = 0\n",
        "\n",
        "epochs_total = train_cfg['training']['epochs']\n",
        "save_steps = checkpoint_cfg['save_steps']\n",
        "max_grad_norm = train_cfg['training']['max_grad_norm']\n",
        "log_steps = train_cfg['logging']['log_steps'] if 'logging' in train_cfg else None\n",
        "\n",
        "for epoch in range(epochs_total):\n",
        "    model.train()\n",
        "    accelerator.print(f'==== Epoch {epoch+1}/{epochs_total} ====')\n",
        "    progress = tqdm(total=len(train_loader), disable=not accelerator.is_local_main_process)\n",
        "    step_losses = []\n",
        "    for step, batch in enumerate(train_loader, start=1):\n",
        "        with accelerator.accumulate(model):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            accelerator.backward(loss)\n",
        "            if max_grad_norm:\n",
        "                accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        step_losses.append(loss.detach().item())\n",
        "        global_step = epoch * len(train_loader) + step\n",
        "        progress.set_description(f'loss={loss.item():.4f}')\n",
        "        progress.update(1)\n",
        "\n",
        "        if log_steps and global_step % log_steps == 0 and accelerator.is_main_process:\n",
        "            log_gpu_memory(f'step {global_step}')\n",
        "\n",
        "        if save_steps and global_step % save_steps == 0:\n",
        "            ckpt_dir = Path(checkpoint_cfg['output_dir']) / f'step_epoch{epoch+1}_step{global_step}'\n",
        "            if accelerator.is_main_process:\n",
        "                ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "                accelerator.unwrap_model(model).save_pretrained(ckpt_dir)\n",
        "                tokenizer.save_pretrained(ckpt_dir / 'tokenizer')\n",
        "            accelerator.wait_for_everyone()\n",
        "            free_cuda()\n",
        "\n",
        "    progress.close()\n",
        "    train_loss = float(np.mean(step_losses))\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    for batch in val_loader:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            val_losses.append(accelerator.gather(outputs.loss.detach()).mean().item())\n",
        "    val_loss = float(np.mean(val_losses))\n",
        "\n",
        "    history['epoch'].append(epoch+1)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    accelerator.print(f'Epoch {epoch+1}: train_loss={train_loss:.4f} val_loss={val_loss:.4f}')\n",
        "\n",
        "    if val_loss + min_delta < best_val:\n",
        "        best_val = val_loss\n",
        "        wait = 0\n",
        "        best_checkpoint_path = Path(checkpoint_cfg['output_dir']) / 'best'\n",
        "        if accelerator.is_main_process:\n",
        "            best_checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
        "            accelerator.unwrap_model(model).save_pretrained(best_checkpoint_path)\n",
        "            tokenizer.save_pretrained(best_checkpoint_path / 'tokenizer')\n",
        "        accelerator.wait_for_everyone()\n",
        "        free_cuda()\n",
        "        accelerator.print('Best checkpoint updated.')\n",
        "    else:\n",
        "        wait += 1\n",
        "        accelerator.print(f'No improvement, patience {wait}/{patience}')\n",
        "        if wait >= patience:\n",
        "            accelerator.print('Early stopping triggered.')\n",
        "            break\n",
        "\n",
        "free_cuda()\n",
        "\n",
        "if best_checkpoint_path and best_checkpoint_path.exists():\n",
        "    accelerator.print(f'Loading best checkpoint from {best_checkpoint_path}')\n",
        "    best_model = AutoModelForMaskedLM.from_pretrained(best_checkpoint_path, config=config)\n",
        "    accelerator.unwrap_model(model).load_state_dict(best_model.state_dict())\n",
        "    del best_model\n",
        "    free_cuda()\n",
        "    accelerator.wait_for_everyone()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_scores(dataloader) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    scores, labels = [], []\n",
        "    for batch in dataloader:\n",
        "        outputs = model(**batch)\n",
        "        loss = accelerator.gather(outputs.loss)\n",
        "        scores.extend(loss.cpu().numpy())\n",
        "        label_tensor = accelerator.gather(batch['anomaly_label'])\n",
        "        labels.extend(label_tensor.cpu().numpy())\n",
        "    return np.asarray(scores), np.asarray(labels)\n",
        "\n",
        "val_scores, val_labels = collect_scores(val_loader)\n",
        "test_scores, test_labels = collect_scores(test_loader)\n",
        "\n",
        "threshold_candidates = np.percentile(val_scores, np.linspace(50, 99, 25))\n",
        "best_threshold, best_f1 = threshold_candidates[-1], 0.0\n",
        "for candidate in threshold_candidates:\n",
        "    preds = (val_scores >= candidate).astype(int)\n",
        "    f1 = f1_score(val_labels, preds, zero_division=0)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_threshold = candidate\n",
        "\n",
        "val_probs = np.exp(-val_scores)\n",
        "test_probs = np.exp(-test_scores)\n",
        "\n",
        "pred_test = (test_scores >= best_threshold).astype(int)\n",
        "f1 = f1_score(test_labels, pred_test, zero_division=0)\n",
        "roc_auc = roc_auc_score(test_labels, test_probs)\n",
        "pr_auc = average_precision_score(test_labels, test_probs)\n",
        "\n",
        "metrics = {\n",
        "    'val_threshold_loss': float(best_threshold),\n",
        "    'val_best_f1': float(best_f1),\n",
        "    'test_f1': float(f1),\n",
        "    'test_roc_auc': float(roc_auc),\n",
        "    'test_pr_auc': float(pr_auc)\n",
        "}\n",
        "metrics_path = metrics_dir / 'openstack_metrics.json'\n",
        "metrics_path.write_text(json.dumps(metrics, indent=2))\n",
        "print(json.dumps(metrics, indent=2))\n",
        "\n",
        "cm = confusion_matrix(test_labels, pred_test)\n",
        "fig, ax = plt.subplots(figsize=(4, 3))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "plt.tight_layout()\n",
        "cm_path = eval_dir / 'confusion_matrix.png'\n",
        "fig.savefig(cm_path)\n",
        "plt.close(fig)\n",
        "print(f'Confusion matrix saved to {cm_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Export TorchScript and ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "export_dir = Path(train_cfg['export']['output_dir'])\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "model_to_export = accelerator.unwrap_model(model).cpu()\n",
        "model_to_export.eval()\n",
        "\n",
        "seq_len = train_cfg['sequence']['max_length']\n",
        "dummy_ids = torch.ones((1, seq_len), dtype=torch.long)\n",
        "dummy_mask = torch.ones((1, seq_len), dtype=torch.long)\n",
        "\n",
        "with torch.no_grad():\n",
        "    traced = torch.jit.trace(model_to_export, (dummy_ids, dummy_mask))\n",
        "    ts_path = export_dir / train_cfg['export']['torchscript_filename']\n",
        "    traced.save(str(ts_path))\n",
        "    print(f'TorchScript saved to {ts_path}')\n",
        "\n",
        "onnx_path = export_dir / train_cfg['export']['onnx_filename']\n",
        "torch.onnx.export(\n",
        "    model_to_export,\n",
        "    (dummy_ids, dummy_mask),\n",
        "    str(onnx_path),\n",
        "    input_names=['input_ids', 'attention_mask'],\n",
        "    output_names=['logits'],\n",
        "    dynamic_axes={'input_ids': {0: 'batch'}, 'attention_mask': {0: 'batch'}, 'logits': {0: 'batch'}},\n",
        "    opset_version=train_cfg['export']['opset']\n",
        ")\n",
        "print(f'ONNX saved to {onnx_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Card Snippet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_card_path = Path(train_cfg['checkpointing']['output_dir']) / 'MODEL_CARD.md'\n",
        "model_card = (\n",
        "    f\"# LogBERT OpenStack Fine-tune\n",
        "\n",
        "\"\n",
        "    f\"- Base checkpoint: {train_cfg['base_checkpoint_dir']}\n",
        "\"\n",
        "    f\"- Sequence length: {train_cfg['sequence']['max_length']}\n",
        "\"\n",
        "    f\"- Replay enabled: {train_cfg['replay']['enabled']}\n",
        "\"\n",
        "    f\"- LoRA enabled: {train_cfg['peft']['lora_enabled']}\n",
        "\n",
        "\"\n",
        "    f\"## Eval Metrics\n",
        "\"\n",
        "    f\"- F1: {metrics['test_f1']:.4f}\n",
        "\"\n",
        "    f\"- ROC-AUC: {metrics['test_roc_auc']:.4f}\n",
        "\"\n",
        "    f\"- PR-AUC: {metrics['test_pr_auc']:.4f}\n",
        "\"\n",
        "    f\"- Threshold (loss): {metrics['val_threshold_loss']:.6f}\n",
        "\n",
        "\"\n",
        "    f\"## Artifacts\n",
        "\"\n",
        "    f\"- TorchScript: {train_cfg['export']['torchscript_filename']}\n",
        "\"\n",
        "    f\"- ONNX: {train_cfg['export']['onnx_filename']}\n",
        "\"\n",
        ")\n",
        "model_card_path.write_text(model_card)\n",
        "print(f'Model card snippet written to {model_card_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Persist Run Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_summary = {\n",
        "    'num_processes': accelerator.state.num_processes,\n",
        "    'device': str(accelerator.device),\n",
        "    'mixed_precision': accelerator.state.mixed_precision\n",
        "}\n",
        "run_payload = {\n",
        "    'train_openstack': train_cfg,\n",
        "    'data_config': data_cfg,\n",
        "    'accelerator_state': state_summary,\n",
        "    'metrics': metrics,\n",
        "    'is_mps': IS_MPS\n",
        "}\n",
        "run_config_path.write_text(json.dumps(run_payload, indent=2))\n",
        "print(f'Run config stored at {run_config_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Artifacts Produced\n",
        "- Fine-tuned checkpoints -> `artifacts/logbert-mlm-os/`\n",
        "- Metrics JSON and confusion matrix -> `artifacts/metrics/openstack/openstack_metrics.json`, `artifacts/eval/confusion_matrix.png`\n",
        "- Exported models -> `artifacts/exported_models/`\n",
        "- Model card snippet -> `artifacts/logbert-mlm-os/MODEL_CARD.md`\n",
        "\n",
        "Pipeline complete."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}