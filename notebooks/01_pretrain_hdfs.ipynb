{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f133fdd1",
      "metadata": {},
      "source": [
        "# 01 \u00b7 Pretrain DistilBERT on HDFS (MLM)\n",
        "\n",
        "Masked language modeling pretraining with Hugging Face Accelerate on two RTX 6000 GPUs (or Apple MPS fallback). This notebook consumes the artifacts from `00_prepare_data.ipynb`, configures multi-device training, and saves checkpoints plus diagnostics for fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4021df59",
      "metadata": {},
      "source": [
        "## Notebook Goals\n",
        "- Load training hyperparameters from `configs/train_hdfs.yaml` and dataset metadata.\n",
        "- Auto-generate `accelerate_config.yaml` for multi-GPU Linux or skip when running on Apple Silicon MPS.\n",
        "- Build PyTorch DataLoaders backed by tokenized Parquet splits with dynamic masking.\n",
        "- Run a custom Accelerate training loop with throughput EMA, p95 step latency, VRAM logging, and gradient norms.\n",
        "- Save checkpoints every N steps and at epoch end while calling `free_cuda()` for memory hygiene.\n",
        "- Persist run metadata (`run_config.json`) and validation metrics for downstream fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a037178",
      "metadata": {},
      "source": [
        "## 1. Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023a0d81",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "from collections import deque\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForMaskedLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    get_scheduler\n",
        ")\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f241a0e",
      "metadata": {},
      "source": [
        "### Load YAML configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8286fefc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_yaml(path: Path) -> Dict:\n",
        "    with path.open('r') as fh:\n",
        "        return yaml.safe_load(fh)\n",
        "\n",
        "data_cfg = load_yaml(Path('configs/data.yaml'))\n",
        "train_cfg = load_yaml(Path('configs/train_hdfs.yaml'))\n",
        "accelerate_config_path = Path(train_cfg['accelerate']['config_path'])\n",
        "print(json.dumps({\n",
        "    'data_cfg': str(Path('configs/data.yaml')),\n",
        "    'train_cfg': str(Path('configs/train_hdfs.yaml')),\n",
        "    'accelerate_cfg': str(accelerate_config_path)\n",
        "}, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4318b9bd",
      "metadata": {},
      "source": [
        "### Device detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c8e651b",
      "metadata": {},
      "outputs": [],
      "source": [
        "IS_MPS = torch.backends.mps.is_available()\n",
        "if IS_MPS:\n",
        "    os.environ.setdefault('ACCELERATE_USE_MPS_DEVICE', '1')\n",
        "    print('Apple Silicon (MPS) detected. Accelerate will target the MPS backend; multi-GPU config will be skipped.')\n",
        "else:\n",
        "    print('MPS not available; defaulting to CUDA/CPU configuration.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ec748e",
      "metadata": {},
      "source": [
        "### Auto-generate Accelerate configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a5c2d2",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [],
      "source": [
        "def build_accelerate_config(cfg: Dict, is_mps: bool) -> Dict:\n",
        "    if is_mps:\n",
        "        return {\n",
        "            'compute_environment': 'LOCAL_MACHINE',\n",
        "            'distributed_type': 'MPS',\n",
        "            'mixed_precision': 'no',\n",
        "            'num_processes': 1,\n",
        "            'gradient_accumulation_steps': cfg['gradient_accumulation_steps'],\n",
        "            'main_process_ip': '127.0.0.1',\n",
        "            'main_process_port': 29500,\n",
        "            'gpu_ids': '0'\n",
        "        }\n",
        "    return {\n",
        "        'compute_environment': 'LOCAL_MACHINE',\n",
        "        'distributed_type': 'MULTI_GPU',\n",
        "        'mixed_precision': cfg['mixed_precision'],\n",
        "        'num_processes': cfg['num_processes'],\n",
        "        'machine_rank': 0,\n",
        "        'main_process_ip': '127.0.0.1',\n",
        "        'main_process_port': 29500,\n",
        "        'deepspeed_config': cfg['zero_stage2_toggle'],\n",
        "        'dynamo_backend': 'NO',\n",
        "        'gradient_accumulation_steps': cfg['gradient_accumulation_steps']\n",
        "    }\n",
        "\n",
        "if IS_MPS:\n",
        "    if accelerate_config_path.exists():\n",
        "        print('MPS mode: skipping accelerate_config.yaml regeneration (existing file will be ignored).')\n",
        "else:\n",
        "    payload = build_accelerate_config(train_cfg['accelerate'], IS_MPS)\n",
        "    accelerate_config_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with accelerate_config_path.open('w') as fh:\n",
        "        yaml.safe_dump(payload, fh, sort_keys=False)\n",
        "    print(f'Accelerate config written to {accelerate_config_path}')\n",
        "    with accelerate_config_path.open('r') as fh:\n",
        "        print(fh.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004fa634",
      "metadata": {},
      "source": [
        "## 2. Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd26519f",
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata_path = Path(data_cfg['preprocessing']['dataset_metadata'])\n",
        "if not metadata_path.exists():\n",
        "    raise FileNotFoundError(f'Metadata not found: {metadata_path}. Run 00_prepare_data.ipynb first.')\n",
        "metadata = json.loads(metadata_path.read_text())\n",
        "print(json.dumps(metadata, indent=2))\n",
        "\n",
        "parquet_dir = Path(data_cfg['preprocessing']['parquet_dir'])\n",
        "train_dataset = load_from_disk(str(parquet_dir / 'hdfs_train_hf'))\n",
        "val_dataset = load_from_disk(str(parquet_dir / 'hdfs_val_hf'))\n",
        "test_dataset = load_from_disk(str(parquet_dir / 'hdfs_test_hf'))\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c207c07",
      "metadata": {},
      "source": [
        "## 3. Tokenizer and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0505ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_dir = Path(train_cfg['artifacts']['tokenizer_dir'])\n",
        "if not tokenizer_dir.exists():\n",
        "    raise FileNotFoundError(f'Tokenizer directory missing: {tokenizer_dir}')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir, use_fast=True)\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': data_cfg['tokens']['special']})\n",
        "\n",
        "model_name = train_cfg['model']['name']\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "config.vocab_size = len(tokenizer)\n",
        "config.gradient_checkpointing = train_cfg['model']['gradient_checkpointing']\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name, config=config)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "print(f'Model {model_name} initialized with vocab size {len(tokenizer)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10cdb96b",
      "metadata": {},
      "source": [
        "## 4. DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e91e363",
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_cfg = train_cfg['sequence']\n",
        "collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=seq_cfg['mlm_probability']\n",
        ")\n",
        "\n",
        "def collate_without_anomaly(examples):\n",
        "    for example in examples:\n",
        "        example.pop('anomaly_label', None)\n",
        "    return collator(examples)\n",
        "\n",
        "per_device_train_bs = train_cfg['training']['train_batch_size_per_device']\n",
        "per_device_eval_bs = train_cfg['training']['eval_batch_size_per_device']\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=per_device_train_bs, shuffle=True, collate_fn=collate_without_anomaly)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=per_device_eval_bs, shuffle=False, collate_fn=collate_without_anomaly)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=per_device_eval_bs, shuffle=False, collate_fn=collate_without_anomaly)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4db41f0c",
      "metadata": {},
      "source": [
        "## 5. Accelerator Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c843a889",
      "metadata": {},
      "outputs": [],
      "source": [
        "mixed_precision = 'no' if IS_MPS else train_cfg['precision']['mixed_precision']\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=train_cfg['training']['grad_accumulation_steps'],\n",
        "    mixed_precision=mixed_precision\n",
        ")\n",
        "print(accelerator.state)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=train_cfg['optimizer']['lr'],\n",
        "    betas=tuple(train_cfg['optimizer']['betas']),\n",
        "    eps=train_cfg['optimizer']['eps'],\n",
        "    weight_decay=train_cfg['optimizer']['weight_decay']\n",
        ")\n",
        "\n",
        "model, optimizer, train_dataloader, val_dataloader, test_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, val_dataloader, test_dataloader\n",
        ")\n",
        "\n",
        "num_update_steps = math.ceil(len(train_dataloader) / train_cfg['training']['grad_accumulation_steps'])\n",
        "total_steps = num_update_steps * train_cfg['training']['epochs']\n",
        "print(f'Total training steps: {total_steps}')\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=train_cfg['optimizer']['scheduler'],\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=train_cfg['optimizer']['warmup_steps'],\n",
        "    num_training_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "555ac90b",
      "metadata": {},
      "source": [
        "## 6. Memory Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d129c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def free_cuda():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "def log_gpu_memory(tag: str):\n",
        "    if torch.cuda.is_available():\n",
        "        alloc = torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
        "        accelerator.print(f'[{tag}] gpu allocated={alloc:.2f} GB reserved={reserved:.2f} GB')\n",
        "    elif IS_MPS:\n",
        "        try:\n",
        "            import torch.mps\n",
        "            stats = torch.mps.current_allocated_memory() / (1024 ** 3)\n",
        "            accelerator.print(f'[{tag}] mps allocated={stats:.2f} GB')\n",
        "        except Exception:\n",
        "            accelerator.print(f'[{tag}] mps memory stats unavailable.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64ea574",
      "metadata": {},
      "source": [
        "## 7. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ce408a",
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_cfg = train_cfg['checkpointing']\n",
        "metrics_dir = Path(train_cfg['artifacts']['metrics_dir'])\n",
        "metrics_dir.mkdir(parents=True, exist_ok=True)\n",
        "run_config_path = Path(train_cfg['artifacts']['run_config_path'])\n",
        "run_config_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "history = {'step': [], 'loss': [], 'lr': [], 'throughput': []}\n",
        "step_times = deque(maxlen=200)\n",
        "ema_throughput = None\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch: int, step: int, tag: str):\n",
        "    accelerator.wait_for_everyone()\n",
        "    ckpt_dir = Path(checkpoint_cfg['output_dir']) / f'{tag}_epoch{epoch}_step{step}'\n",
        "    if accelerator.is_main_process:\n",
        "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "        unwrapped = accelerator.unwrap_model(model)\n",
        "        unwrapped.save_pretrained(ckpt_dir)\n",
        "        tokenizer.save_pretrained(ckpt_dir / 'tokenizer')\n",
        "        torch.save(optimizer.state_dict(), ckpt_dir / 'optimizer.pt')\n",
        "        torch.save(scheduler.state_dict(), ckpt_dir / 'scheduler.pt')\n",
        "        with (ckpt_dir / 'training_state.json').open('w') as fh:\n",
        "            json.dump({'epoch': epoch, 'step': step}, fh)\n",
        "        accelerator.print(f'[checkpoint] saved -> {ckpt_dir}')\n",
        "    accelerator.wait_for_everyone()\n",
        "    free_cuda()\n",
        "\n",
        "train_epochs = train_cfg['training']['epochs']\n",
        "save_steps = checkpoint_cfg['save_steps']\n",
        "max_grad_norm = train_cfg['training']['max_grad_norm']\n",
        "\n",
        "for epoch in range(train_epochs):\n",
        "    model.train()\n",
        "    accelerator.print(f'==== Epoch {epoch+1}/{train_epochs} ====')\n",
        "    progress = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "    for step, batch in enumerate(train_dataloader, start=1):\n",
        "        start_time = time.perf_counter()\n",
        "        with accelerator.accumulate(model):\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            accelerator.backward(loss)\n",
        "            if max_grad_norm:\n",
        "                accelerator.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        duration = time.perf_counter() - start_time\n",
        "        step_times.append(duration)\n",
        "\n",
        "        global_step = epoch * len(train_dataloader) + step\n",
        "        tokens_processed = batch['input_ids'].numel()\n",
        "        throughput = tokens_processed / max(duration, 1e-6)\n",
        "        beta = train_cfg['logging']['throughput_ema_beta']\n",
        "        ema_throughput = throughput if ema_throughput is None else (beta * ema_throughput + (1 - beta) * throughput)\n",
        "\n",
        "        history['step'].append(int(global_step))\n",
        "        history['loss'].append(float(loss.detach().item()))\n",
        "        history['lr'].append(float(lr_scheduler.get_last_lr()[0]))\n",
        "        history['throughput'].append(float(ema_throughput))\n",
        "\n",
        "        if accelerator.is_local_main_process:\n",
        "            p95 = float(np.percentile(step_times, 95)) if len(step_times) >= 5 else float(duration)\n",
        "            progress.set_description(f'loss={loss.item():.4f} ema_tok_s={ema_throughput:,.0f} p95={p95:.3f}s')\n",
        "        progress.update(1)\n",
        "\n",
        "        if train_cfg['logging']['log_steps'] and global_step % train_cfg['logging']['log_steps'] == 0 and accelerator.is_main_process:\n",
        "            log_gpu_memory(f'step {global_step}')\n",
        "\n",
        "        if save_steps and global_step % save_steps == 0:\n",
        "            save_checkpoint(model, optimizer, lr_scheduler, epoch+1, global_step, tag='step')\n",
        "\n",
        "    progress.close()\n",
        "    epoch_steps = (epoch + 1) * len(train_dataloader)\n",
        "    save_checkpoint(model, optimizer, lr_scheduler, epoch+1, epoch_steps, tag='epoch')\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for batch in val_dataloader:\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "            losses.append(accelerator.gather(outputs.loss.detach()).mean().item())\n",
        "    val_loss = float(np.mean(losses))\n",
        "    accelerator.print(f'Validation loss after epoch {epoch+1}: {val_loss:.4f}')\n",
        "\n",
        "accelerator.wait_for_everyone()\n",
        "free_cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e0feed",
      "metadata": {},
      "source": [
        "## 8. Evaluation and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59aaaf5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_loader(dataloader) -> float:\n",
        "    losses = []\n",
        "    for batch in dataloader:\n",
        "        outputs = model(**batch)\n",
        "        gathered = accelerator.gather(outputs.loss)\n",
        "        losses.extend(gathered.cpu().numpy())\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "val_loss = evaluate_loader(val_dataloader)\n",
        "test_loss = evaluate_loader(test_dataloader)\n",
        "perplexity = math.exp(test_loss)\n",
        "\n",
        "metrics = {\n",
        "    'val_loss': val_loss,\n",
        "    'test_loss': test_loss,\n",
        "    'test_perplexity': perplexity,\n",
        "    'steps_tracked': len(history['step'])\n",
        "}\n",
        "metrics_path = metrics_dir / 'hdfs_pretraining_metrics.json'\n",
        "metrics_path.write_text(json.dumps(metrics, indent=2))\n",
        "print(json.dumps(metrics, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a21757",
      "metadata": {},
      "source": [
        "## 9. Persist Run Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1935b86",
      "metadata": {},
      "outputs": [],
      "source": [
        "state_summary = {\n",
        "    'num_processes': accelerator.state.num_processes,\n",
        "    'process_index': accelerator.state.process_index,\n",
        "    'local_process_index': accelerator.state.local_process_index,\n",
        "    'device': str(accelerator.device),\n",
        "    'mixed_precision': accelerator.state.mixed_precision,\n",
        "    'distributed_type': str(accelerator.state.distributed_type)\n",
        "}\n",
        "run_payload = {\n",
        "    'train_config': train_cfg,\n",
        "    'data_config': data_cfg,\n",
        "    'accelerator_state': state_summary,\n",
        "    'is_mps': IS_MPS\n",
        "}\n",
        "run_config_path.write_text(json.dumps(run_payload, indent=2))\n",
        "print(f'Run configuration written to {run_config_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Artifacts Produced\n",
        "- Checkpoints saved under `artifacts/logbert-mlm-hdfs/`\n",
        "- Validation/test metrics stored at `artifacts/metrics/hdfs/hdfs_pretraining_metrics.json`\n",
        "- Run configuration captured at `artifacts/logbert-mlm-hdfs/run_config.json`\n",
        "\n",
        "Continuing pipeline: open `02_finetune_openstack.ipynb`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}