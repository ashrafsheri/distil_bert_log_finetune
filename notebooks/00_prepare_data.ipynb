{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8007e7c1",
      "metadata": {},
      "source": [
        "# 00 Â· Prepare Data for LogBERT Pipeline\n",
        "\n",
        "This notebook downloads public log datasets, applies regex normalization, mines templates with Drain3, and materializes tokenized Parquet splits ready for training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af603d7",
      "metadata": {},
      "source": [
        "## Notebook Goals\n",
        "- Install pinned dependencies for the workstation environment.\n",
        "- Fetch HDFS and OpenStack log corpora with checksum verification and automatic mirror fallback.\n",
        "- Apply regex-based normalization rules from `configs/data.yaml` and preview before/after examples.\n",
        "- Mine log templates in streaming mode with Drain3 and persist template transitions as Parquet.\n",
        "- Build Hugging Face datasets with BERT-compatible tokenization, time-based splits (80/10/10), and truncation stats.\n",
        "- Save artifacts (tokenizer, processed Parquet, metadata) for downstream notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d0a20f",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2e3104",
      "metadata": {
        "tags": [
          "helper",
          "collapsible"
        ]
      },
      "outputs": [],
      "source": [
        "import os, sys, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "if os.environ.get('SKIP_REQUIREMENTS', '0') == '1':\n",
        "    print('SKIP_REQUIREMENTS=1 -> skipping pip install from requirements.txt')\n",
        "else:\n",
        "    req_path = Path('requirements.txt')\n",
        "    if req_path.exists():\n",
        "        print('[setup] Installing dependencies from requirements.txt ...')\n",
        "        completed = subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', str(req_path)])\n",
        "        if completed.returncode != 0:\n",
        "            raise RuntimeError('pip installation failed; inspect output above.')\n",
        "    else:\n",
        "        print('requirements.txt not found; skipping installation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "508748f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3cfb25df03546e68b01851ffff39693",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "import math\n",
        "import tarfile\n",
        "import hashlib\n",
        "import shutil\n",
        "import textwrap\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from drain3 import TemplateMiner\n",
        "from drain3.template_miner_config import TemplateMinerConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import yaml\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f8a5821",
      "metadata": {},
      "source": [
        "### Load Configuration and Prepare Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7d8df71b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00_prepare_data.ipynb       02_finetune_openstack.ipynb\n",
            "01_pretrain_hdfs.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "5c525a35",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[setup] Configuration loaded and folders prepared.\n"
          ]
        }
      ],
      "source": [
        "def load_yaml(path: Path) -> Dict:\n",
        "    with path.open('r') as fh:\n",
        "        return yaml.safe_load(fh)\n",
        "\n",
        "data_config = load_yaml(Path('../configs/data.yaml'))\n",
        "RAW_HDFS_DIR = Path(data_config['raw_paths']['hdfs'])\n",
        "RAW_OPENSTACK_DIR = Path(data_config['raw_paths']['openstack'])\n",
        "ARTIFACTS_DIR = Path(data_config['artifacts_dir'])\n",
        "TOKENIZER_DIR = ARTIFACTS_DIR / 'tokenizer'\n",
        "PARQUET_DIR = Path(data_config['preprocessing']['parquet_dir'])\n",
        "METADATA_PATH = Path(data_config['preprocessing']['dataset_metadata'])\n",
        "for folder in [RAW_HDFS_DIR, RAW_OPENSTACK_DIR, ARTIFACTS_DIR, TOKENIZER_DIR, PARQUET_DIR, METADATA_PATH.parent]:\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "print('[setup] Configuration loaded and folders prepared.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c516d6",
      "metadata": {},
      "source": [
        "## 2. Download Public Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c9f84d4a",
      "metadata": {
        "tags": [
          "helper",
          "download"
        ]
      },
      "outputs": [],
      "source": [
        "def stream_download(url: str, destination: Path) -> bool:\n",
        "    chunk = 1 << 20\n",
        "    try:\n",
        "        with requests.get(url, stream=True, timeout=60) as resp:\n",
        "            resp.raise_for_status()\n",
        "            total = int(resp.headers.get('content-length', 0))\n",
        "            with destination.open('wb') as fh, tqdm(total=total, unit='B', unit_scale=True, desc=f'download {destination.name}') as bar:\n",
        "                for part in resp.iter_content(chunk_size=chunk):\n",
        "                    if part:\n",
        "                        fh.write(part)\n",
        "                        bar.update(len(part))\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        print(f'[warn] download failed from {url}: {exc}')\n",
        "        return False\n",
        "\n",
        "def sha256sum(path: Path) -> str:\n",
        "    hash_obj = hashlib.sha256()\n",
        "    with path.open('rb') as fh:\n",
        "        for chunk in iter(lambda: fh.read(1 << 20), b''):\n",
        "            hash_obj.update(chunk)\n",
        "    return hash_obj.hexdigest()\n",
        "\n",
        "def ensure_download(urls: List[str], target: Path, expected_sha: Optional[str] = None) -> Path:\n",
        "    for url in urls:\n",
        "        if stream_download(url, target):\n",
        "            break\n",
        "    else:\n",
        "        raise RuntimeError(f'all download mirrors failed for {target.name}')\n",
        "    checksum = sha256sum(target)\n",
        "    if expected_sha and checksum != expected_sha:\n",
        "        raise ValueError(f'sha mismatch for {target.name}: expected {expected_sha}, got {checksum}')\n",
        "    return target\n",
        "\n",
        "def maybe_update_sha(config_section: Dict, key: str, computed_sha: str) -> None:\n",
        "    current = config_section.get(key)\n",
        "    if isinstance(current, str) and current:\n",
        "        return\n",
        "    config_section[key] = computed_sha\n",
        "    with Path('../configs/data.yaml').open('w') as fh:\n",
        "        yaml.safe_dump(data_config, fh, sort_keys=False)\n",
        "    print(f'[sha] recorded SHA256 for {key}: {computed_sha}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3fb2cd23",
      "metadata": {
        "tags": [
          "download"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[download] HDFS corpus\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87f03cfbd6cb44aba96014ad83b10cc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "download HDFS_1.tar.gz:   0%|          | 0.00/162M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[sha] recorded SHA256 for sha256: 6ca6c5bc2671c66afecee9369a2fdac606bf33997a2494ac66aa411fe3e95169\n",
            "[extract] HDFS log already present\n",
            "[ready] HDFS log at data/hdfs/raw/HDFS.log\n"
          ]
        }
      ],
      "source": [
        "hdfs_cfg = data_config['datasets']['hdfs']\n",
        "archive_path = RAW_HDFS_DIR / hdfs_cfg['archive_name']\n",
        "archive_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "print('[download] HDFS corpus')\n",
        "ensure_download(hdfs_cfg['urls'], archive_path, hdfs_cfg.get('sha256') or None)\n",
        "computed_sha = sha256sum(archive_path)\n",
        "maybe_update_sha(hdfs_cfg, 'sha256', computed_sha)\n",
        "\n",
        "if not (RAW_HDFS_DIR / hdfs_cfg['log_file']).exists():\n",
        "    print('[extract] HDFS archive')\n",
        "    with tarfile.open(archive_path, 'r:gz') as tf:\n",
        "        tf.extractall(RAW_HDFS_DIR)\n",
        "else:\n",
        "    print('[extract] HDFS log already present')\n",
        "\n",
        "hdfs_log_path = RAW_HDFS_DIR / hdfs_cfg['log_file']\n",
        "print(f'[ready] HDFS log at {hdfs_log_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d54902ca",
      "metadata": {
        "tags": [
          "download"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[download] OpenStack normal logs\n",
            "[cache] Detected existing openstack_normal.log, skipping download.\n",
            "[sha] recorded SHA256 for normal: 81bc1a64d6788efe1a6f30b9a9958b14eb53c1bfe6a2d166bf23c821d8b77b71\n",
            "[download] OpenStack abnormal logs\n",
            "[cache] Detected existing openstack_abnormal.log, skipping download.\n",
            "[ready] OpenStack logs -> {'normal': PosixPath('data/openstack/raw/openstack_normal.log'), 'abnormal': PosixPath('data/openstack/raw/openstack_abnormal.log')}\n"
          ]
        }
      ],
      "source": [
        "openstack_cfg = data_config['datasets']['openstack']\n",
        "RAW_OPENSTACK_DIR.mkdir(parents=True, exist_ok=True)\n",
        "paths: Dict[str, Path] = {}\n",
        "print('[download] OpenStack normal logs')\n",
        "normal_target = RAW_OPENSTACK_DIR / 'openstack_normal.log'\n",
        "normal_urls = openstack_cfg['normal_urls']\n",
        "normal_sha_cfg = (openstack_cfg.get('sha256') or {}).get('normal')\n",
        "normal_acquired = False\n",
        "if normal_target.exists():\n",
        "    print('[cache] Detected existing openstack_normal.log, skipping download.')\n",
        "    normal_acquired = True\n",
        "    normal_urls = []\n",
        "    maybe_update_sha(openstack_cfg['sha256'], 'normal', sha256sum(normal_target))\n",
        "if not normal_acquired:\n",
        "    for url in normal_urls:\n",
        "        try:\n",
        "            if url.endswith('.tar.gz'):\n",
        "                tmp_tar = RAW_OPENSTACK_DIR / 'openstack_bundle.tar.gz'\n",
        "                ensure_download([url], tmp_tar, None)\n",
        "                with tarfile.open(tmp_tar, 'r:gz') as tf:\n",
        "                    member = next((m for m in tf.getmembers() if m.name.endswith('openstack_normal.log')), None)\n",
        "                    if member is None:\n",
        "                        raise ValueError('openstack_normal.log not found inside archive')\n",
        "                    tf.extract(member, RAW_OPENSTACK_DIR)\n",
        "                    extracted_path = RAW_OPENSTACK_DIR / member.name\n",
        "                    final_path = RAW_OPENSTACK_DIR / 'openstack_normal.log'\n",
        "                    final_path.write_bytes(extracted_path.read_bytes())\n",
        "                    if extracted_path.exists():\n",
        "                        extracted_path.unlink()\n",
        "                    extracted_dir = extracted_path.parent\n",
        "                    if extracted_dir != RAW_OPENSTACK_DIR and extracted_dir.exists():\n",
        "                        shutil.rmtree(extracted_dir, ignore_errors=True)\n",
        "                tmp_tar.unlink(missing_ok=True)\n",
        "                normal_acquired = True\n",
        "            else:\n",
        "                ensure_download([url], normal_target, normal_sha_cfg)\n",
        "                normal_acquired = True\n",
        "            if normal_acquired:\n",
        "                maybe_update_sha(openstack_cfg['sha256'], 'normal', sha256sum(normal_target))\n",
        "                break\n",
        "        except Exception as exc:\n",
        "            print(f'[warn] fallback download failed from {url}: {exc}')\n",
        "if not normal_acquired:\n",
        "    raise RuntimeError('All download mirrors failed for OpenStack normal logs')\n",
        "paths['normal'] = normal_target\n",
        "\n",
        "print('[download] OpenStack abnormal logs')\n",
        "abnormal_target = RAW_OPENSTACK_DIR / 'openstack_abnormal.log'\n",
        "if abnormal_target.exists():\n",
        "    print('[cache] Detected existing openstack_abnormal.log, skipping download.')\n",
        "else:\n",
        "    ensure_download(openstack_cfg['abnormal_urls'], abnormal_target, (openstack_cfg.get('sha256') or {}).get('abnormal'))\n",
        "    maybe_update_sha(openstack_cfg['sha256'], 'abnormal', sha256sum(abnormal_target))\n",
        "paths['abnormal'] = abnormal_target\n",
        "print(f'[ready] OpenStack logs -> {paths}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c041c59b",
      "metadata": {},
      "source": [
        "## 3. Load and Inspect Raw Logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa97922",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x107fda990>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/ashrafshahreyar/miniconda3/envs/logAnom311/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ],
      "source": [
        "def read_hdfs_log(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path, header=None, names=['raw'], sep='', engine='python')\n",
        "    df['timestamp'] = pd.to_datetime(df['raw'].str.slice(0, 19), errors='coerce')\n",
        "    df['content'] = df['raw']\n",
        "    return df.dropna(subset=['timestamp']).reset_index(drop=True)\n",
        "\n",
        "def read_openstack_logs(paths: Dict[str, Path]) -> pd.DataFrame:\n",
        "    frames = []\n",
        "    for label, path in paths.items():\n",
        "        df = pd.read_csv(path, header=None, names=['raw'], sep='', engine='python')\n",
        "        df['timestamp'] = pd.to_datetime(df['raw'].str.extract(r'(\\d{4}-\\d{2}-\\d{2} [^ ]+)')[0], errors='coerce')\n",
        "        df['label'] = 1 if label == 'abnormal' else 0\n",
        "        df['content'] = df['raw']\n",
        "        frames.append(df.dropna(subset=['timestamp']))\n",
        "    return pd.concat(frames, ignore_index=True).sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "hdfs_df = read_hdfs_log(hdfs_log_path)\n",
        "openstack_df = read_openstack_logs(paths)\n",
        "print(f'[dataset] HDFS records: {len(hdfs_df):,} | OpenStack records: {len(openstack_df):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274fb81b",
      "metadata": {},
      "outputs": [],
      "source": [
        "display(hdfs_df[['timestamp','content']].head())\n",
        "display(openstack_df[['timestamp','label','content']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01525674",
      "metadata": {},
      "source": [
        "## 4. Regex Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f35b0a",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class LogNormalizer:\n",
        "    def __init__(self, rules: Iterable[Dict[str, str]]):\n",
        "        self.rules = [(rule['name'], re.compile(rule['pattern']), rule['replace']) for rule in rules]\n",
        "\n",
        "    def apply(self, text: str) -> str:\n",
        "        result = text\n",
        "        for _, pattern, repl in self.rules:\n",
        "            result = pattern.sub(repl, result)\n",
        "        return result\n",
        "\n",
        "    def normalize_series(self, series: pd.Series) -> pd.Series:\n",
        "        return series.astype(str).apply(self.apply)\n",
        "\n",
        "normalizer = LogNormalizer(data_config['normalizer']['rules'])\n",
        "for df in (hdfs_df, openstack_df):\n",
        "    df['normalized'] = normalizer.normalize_series(df['content'])\n",
        "\n",
        "preview = pd.DataFrame({'original': hdfs_df['content'].head(5), 'normalized': hdfs_df['normalized'].head(5)})\n",
        "display(preview)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6550cd7",
      "metadata": {},
      "source": [
        "## 5. Template Mining with Drain3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5168a698",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [],
      "source": [
        "drain_cfg = data_config['drain3']\n",
        "config_text = f\"[Drain]\n",
        "depth = {drain_cfg['depth']}\n",
        "st = {drain_cfg['st']}\n",
        "max_children = {drain_cfg['max_children']}\n",
        "extra_delimiters = {','.join(drain_cfg['extra_delimiters'])}\"\n",
        "template_config = TemplateMinerConfig()\n",
        "template_config.load_from_string(config_text)\n",
        "template_miner = TemplateMiner(template_config=template_config)\n",
        "\n",
        "records = []\n",
        "transition_counts = {}\n",
        "ngram = drain_cfg.get('template_transition_ngram', 3)\n",
        "print('[drain3] streaming normalized HDFS logs')\n",
        "cluster_trace: List[str] = []\n",
        "for row in tqdm(hdfs_df.to_dict('records'), desc='Drain3 HDFS'):\n",
        "    result = template_miner.add_log_message(row['normalized'])\n",
        "    cluster_id = result['cluster_id']\n",
        "    records.append({\n",
        "        'timestamp': row['timestamp'],\n",
        "        'template_id': cluster_id,\n",
        "        'template': result['template_mined'],\n",
        "        'content': row['normalized']\n",
        "    })\n",
        "    cluster_trace.append(cluster_id)\n",
        "\n",
        "for idx in range(len(cluster_trace) - ngram + 1):\n",
        "    key = tuple(cluster_trace[idx: idx + ngram])\n",
        "    transition_counts[key] = transition_counts.get(key, 0) + 1\n",
        "\n",
        "transition_output = Path(drain_cfg['transition_output'])\n",
        "transition_output.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "template_df = pd.DataFrame(records)\n",
        "index_path = transition_output.with_name('template_index.parquet')\n",
        "template_df.to_parquet(index_path, index=False)\n",
        "\n",
        "if transition_counts:\n",
        "    transition_df = pd.DataFrame([\n",
        "        {'ngram': '->'.join(key), 'count': count} for key, count in transition_counts.items()\n",
        "    ])\n",
        "    transition_df.to_parquet(transition_output, index=False)\n",
        "    display(transition_df.sort_values('count', ascending=False).head(10))\n",
        "else:\n",
        "    print('[drain3] no transitions recorded; dataset may be small')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b1d406",
      "metadata": {},
      "source": [
        "## 6. Tokenizer with Special Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d42a2f76",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer_cfg = data_config['tokenizer']\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_cfg['base_model'], use_fast=True)\n",
        "special_tokens = data_config['tokens']['special']\n",
        "added = tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
        "TOKENIZER_DIR.mkdir(parents=True, exist_ok=True)\n",
        "tokenizer.save_pretrained(TOKENIZER_DIR)\n",
        "print(f'[tokenizer] added {added} special tokens and saved to {TOKENIZER_DIR}')\n",
        "print('sample tokens:', tokenizer.tokenize(hdfs_df['normalized'].iloc[0])[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "958e3eb9",
      "metadata": {},
      "source": [
        "## 7. Build Tokenized Parquet Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08065c9d",
      "metadata": {
        "tags": [
          "helper"
        ]
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def tokenize_dataframe(df: pd.DataFrame, max_length: int) -> Tuple[pd.DataFrame, float]:\n",
        "    encodings = tokenizer(\n",
        "        list(df['normalized']),\n",
        "        padding=False,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "    truncated = sum(len(ids) == max_length for ids in encodings['input_ids'])\n",
        "    tokens = pd.DataFrame({\n",
        "        'input_ids': encodings['input_ids'],\n",
        "        'attention_mask': encodings['attention_mask'],\n",
        "        'labels': encodings['input_ids'],\n",
        "        'template_id': df.get('template_id', pd.Series([''] * len(df)))\n",
        "    })\n",
        "    if 'label' in df.columns:\n",
        "        tokens['anomaly_label'] = df['label'].astype(int).values\n",
        "    else:\n",
        "        tokens['anomaly_label'] = 0\n",
        "    tokens['timestamp'] = df['timestamp'].values\n",
        "    tokens['raw'] = df['content'].values\n",
        "    tokens['normalized'] = df['normalized'].values\n",
        "    trunc_rate = truncated / max(len(df), 1)\n",
        "    return tokens, trunc_rate\n",
        "\n",
        "def time_splits(df: pd.DataFrame, splits_cfg: Dict[str, float]) -> Dict[str, pd.DataFrame]:\n",
        "    df_sorted = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    n = len(df_sorted)\n",
        "    train_end = int(n * splits_cfg['train'])\n",
        "    val_end = train_end + int(n * splits_cfg['val'])\n",
        "    return {'train': df_sorted.iloc[:train_end],'val': df_sorted.iloc[train_end:val_end],'test': df_sorted.iloc[val_end:]}\n",
        "\n",
        "splits_cfg = data_config['splits']\n",
        "hdfs_df = hdfs_df.merge(template_df[['timestamp','template_id']], on='timestamp', how='left')\n",
        "hdfs_splits = time_splits(hdfs_df, splits_cfg)\n",
        "\n",
        "hdfs_stats = {}\n",
        "for split_name, split_df in hdfs_splits.items():\n",
        "    tokens_df, trunc = tokenize_dataframe(split_df, data_config['tokenizer']['max_length'])\n",
        "    file_path = PARQUET_DIR / f'hdfs_{split_name}.parquet'\n",
        "    tokens_df.to_parquet(file_path, index=False)\n",
        "    ds = Dataset.from_pandas(tokens_df.drop(columns=['raw','normalized']), preserve_index=False)\n",
        "    ds.save_to_disk(str(PARQUET_DIR / f'hdfs_{split_name}_hf'))\n",
        "    hdfs_stats[split_name] = {\n",
        "        'count': len(tokens_df),\n",
        "        'avg_length': float(tokens_df['input_ids'].map(len).mean()),\n",
        "        'truncation_rate': round(trunc, 4)\n",
        "    }\n",
        "    print(f'[dataset] saved HDFS {split_name} split -> {file_path}')\n",
        "\n",
        "display(pd.DataFrame(hdfs_stats).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4374deaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "openstack_splits = time_splits(openstack_df, splits_cfg)\n",
        "openstack_stats = {}\n",
        "for split_name, split_df in openstack_splits.items():\n",
        "    tokens_df, trunc = tokenize_dataframe(split_df, data_config['tokenizer']['max_length'])\n",
        "    file_path = PARQUET_DIR / f'openstack_{split_name}.parquet'\n",
        "    tokens_df.to_parquet(file_path, index=False)\n",
        "    ds = Dataset.from_pandas(tokens_df.drop(columns=['raw','normalized']), preserve_index=False)\n",
        "    ds.save_to_disk(str(PARQUET_DIR / f'openstack_{split_name}_hf'))\n",
        "    openstack_stats[split_name] = {\n",
        "        'count': len(tokens_df),\n",
        "        'avg_length': float(tokens_df['input_ids'].map(len).mean()),\n",
        "        'truncation_rate': round(trunc, 4)\n",
        "    }\n",
        "    print(f'[dataset] saved OpenStack {split_name} split -> {file_path}')\n",
        "\n",
        "display(pd.DataFrame(openstack_stats).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0acf28e5",
      "metadata": {},
      "source": [
        "## 8. Persist Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c1a84bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata = {\n",
        "    'generated_at': datetime.utcnow().isoformat() + 'Z',\n",
        "    'hdfs': hdfs_stats,\n",
        "    'openstack': openstack_stats,\n",
        "    'tokenizer_dir': str(TOKENIZER_DIR),\n",
        "    'template_index': str(Path(data_config['drain3']['transition_output']).with_name('template_index.parquet')),\n",
        "    'template_transition': str(Path(data_config['drain3']['transition_output']))\n",
        "}\n",
        "METADATA_PATH.write_text(json.dumps(metadata, indent=2))\n",
        "print(f'[metadata] saved to {METADATA_PATH}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54d5852",
      "metadata": {},
      "source": [
        "## Artifacts Produced\n",
        "- Tokenizer with special tokens -> `artifacts/tokenizer/`\n",
        "- HDFS Parquet splits and HF datasets -> `artifacts/datasets/hdfs_*.parquet`, `*_hf/`\n",
        "- OpenStack Parquet splits and HF datasets -> `artifacts/datasets/openstack_*.parquet`, `*_hf/`\n",
        "- Drain3 template index and transitions -> `artifacts/drain3/`\n",
        "- Dataset metadata summary -> `artifacts/metadata/datasets.json`\n",
        "\n",
        "Continue with notebook `01_pretrain_hdfs.ipynb` for multi-GPU MLM pretraining."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "logAnom311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
