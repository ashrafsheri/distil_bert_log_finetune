{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d91c34",
   "metadata": {},
   "source": [
    "# Improved Apache Training with Supervised Classification\n",
    "\n",
    "## üîç **Why Previous Transformer Failed (2.6% F1-Score):**\n",
    "\n",
    "**Root Causes:**\n",
    "1. **Tiny vocabulary**: Only 32 templates from normal synthetic logs\n",
    "2. **Small dataset**: Only 790 sequences (not enough for generalization)\n",
    "3. **Unsupervised approach**: Model trained only on normal, can't distinguish attacks\n",
    "4. **High uncertainty**: Validation perplexity 26.4 (should be <10)\n",
    "\n",
    "## ‚úÖ **New Approach: Supervised Attack Classification**\n",
    "\n",
    "Instead of anomaly detection (unsupervised), train a **supervised classifier** using your labeled synthetic data:\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ Uses **all 10,000 logs** (normal + attacks) for training\n",
    "- ‚úÖ Learns to **distinguish attack patterns** directly\n",
    "- ‚úÖ Multi-class classification (SQL, XSS, traversal, etc.)\n",
    "- ‚úÖ Much better performance on labeled data\n",
    "- ‚úÖ Expected F1-score: **70-85%** (vs 2.6% unsupervised)\n",
    "\n",
    "**Method:**\n",
    "- Fine-tune transformer with **classification head**\n",
    "- Train on sequences with attack labels\n",
    "- Direct optimization for attack detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdaf6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid' if 'seaborn-v0_8-darkgrid' in plt.style.available else 'default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72aca0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Repo root: /home/tpi/distil_shahreyar\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "CWD = Path.cwd().resolve()\n",
    "REPO_ROOT = CWD.parent if CWD.name == 'notebooks' else CWD\n",
    "\n",
    "cfg = yaml.safe_load((REPO_ROOT / 'configs/train_openstack.yaml').read_text())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Repo root: {REPO_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419cb0f6",
   "metadata": {},
   "source": [
    "## 1. Load ALL Synthetic Logs (Normal + Attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc4aaeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 10,050 logs\n",
      "Normal: 8,500\n",
      "Anomalous: 1,500\n",
      "\n",
      "Attack distribution:\n",
      "  sql_injection: 375\n",
      "  xss: 300\n",
      "  path_traversal: 225\n",
      "  command_injection: 150\n",
      "  scanning: 450\n"
     ]
    }
   ],
   "source": [
    "# Load synthetic logs and labels\n",
    "log_file = REPO_ROOT / 'data/apache_logs/synthetic_nodejs_apache_10k.log'\n",
    "label_file = REPO_ROOT / 'data/apache_logs/synthetic_apache_labels.json'\n",
    "\n",
    "with open(label_file, 'r') as f:\n",
    "    label_data = json.load(f)\n",
    "\n",
    "ground_truth = label_data['labels']\n",
    "metadata = label_data['metadata']\n",
    "\n",
    "print(f\"Dataset: {metadata['total_logs']:,} logs\")\n",
    "print(f\"Normal: {metadata['normal_logs']:,}\")\n",
    "print(f\"Anomalous: {metadata['anomalous_logs']:,}\")\n",
    "print(f\"\\nAttack distribution:\")\n",
    "for attack_type, count in metadata['attack_distribution'].items():\n",
    "    print(f\"  {attack_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac640dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing logs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Parsed 9,427 log entries\n",
      "\n",
      "Label distribution:\n",
      "attack_type\n",
      "normal            8500\n",
      "scanning           450\n",
      "path_traversal     225\n",
      "xss                141\n",
      "sql_injection       61\n",
      "brute_force         50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apache log parser\n",
    "APACHE_PATTERN = re.compile(\n",
    "    r'^(?P<ip>\\S+) \\S+ \\S+ '\n",
    "    r'\\[(?P<timestamp>[^\\]]+)\\] '\n",
    "    r'\"(?P<method>\\S+) (?P<path>\\S+) (?P<protocol>\\S+)\" '\n",
    "    r'(?P<status>\\d+) '\n",
    "    r'(?P<size>\\S+)'\n",
    ")\n",
    "\n",
    "RE_IPv4 = re.compile(r'\\b(?:(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\.){3}(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\b')\n",
    "RE_NUM = re.compile(r'(?<![A-Za-z])[-+]?\\d+(?:\\.\\d+)?(?![A-Za-z])')\n",
    "RE_PATH = re.compile(r'(?:/[^/\\s]+)+')\n",
    "RE_URL = re.compile(r'https?://\\S+')\n",
    "\n",
    "def normalize_message(msg: str) -> str:\n",
    "    if not msg:\n",
    "        return msg\n",
    "    out = msg\n",
    "    out = RE_URL.sub('<URL>', out)\n",
    "    out = RE_IPv4.sub('<IP>', out)\n",
    "    \n",
    "    def normalize_path(match):\n",
    "        path = match.group(0)\n",
    "        path = re.sub(r'/\\d+', '/<NUM>', path)\n",
    "        path = re.sub(r'/[0-9a-fA-F]{8,}', '/<HEX>', path)\n",
    "        return path\n",
    "    \n",
    "    out = RE_PATH.sub(normalize_path, out)\n",
    "    \n",
    "    def bucket_number(m):\n",
    "        s = m.group(0)\n",
    "        try:\n",
    "            val = float(s) if '.' in s else int(s)\n",
    "            if val == 0:\n",
    "                return '<NUM_E0>'\n",
    "            mag = int(math.floor(math.log10(abs(val))))\n",
    "            return f'<NUM_E{mag}>'\n",
    "        except:\n",
    "            return '<NUM>'\n",
    "    \n",
    "    out = RE_NUM.sub(bucket_number, out)\n",
    "    return re.sub(r'\\s+', ' ', out).strip()\n",
    "\n",
    "def parse_apache_log(log_path: Path):\n",
    "    records = []\n",
    "    with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            match = APACHE_PATTERN.match(line)\n",
    "            if not match:\n",
    "                continue\n",
    "            \n",
    "            d = match.groupdict()\n",
    "            try:\n",
    "                ts = pd.to_datetime(d['timestamp'], format='%d/%b/%Y:%H:%M:%S %z', errors='coerce')\n",
    "            except:\n",
    "                ts = pd.NaT\n",
    "            \n",
    "            message = f\"{d.get('method', 'GET')} {d.get('path', '/')} {d.get('protocol', 'HTTP/1.1')} {d.get('status', '200')}\"\n",
    "            \n",
    "            records.append({\n",
    "                'timestamp': ts,\n",
    "                'ip': d['ip'],\n",
    "                'method': d.get('method'),\n",
    "                'path': d.get('path'),\n",
    "                'status': int(d.get('status', 0)),\n",
    "                'norm_message': normalize_message(message),\n",
    "                'line_num': line_num\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Add ground truth labels\n",
    "    df['is_anomaly'] = df['line_num'].apply(lambda x: ground_truth.get(str(x), {}).get('is_anomaly', False))\n",
    "    df['attack_type'] = df['line_num'].apply(lambda x: ground_truth.get(str(x), {}).get('attack_type', 'normal'))\n",
    "    \n",
    "    # Convert to numeric labels\n",
    "    attack_types = ['normal', 'sql_injection', 'xss', 'path_traversal', 'command_injection', 'scanning']\n",
    "    attack_to_id = {att: i for i, att in enumerate(attack_types)}\n",
    "    \n",
    "    df['label'] = df['attack_type'].fillna('normal').map(attack_to_id)\n",
    "    \n",
    "    return df, attack_to_id, attack_types\n",
    "\n",
    "print(\"Parsing logs...\")\n",
    "df, attack_to_id, attack_types = parse_apache_log(log_file)\n",
    "\n",
    "print(f\"\\n‚úì Parsed {len(df):,} log entries\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['attack_type'].fillna('normal').value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5160a078",
   "metadata": {},
   "source": [
    "## 2. Build Vocabulary (All Logs - Normal + Attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fb3af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 54 (vs previous 32 from normal-only logs)\n",
      "\n",
      "Top 10 templates:\n",
      "   1. [ 466x] GET /api/users HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "   2. [ 454x] GET /static/images/logo.png HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "   3. [ 450x] GET /health HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "   4. [ 448x] GET /api/search?q=product HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "   5. [ 447x] GET / HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "   6. [ 434x] GET /api/auth/logout HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "   7. [ 427x] GET /api/auth/login HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "   8. [ 423x] GET /static/js/app.js HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "   9. [ 419x] GET /metrics HTTP/<NUM>.<NUM_E0> <NUM_E2>\n",
      "  10. [ 417x] GET /docs HTTP/<NUM>.<NUM_E0> <NUM_E2>\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary from ALL logs\n",
    "template_counts = Counter(df['norm_message'])\n",
    "id_to_template = sorted(template_counts.keys(), key=lambda x: template_counts[x], reverse=True)\n",
    "template_to_id = {t: i for i, t in enumerate(id_to_template)}\n",
    "\n",
    "vocab_size = len(id_to_template)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size} (vs previous 32 from normal-only logs)\")\n",
    "print(f\"\\nTop 10 templates:\")\n",
    "for i, (template, count) in enumerate(template_counts.most_common(10), 1):\n",
    "    print(f\"  {i:2d}. [{count:4d}x] {template}\")\n",
    "\n",
    "# Map templates\n",
    "df['template_id'] = df['norm_message'].map(template_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6434108",
   "metadata": {},
   "source": [
    "## 3. Create Labeled Sequences for Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6fc2c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff52f767dcf74e3f8f8a8634a5983b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating labeled sequences:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Created 979 labeled sequences\n",
      "\n",
      "Sequence label distribution:\n",
      "  normal: 883 (90.2%)\n",
      "  xss: 9 (0.9%)\n",
      "  path_traversal: 23 (2.3%)\n",
      "  scanning: 64 (6.5%)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences with labels\n",
    "WINDOW_SIZE = 20\n",
    "STRIDE = 10\n",
    "\n",
    "sequences = []\n",
    "sequence_labels = []\n",
    "\n",
    "df_sorted = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "for ip, group in tqdm(df_sorted.groupby('ip'), desc=\"Creating labeled sequences\"):\n",
    "    templates = group['template_id'].tolist()\n",
    "    labels = group['label'].tolist()\n",
    "    \n",
    "    if len(templates) < 2:\n",
    "        continue\n",
    "    \n",
    "    for i in range(0, len(templates) - 1, STRIDE):\n",
    "        window = templates[i:i + WINDOW_SIZE]\n",
    "        window_labels = labels[i:i + WINDOW_SIZE]\n",
    "        \n",
    "        if len(window) < 2:\n",
    "            continue\n",
    "        \n",
    "        sequences.append(window)\n",
    "        \n",
    "        # Sequence label = most common attack type in window\n",
    "        # If any attack exists, use that; otherwise 'normal'\n",
    "        non_normal = [l for l in window_labels if l != 0 and not pd.isna(l)]\n",
    "        if non_normal:\n",
    "            counter = Counter(non_normal)\n",
    "            seq_label = int(counter.most_common(1)[0][0])  # Convert to int\n",
    "        else:\n",
    "            seq_label = 0  # normal\n",
    "        \n",
    "        sequence_labels.append(seq_label)\n",
    "\n",
    "print(f\"\\n‚úì Created {len(sequences):,} labeled sequences\")\n",
    "print(f\"\\nSequence label distribution:\")\n",
    "label_counts = Counter(sequence_labels)\n",
    "for label_id, count in sorted(label_counts.items()):\n",
    "    print(f\"  {attack_types[label_id]}: {count} ({count/len(sequence_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f510af",
   "metadata": {},
   "source": [
    "## 4. Supervised Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10277b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized:\n",
      "  Vocabulary: 54 + 1 (pad) = 55\n",
      "  Classes: 6 (normal, sql_injection, xss, path_traversal, command_injection, scanning)\n",
      "  Parameters: 2,182,278\n"
     ]
    }
   ],
   "source": [
    "# Classification model (transformer encoder + classification head)\n",
    "class AttackClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_classes: int, pad_id: int, \n",
    "                 d_model: int, n_layers: int, n_heads: int,\n",
    "                 ffn_dim: int, dropout: float, max_length: int):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.positional = nn.Parameter(torch.zeros(1, max_length, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=n_heads, dim_feedforward=ffn_dim,\n",
    "            dropout=dropout, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        seq_len = input_ids.size(1)\n",
    "        x = self.embedding(input_ids)\n",
    "        x = x + self.positional[:, :seq_len, :]\n",
    "        \n",
    "        key_padding = attention_mask == 0\n",
    "        x = self.encoder(x, src_key_padding_mask=key_padding)\n",
    "        x = self.dropout(self.norm(x))\n",
    "        \n",
    "        # Use [CLS] token (first position) or mean pooling\n",
    "        pooled = x.mean(dim=1)  # Mean pooling over sequence\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "pad_id = vocab_size\n",
    "total_vocab = vocab_size + 1\n",
    "num_classes = len(attack_types)\n",
    "\n",
    "model = AttackClassifier(\n",
    "    vocab_size=total_vocab,\n",
    "    num_classes=num_classes,\n",
    "    pad_id=pad_id,\n",
    "    d_model=256,\n",
    "    n_layers=4,  # Smaller model for small dataset\n",
    "    n_heads=8,\n",
    "    ffn_dim=512,\n",
    "    dropout=0.1,\n",
    "    max_length=100\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model initialized:\")\n",
    "print(f\"  Vocabulary: {vocab_size} + 1 (pad) = {total_vocab}\")\n",
    "print(f\"  Classes: {num_classes} ({', '.join(attack_types)})\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c66260",
   "metadata": {},
   "source": [
    "## 5. Training Setup with Class Weights (Handle Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5a917a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class weights (to handle imbalance):\n",
      "  normal: 0.18\n",
      "  sql_injection: 1.00\n",
      "  xss: 18.13\n",
      "  path_traversal: 7.09\n",
      "  command_injection: 1.00\n",
      "  scanning: 2.55\n",
      "\n",
      "Training setup:\n",
      "  Train sequences: 783\n",
      "  Val sequences: 196\n",
      "  Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "class LabeledSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "class ClassificationCollator:\n",
    "    def __init__(self, pad_id: int, max_length: int):\n",
    "        self.pad_id = pad_id\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        sequences, labels = zip(*batch)\n",
    "        \n",
    "        truncated = [seq[:self.max_length] for seq in sequences]\n",
    "        max_len = max(len(seq) for seq in truncated)\n",
    "        bs = len(truncated)\n",
    "        \n",
    "        input_ids = torch.full((bs, max_len), self.pad_id, dtype=torch.long)\n",
    "        attention_mask = torch.zeros((bs, max_len), dtype=torch.long)\n",
    "        \n",
    "        for i, seq in enumerate(truncated):\n",
    "            input_ids[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
    "            attention_mask[i, :len(seq)] = 1\n",
    "        \n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels_tensor\n",
    "        }\n",
    "\n",
    "# Split data\n",
    "dataset = LabeledSequenceDataset(sequences, sequence_labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "collator = ClassificationCollator(pad_id=pad_id, max_length=100)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collator)\n",
    "\n",
    "# Class weights for imbalanced data\n",
    "label_counts = Counter(sequence_labels)\n",
    "total_samples = len(sequence_labels)\n",
    "\n",
    "# Handle case where some classes have no samples\n",
    "weights = []\n",
    "for i in range(num_classes):\n",
    "    count = label_counts.get(i, 0)\n",
    "    if count > 0:\n",
    "        weights.append(total_samples / (num_classes * count))\n",
    "    else:\n",
    "        # For missing classes, use a default weight of 1.0\n",
    "        weights.append(1.0)\n",
    "\n",
    "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "\n",
    "print(f\"\\nClass weights (to handle imbalance):\")\n",
    "for i, (attack_type, weight) in enumerate(zip(attack_types, class_weights)):\n",
    "    print(f\"  {attack_type}: {weight:.2f}\")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"  Train sequences: {train_size:,}\")\n",
    "print(f\"  Val sequences: {val_size:,}\")\n",
    "print(f\"  Batch size: 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd75e9a",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7145eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING SUPERVISED TRAINING\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065966d3f84e4a5e87d59edf92760da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7f990fe585497c8ee9d1c622bacbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.98      0.99       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.00      0.00      0.00         1\n",
      "   path_traversal       0.83      1.00      0.91         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.67      1.00      0.80         6\n",
      "\n",
      "        micro avg       0.98      0.98      0.98       196\n",
      "        macro avg       0.42      0.50      0.45       196\n",
      "     weighted avg       0.98      0.98      0.98       196\n",
      "\n",
      "\n",
      "Epoch 1/15:\n",
      "  Train Loss: 0.5291 | Train Acc: 0.9885\n",
      "  Val Loss:   0.4663 | Val Acc:   0.9796\n",
      "  ‚úì Saved best model (val_acc: 0.9796)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7305a86001494baf83457c78722c3e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daec75746f5a477abc3498ebf38f8e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.98      0.99       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.60      0.75         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.60      1.00      0.75         6\n",
      "\n",
      "        micro avg       0.97      0.97      0.97       196\n",
      "        macro avg       0.52      0.60      0.53       196\n",
      "     weighted avg       0.99      0.97      0.98       196\n",
      "\n",
      "\n",
      "Epoch 2/15:\n",
      "  Train Loss: 0.2655 | Train Acc: 0.9911\n",
      "  Val Loss:   0.4327 | Val Acc:   0.9745\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f6deed77dc4a8c89d51007afa6946f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d333317ddfb44139163c7438d78a9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.98      0.99       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       1.00      1.00      1.00         1\n",
      "   path_traversal       1.00      1.00      1.00         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.67      1.00      0.80         6\n",
      "\n",
      "        micro avg       0.98      0.98      0.98       196\n",
      "        macro avg       0.61      0.66      0.63       196\n",
      "     weighted avg       0.99      0.98      0.99       196\n",
      "\n",
      "\n",
      "Epoch 3/15:\n",
      "  Train Loss: 0.1276 | Train Acc: 0.9949\n",
      "  Val Loss:   0.2421 | Val Acc:   0.9847\n",
      "  ‚úì Saved best model (val_acc: 0.9847)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545d3de2d4fb43778bb121ce1a5c30d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95f41a0deda4cbb98595e4477816b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.98      0.99       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.33      1.00      0.50         1\n",
      "   path_traversal       1.00      0.60      0.75         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.67      1.00      0.80         6\n",
      "\n",
      "        micro avg       0.97      0.97      0.97       196\n",
      "        macro avg       0.50      0.60      0.51       196\n",
      "     weighted avg       0.99      0.97      0.98       196\n",
      "\n",
      "\n",
      "Epoch 4/15:\n",
      "  Train Loss: 0.1134 | Train Acc: 0.9974\n",
      "  Val Loss:   0.3079 | Val Acc:   0.9745\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac7124ecc1b4e36ab1c7bc31e0a176a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0444f3f8c3af4145b758e9870af1f562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.98      0.99       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.00      0.00      0.00         1\n",
      "   path_traversal       0.83      1.00      0.91         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.67      1.00      0.80         6\n",
      "\n",
      "        micro avg       0.98      0.98      0.98       196\n",
      "        macro avg       0.42      0.50      0.45       196\n",
      "     weighted avg       0.98      0.98      0.98       196\n",
      "\n",
      "\n",
      "Epoch 5/15:\n",
      "  Train Loss: 0.0649 | Train Acc: 0.9974\n",
      "  Val Loss:   0.1725 | Val Acc:   0.9796\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c37b8d4c224821b1d204241f5a461a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513b3d58b83e4d73b8e5fb2453810313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.98      0.99       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.33      1.00      0.50         1\n",
      "   path_traversal       1.00      0.60      0.75         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.67      1.00      0.80         6\n",
      "\n",
      "        micro avg       0.97      0.97      0.97       196\n",
      "        macro avg       0.50      0.60      0.51       196\n",
      "     weighted avg       0.99      0.97      0.98       196\n",
      "\n",
      "\n",
      "Epoch 6/15:\n",
      "  Train Loss: 0.1648 | Train Acc: 0.9923\n",
      "  Val Loss:   0.2742 | Val Acc:   0.9745\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94da60bfa42c4db58c5228bfe3adefaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b279aba5fda4529b83307929be7ac68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      1.00      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.80      0.89         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       1.00      1.00      1.00         6\n",
      "\n",
      "        micro avg       0.99      0.99      0.99       196\n",
      "        macro avg       0.58      0.63      0.59       196\n",
      "     weighted avg       1.00      0.99      1.00       196\n",
      "\n",
      "\n",
      "Epoch 7/15:\n",
      "  Train Loss: 0.0394 | Train Acc: 0.9974\n",
      "  Val Loss:   0.1883 | Val Acc:   0.9949\n",
      "  ‚úì Saved best model (val_acc: 0.9949)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf07c2e65954029af6df1d1a096f701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27511343fc24d4ca59afa771e5c78c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.99      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.60      0.75         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.75      1.00      0.86         6\n",
      "\n",
      "        micro avg       0.98      0.98      0.98       196\n",
      "        macro avg       0.54      0.60      0.55       196\n",
      "     weighted avg       0.99      0.98      0.98       196\n",
      "\n",
      "\n",
      "Epoch 8/15:\n",
      "  Train Loss: 0.0219 | Train Acc: 1.0000\n",
      "  Val Loss:   0.2901 | Val Acc:   0.9847\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2795ada8eb264cd582219fa01fcc1a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff6ccf51da244d98858cd947bb9182d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.99      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.80      0.89         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.86      1.00      0.92         6\n",
      "\n",
      "        micro avg       0.99      0.99      0.99       196\n",
      "        macro avg       0.56      0.63      0.58       196\n",
      "     weighted avg       0.99      0.99      0.99       196\n",
      "\n",
      "\n",
      "Epoch 9/15:\n",
      "  Train Loss: 0.0142 | Train Acc: 1.0000\n",
      "  Val Loss:   0.1317 | Val Acc:   0.9898\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7322af18d0414b7aa32230ee83248e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182b079c13fc4f59aadbd1328dd1398e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.99      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.80      0.89         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.86      1.00      0.92         6\n",
      "\n",
      "        micro avg       0.99      0.99      0.99       196\n",
      "        macro avg       0.56      0.63      0.58       196\n",
      "     weighted avg       0.99      0.99      0.99       196\n",
      "\n",
      "\n",
      "Epoch 10/15:\n",
      "  Train Loss: 0.0129 | Train Acc: 1.0000\n",
      "  Val Loss:   0.2334 | Val Acc:   0.9898\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fc8ab416924ef7b7cff292a7778054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff97eda8fc26483498791c291312fbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.99      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.80      0.89         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.86      1.00      0.92         6\n",
      "\n",
      "        micro avg       0.99      0.99      0.99       196\n",
      "        macro avg       0.56      0.63      0.58       196\n",
      "     weighted avg       0.99      0.99      0.99       196\n",
      "\n",
      "\n",
      "Epoch 11/15:\n",
      "  Train Loss: 0.0065 | Train Acc: 1.0000\n",
      "  Val Loss:   0.1956 | Val Acc:   0.9898\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5f640de6e945b4954aa99d7c4f2737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b5cb7d76e947a9831a5ffa9ac6d002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.99      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.80      0.89         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.86      1.00      0.92         6\n",
      "\n",
      "        micro avg       0.99      0.99      0.99       196\n",
      "        macro avg       0.56      0.63      0.58       196\n",
      "     weighted avg       0.99      0.99      0.99       196\n",
      "\n",
      "\n",
      "Epoch 12/15:\n",
      "  Train Loss: 0.0041 | Train Acc: 1.0000\n",
      "  Val Loss:   0.1931 | Val Acc:   0.9898\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddde357e2c1f444cbf3c2a9c943d4171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaabbf62c91426c880a4830712e8f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.99      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.80      0.89         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.86      1.00      0.92         6\n",
      "\n",
      "        micro avg       0.99      0.99      0.99       196\n",
      "        macro avg       0.56      0.63      0.58       196\n",
      "     weighted avg       0.99      0.99      0.99       196\n",
      "\n",
      "\n",
      "Epoch 13/15:\n",
      "  Train Loss: 0.0044 | Train Acc: 1.0000\n",
      "  Val Loss:   0.1939 | Val Acc:   0.9898\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c878ab08eb45ffa719da8a9bb4460d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8aa9b34fd704c2081654d1c3ad751b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.99      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.80      0.89         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.86      1.00      0.92         6\n",
      "\n",
      "        micro avg       0.99      0.99      0.99       196\n",
      "        macro avg       0.56      0.63      0.58       196\n",
      "     weighted avg       0.99      0.99      0.99       196\n",
      "\n",
      "\n",
      "Epoch 14/15:\n",
      "  Train Loss: 0.0037 | Train Acc: 1.0000\n",
      "  Val Loss:   0.1922 | Val Acc:   0.9898\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072ba12b6e7d4ff0873370abd655055b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 [Train]:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7fd57e83f247ecabd254231f030858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15 [Val]:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "           normal       1.00      0.99      1.00       184\n",
      "    sql_injection       0.00      0.00      0.00         0\n",
      "              xss       0.50      1.00      0.67         1\n",
      "   path_traversal       1.00      0.80      0.89         5\n",
      "command_injection       0.00      0.00      0.00         0\n",
      "         scanning       0.86      1.00      0.92         6\n",
      "\n",
      "        micro avg       0.99      0.99      0.99       196\n",
      "        macro avg       0.56      0.63      0.58       196\n",
      "     weighted avg       0.99      0.99      0.99       196\n",
      "\n",
      "\n",
      "Epoch 15/15:\n",
      "  Train Loss: 0.0032 | Train Acc: 1.0000\n",
      "  Val Loss:   0.1964 | Val Acc:   0.9898\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE\n",
      "======================================================================\n",
      "Best validation accuracy: 0.9949\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\")\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'acc': f\"{correct/total:.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, loader, criterion, device, epoch, attack_types):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch} [Val]\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    \n",
    "    # Classification report with explicit labels\n",
    "    print(f\"\\n{classification_report(all_labels, all_preds, labels=list(range(num_classes)), target_names=attack_types, zero_division=0)}\")\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 15\n",
    "best_val_acc = 0\n",
    "\n",
    "output_dir = REPO_ROOT / 'artifacts/apache_supervised_model'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STARTING SUPERVISED TRAINING\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "    val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, device, epoch, attack_types)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'attack_types': attack_types,\n",
    "            'vocab_size': total_vocab,\n",
    "            'pad_id': pad_id\n",
    "        }, output_dir / 'best.pt')\n",
    "        print(f\"  ‚úì Saved best model (val_acc: {val_acc:.4f})\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec2cfc",
   "metadata": {},
   "source": [
    "## 7. Results\n",
    "\n",
    "**Expected Performance:**\n",
    "- Overall Accuracy: **70-85%**\n",
    "- Per-class F1-scores:\n",
    "  - Normal: 75-85% (high precision, good recall)\n",
    "  - SQL Injection: 80-90% (very distinctive patterns)\n",
    "  - XSS: 75-85% (clear signatures)\n",
    "  - Path Traversal: 80-90% (obvious patterns)\n",
    "  - Command Injection: 85-95% (critical signatures)\n",
    "  - Scanning: 60-75% (harder to distinguish)\n",
    "\n",
    "**Why This Works Better:**\n",
    "1. ‚úÖ Supervised learning with labeled data\n",
    "2. ‚úÖ Multi-class classification (learns specific attack patterns)\n",
    "3. ‚úÖ Class weights handle imbalance\n",
    "4. ‚úÖ Uses all data (normal + attacks)\n",
    "5. ‚úÖ Direct optimization for attack detection\n",
    "\n",
    "This should give you **70-85% F1-score** instead of the current 2.6%!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log_anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
