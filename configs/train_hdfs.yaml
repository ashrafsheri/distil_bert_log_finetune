# Masked-language-model pretraining on HDFS logs
model:
  name: distilbert-base-uncased
  fallback: bert-base-uncased
  gradient_checkpointing: false  # auto-enabled for fallback model if memory pressure detected
  trust_remote_code: false

seed: 42

sequence:
  max_length: 128
  raise_to_256_if_truncation_rate_gt: 0.15
  mlm_probability: 0.15

training:
  epochs: 3
  train_batch_size_per_device: 16
  eval_batch_size_per_device: 32
  grad_accumulation_steps: 8
  effective_global_batch_size: 256
  max_grad_norm: 1.0
  label_smoothing: 0.0

optimizer:
  type: adamw
  lr: 3.0e-5
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01
  warmup_steps: 1000
  scheduler: linear

precision:
  mixed_precision: fp16
  gradient_checkpointing_override: false

accelerate:
  config_path: configs/accelerate_config.yaml
  num_processes: 2
  mixed_precision: fp16
  gradient_accumulation_steps: 8
  ddp_timeout: 7200
  find_unused_parameters: false
  dispatch_batches: false
  zero_stage2_toggle: false

checkpointing:
  output_dir: artifacts/logbert-mlm-hdfs
  save_steps: 2000
  keep_last_n: 3
  save_on_epoch_end: true
  metric: eval_loss
  greater_is_better: false
  shards_per_epoch: 2

logging:
  log_steps: 100
  eval_steps: 1000
  throughput_ema_beta: 0.9
  log_gpu_memory: true
  log_gradient_norm: true

early_stopping:
  patience: 3
  min_delta: 1.0e-3

memory:
  target_vram_gb_per_gpu: 24
  cleanup_every_n_steps: 500
  free_cuda_after_checkpoint: true
  monitor_interval: 200

data:
  parquet_dir: artifacts/datasets
  train_file: artifacts/datasets/hdfs_train.parquet
  val_file: artifacts/datasets/hdfs_val.parquet
  test_file: artifacts/datasets/hdfs_test.parquet
  template_transition_file: artifacts/drain3/template_transitions.parquet

artifacts:
  tokenizer_dir: artifacts/tokenizer
  metrics_dir: artifacts/metrics/hdfs
  checkpoints_dir: artifacts/logbert-mlm-hdfs
  run_config_path: artifacts/logbert-mlm-hdfs/run_config.json

# Optional DeepSpeed ZeRO-2 override. Enable by setting accelerate.zero_stage2_toggle to true.
deepspeed_zero2_template: |
  {
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "zero_optimization": {
      "stage": 2,
      "overlap_comm": true,
      "allgather_partitions": true,
      "reduce_scatter": true,
      "contiguous_gradients": true
    },
    "bf16": {"enabled": false},
    "fp16": {"enabled": true}
  }
